{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This site contains the documentation for the <code>olfactory-navigation</code> framework.</p> <p>This project consists in a framework that allows one to define an olfactory environment and agents to navigate this environment in order to try and find the source of an odor using exclusively its sense of smell.</p> <p>We can imagine for instance a dog attempting to find a treat in a field where the wind carries the odor towards him.</p> <p>The framework aims to allow a user to create agents using different techniques and evaluate its performance (computational and technical) during simulations on olfactory environments.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ol> <li> <p>Code Reference</p> <p>1.1 olfactory_navigation</p> </li> <li> <p>Tutorials</p> <p>2.1 How to Create an Agent?</p> </li> </ol>"},{"location":"#pimlb","title":"PiMLb","text":"<p>This framework was built as a project within the PiMLb unit of the Malga lab.</p>"},{"location":"agent_creation/","title":"How to Create an Agent?","text":"<p>In this page, we will describe how to create an Agent by describing the various parts required to create a valid agent.</p>"},{"location":"agent_creation/#the-agent-concept","title":"The Agent Concept","text":"<p>The concept of an agent within the olfactory navigation framework is a an abstract concept. It should be able to perform two things:</p> <ol> <li>Be Trained</li> <li>Be Tested</li> </ol> <p>These two steps will be defined below.</p> <p>Fundamentally, to create an agent, the user has to create a subclass of the generic Agent class. Along with this, the various functions need to be overwritten as needed.</p>"},{"location":"agent_creation/#attributes","title":"Attributes","text":"<p>The agent has already a set of attributes by default:</p> <ul> <li>environment: The Environment the agent should be trained on</li> <li>name: By default the class name</li> <li>threshold: The olfactory sensitivity of the agent</li> </ul> <p>Additionally, other attributes can be defined within the init function. Those attribute should be of one of three categories:</p> <ul> <li>Static: Those attribute should be there to help within the functions of the agent.</li> <li>Trainable: Those attribute typically should be set to None within the init and set to something after the train() function has been run.</li> <li>Status: Those attributes are dynamic variables that will change over the course of the simulation. They should reprensent the status of the agent during the simulation (for example, where the agent believes he is in the space, or an internal clock, or even a memory).</li> </ul>"},{"location":"agent_creation/#training","title":"Training","text":"<p>For the training part, the concept of the agent is trained. We define the brain of the agent.</p> <p>For this, the function 'Agent.train()' needs to be defined (overwritten).</p>"},{"location":"agent_creation/#testing","title":"Testing","text":"<p>For the testing, the agent needs to be able to interact be interacted with from the run_test() function of the simulation module.</p> <p>For this the following functions need to be implemented:</p> <ul> <li>initialize_state()</li> <li>choose_action()</li> <li>update_state(observation, source_reached)</li> <li>kill()</li> </ul>"},{"location":"agent_creation/#saving","title":"Saving","text":"<p>The agent must be able to be saved to memory for reproducability's sake. Related functions</p> <p>By default, the agent can be saved as a pickle file. But it is remended to define your own save() and load() functions as a pickle file is a blackbox and may save too many attributes, so not very space efficient. For instance, for the agent to be re-used later, only the Trainable variables are needed.</p>"},{"location":"agent_creation/#gpu-support","title":"GPU support","text":"<p>To speedup large operations, the </p>"},{"location":"reference/","title":"olfactory_navigation","text":""},{"location":"reference/#olfactory_navigation.Agent","title":"<code>Agent</code>","text":"<p>Generic agent class</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>class Agent:\n    '''\n    Generic agent class\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        self.environment = environment\n        self.threshold = threshold\n\n        # setup name\n        if name is None:\n            self.name = self.class_name\n            self.name += f'-tresh_{self.threshold}'\n        else:\n            self.name = name\n\n        self.saved_at = None\n\n        self.on_gpu = False\n        self._alternate_version = None\n\n\n    @property\n    def class_name(self):\n        return self.__class__.__name__\n\n\n    def to_gpu(self) -&gt; 'Agent':\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n        '''\n        raise NotImplementedError('The to_gpu function is not implemented, make an agent subclass to implement the method')\n\n\n    def to_cpu(self) -&gt; 'Agent':\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        cpu_agent : Agent\n            A new environment instance where the arrays are on the cpu memory.\n        '''\n        if self.on_gpu:\n            assert self._alternate_version is not None, \"Something went wrong\"\n            return self._alternate_version\n\n        return self\n\n\n    def train(self) -&gt; None:\n        '''\n        Function to call the particular flavour of training of the agent.\n        '''\n        raise NotImplementedError('The train function is not implemented, make an agent subclass to implement the method')\n\n\n    def save(self,\n             folder:str|None=None,\n             force:bool=False,\n             save_environment:bool=False\n             ) -&gt; None:\n        '''\n        Function to save a trained agent to memory.\n        '''\n        raise NotImplementedError('The save function is not implemented, make an agent subclass to implement the method')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'Agent':\n        '''\n        Function to load a trained agent from memory.\n        '''\n        from olfactory_navigation import agents\n\n        for name, obj in inspect.getmembers(agents):\n            if inspect.isclass(obj) and (name in folder) and issubclass(obj, cls) and (obj != cls):\n                return obj.load(folder)\n\n        raise NotImplementedError('The load function is not implemented, make an agent subclass to implement the method')\n\n\n    def initialize_state(self, n:int=1) -&gt; None:\n        '''\n        Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents to initialize.\n        '''\n        raise NotImplementedError('The initialize_state function is not implemented, make an agent subclass to implement the method')\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to allow for the agent to choose an action to take based on its current state.\n        It then stores this action in its state.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A vector in 2D space of the movement the agent will take\n        '''\n        raise NotImplementedError('The choose_action function is not implemented, make an agent subclass to implement the method')\n\n\n    def update_state(self,\n                     observation:int|np.ndarray,\n                     source_reached:bool|np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state of the agent based on the previous action taken and the observation received.\n        '''\n        raise NotImplementedError('The update_state function is not implemented, make an agent subclass to implement the method')\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that still haven't reached the source\n        '''\n        raise NotImplementedError('The kill function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to allow for the agent to choose an action to take based on its current state. It then stores this action in its state.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A vector in 2D space of the movement the agent will take</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to allow for the agent to choose an action to take based on its current state.\n    It then stores this action in its state.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A vector in 2D space of the movement the agent will take\n    '''\n    raise NotImplementedError('The choose_action function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents to initialize.</p> <code>1</code> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def initialize_state(self, n:int=1) -&gt; None:\n    '''\n    Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents to initialize.\n    '''\n    raise NotImplementedError('The initialize_state function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that still haven't reached the source</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that still haven't reached the source\n    '''\n    raise NotImplementedError('The kill function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load a trained agent from memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'Agent':\n    '''\n    Function to load a trained agent from memory.\n    '''\n    from olfactory_navigation import agents\n\n    for name, obj in inspect.getmembers(agents):\n        if inspect.isclass(obj) and (name in folder) and issubclass(obj, cls) and (obj != cls):\n            return obj.load(folder)\n\n    raise NotImplementedError('The load function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.save","title":"<code>save(folder=None, force=False, save_environment=False)</code>","text":"<p>Function to save a trained agent to memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         force:bool=False,\n         save_environment:bool=False\n         ) -&gt; None:\n    '''\n    Function to save a trained agent to memory.\n    '''\n    raise NotImplementedError('The save function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Name Type Description <code>cpu_agent</code> <code>Agent</code> <p>A new environment instance where the arrays are on the cpu memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def to_cpu(self) -&gt; 'Agent':\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    cpu_agent : Agent\n        A new environment instance where the arrays are on the cpu memory.\n    '''\n    if self.on_gpu:\n        assert self._alternate_version is not None, \"Something went wrong\"\n        return self._alternate_version\n\n    return self\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def to_gpu(self) -&gt; 'Agent':\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n    '''\n    raise NotImplementedError('The to_gpu function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.train","title":"<code>train()</code>","text":"<p>Function to call the particular flavour of training of the agent.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def train(self) -&gt; None:\n    '''\n    Function to call the particular flavour of training of the agent.\n    '''\n    raise NotImplementedError('The train function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state of the agent based on the previous action taken and the observation received.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def update_state(self,\n                 observation:int|np.ndarray,\n                 source_reached:bool|np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state of the agent based on the previous action taken and the observation received.\n    '''\n    raise NotImplementedError('The update_state function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment","title":"<code>Environment</code>","text":"<p>Class to represent an olfactory environment.</p> <p>It is defined based on an olfactory data set.</p> ----------------- |                       | <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or ndarray</code> <p>The dataset containing the olfactory data. It can be provided as a path to a file containing said array.</p> required <code>source_position</code> <code>list or ndarray</code> <p>The center point of the source provided as a list or a 1D array with the components being x,y.</p> required <code>source_radius</code> <code>int</code> <p>The radius from the center point of the source in which we consider the agent has reached the source.</p> <code>1</code> <code>discretization</code> <code>int</code> <p>How many units should be kept in the final array (a discretization of 2 will retain every other point of the original array).</p> <code>1</code> <code>margins</code> <code>int or list or ndarray</code> <p>How many discretized units have to be added to the data as margins. If a unique element is provided, the margin will be this same value on each side. If a list or array of 2 elements is provided, the first number will be vertical margins (y-axis), while the other will be on the x-axis (horizontal).</p> <code>0</code> <code>boundary_condition</code> <code>stop or wrap or wrap_vertical or wrap_horizontal or clip</code> <p>How the agent should behave at the boundary. Stop means for the agent to stop at the boundary, if the agent tries to move north while being on the top edge, it will stay in the same state. Wrap means for the borders to be like portals, when entering on one side, it reappears on the other side. Wrap can be specified to be only vertically or horizontally</p> <code>'stop'</code> <code>start_zone</code> <code>Literal['odor_present', 'data_zone'] | ndarray</code> <code>'data_zone'</code> <code>odor_present_threshold</code> <code>float | None</code> <code>None</code> <code>name</code> <code>str | None</code> <code>None</code> Arguments Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>class Environment:\n    '''\n    Class to represent an olfactory environment.\n\n    It is defined based on an olfactory data set.\n\n    -------------------------\n    |                       |\n    |   -----------------   |\n    |   |               |   |\n    |   -----------------   |\n    |                       |\n    -------------------------\n\n    # TODO: Add support for a 'real' grid, eg in meters... \n\n    margins can be provided as:\n    - An equal margin on each side\n    - A array of 2 elements for x and y margins\n    - A 2D array for each element being [axis, side] where axis is [vertical, horizontal] and side is [L,R]\n\n    ...\n    # TODO Write these\n    Parameters\n    ----------\n    data : str or np.ndarray\n        The dataset containing the olfactory data. It can be provided as a path to a file containing said array.\n    source_position : list or np.ndarray\n        The center point of the source provided as a list or a 1D array with the components being x,y.\n    source_radius : int, default=1\n        The radius from the center point of the source in which we consider the agent has reached the source.\n    discretization : int, default=1\n        How many units should be kept in the final array (a discretization of 2 will retain every other point of the original array).\n    margins : int or list or np.ndarray, default=0\n        How many discretized units have to be added to the data as margins.\n        If a unique element is provided, the margin will be this same value on each side.\n        If a list or array of 2 elements is provided, the first number will be vertical margins (y-axis), while the other will be on the x-axis (horizontal).\n    boundary_condition : 'stop' or 'wrap' or 'wrap_vertical' or 'wrap_horizontal' or 'clip', default='stop'\n        How the agent should behave at the boundary.\n        Stop means for the agent to stop at the boundary, if the agent tries to move north while being on the top edge, it will stay in the same state.\n        Wrap means for the borders to be like portals, when entering on one side, it reappears on the other side.\n        Wrap can be specified to be only vertically or horizontally\n    start_zone:Literal['odor_present','data_zone']|np.ndarray='data_zone',\n    odor_present_threshold:float|None=None,\n    name:str|None=None\n\n    Arguments\n    ---------\n\n    '''\n    def __init__(self,\n                 data:str|np.ndarray,\n                 source_position:list|np.ndarray,\n                 source_radius:int=1,\n                 discretization:int=1,\n                 margins:int|list|np.ndarray=0,\n                 boundary_condition:Literal['stop', 'wrap', 'wrap_vertical', 'wrap_horizontal', 'clip' ,'no']='stop',\n                 start_zone:Literal['odor_present','data_zone']|np.ndarray='data_zone',\n                 odor_present_threshold:float|None=None,\n                 name:str|None=None,\n                 seed : int = 12131415,\n                 ) -&gt; None:\n        self.saved_at = None\n\n        # Load from file if string provided\n        self.source_data_file = data if isinstance(data, str) else None\n        if isinstance(data, str):\n            data_file = data\n            if data_file.endswith('.npy'):\n                data = np.load(data_file)\n            else:\n                raise NotImplementedError('File format loading not implemented')\n\n        # Making margins a 2x2 array \n        if isinstance(margins, int):\n            self.margins = np.ones((2,2)) * margins\n        elif isinstance(margins, list) or (margins.shape == (2,)):\n            assert len(margins) == 2, 'Margins, if provided as a list must contain only two elements.'\n            margins = np.array(margins)\n            self.margins = np.hstack((margins[:,None], margins[:,None]))\n        elif margins.shape == (2,2):\n            self.margins = margins\n        else:\n            raise ValueError('margins argument should be either an integer or a 1D or 2D array with either shape (2) or (2,2)')\n        assert self.margins.dtype == int, 'margins should be integers'\n\n        # Reading shape of data array\n        timesteps, self.height, self.width = data.shape\n        self.padded_height = self.height + np.sum(self.margins[0])\n        self.padded_width = self.width + np.sum(self.margins[1])\n        self.shape = (self.padded_height, self.padded_width)\n\n        # Preprocess data with discretization\n        self.discretization = discretization\n        if discretization != 1:\n            raise NotImplementedError('Different discretizations have not been implemented yet') # TODO\n        self.grid : np.ndarray = data\n\n        # Apply margins to grid\n        self.grid = np.hstack([np.zeros((timesteps, self.margins[0,0], self.width)), self.grid, np.zeros((timesteps, self.margins[0,1], self.width))])\n        self.grid = np.dstack([np.zeros((timesteps, self.padded_height, self.margins[1,0])), self.grid, np.zeros((timesteps, self.padded_height, self.margins[1,1]))])\n\n        # Saving arguments\n        self.data_source_position = np.array(source_position)\n        self.source_position = self.data_source_position + self.margins[:,0]\n        self.source_radius = source_radius\n        self.boundary_condition = boundary_condition\n\n        # Starting zone\n        self.start_probabilities = np.zeros(self.shape)\n        if start_zone == 'data_zone':\n            self.start_probabilities[self.margins[0,0]:self.margins[0,0]+self.height, self.margins[1,0]:self.margins[1,0]+self.width] = 1.0\n        elif start_zone == 'odor_present':\n            self.start_probabilities = (np.mean((self.grid &gt; (odor_present_threshold if odor_present_threshold is not None else 0)).astype(int), axis=0) &gt; 0).astype(float)\n        elif isinstance(start_zone, np.ndarray):\n            if start_zone.shape == (2,2):\n                self.start_probabilities[start_zone[0,0]:start_zone[0,1], start_zone[1,0]:start_zone[1,1]] = 1.0\n            elif start_zone.shape == self.shape:\n                self.start_probabilities = start_zone\n            else:\n                raise ValueError('If an np.ndarray is provided for the start_zone it has to be 2x2...')\n        else:\n            raise ValueError('start_zone value is wrong')\n\n        self.start_type = start_zone\n\n        # Odor present tresh\n        self.odor_present_threshold = odor_present_threshold\n\n        # Removing the source area from the starting zone\n        source_mask = np.fromfunction(lambda x,y: ((x - self.source_position[0])**2 + (y - self.source_position[1])**2) &lt;= self.source_radius**2, shape=self.shape)\n        self.start_probabilities[source_mask] = 0\n\n        self.start_probabilities /= np.sum(self.start_probabilities)\n\n        # Name\n        self.name = name\n        if self.name is None:\n            self.name =  f'{self.padded_height}_{self.padded_width}' # Size of env\n            self.name += f'-edge_{self.boundary_condition}' # Boundary condition\n            self.name += f'-start_{self.start_type if self.start_type is not None else \"custom\"}' # Start zone\n            self.name += f'-source_{self.source_position[0]}_{self.source_position[1]}_radius{self.source_radius}' # Source\n\n        # gpu support\n        self._alternate_version = None\n        self.on_gpu = False\n\n        # random state\n        self.seed = seed\n        self.rnd_state = np.random.RandomState(seed = seed)\n\n\n    def plot(self, frame:int=0, ax:plt.Axes=None) -&gt; None:\n        '''\n        Simple function to plot the environment\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            An ax on which the environment can be plot\n        '''\n        # If on GPU use the CPU version to plot\n        if self.on_gpu:\n            self._alternate_version.plot(\n                frame=frame,\n                ax=ax\n            )\n            return\n\n        if ax is None:\n            _, ax = plt.subplots(1, figsize=(15,5))\n\n        # Odor grid\n        odor = plt.Rectangle([0,0], 1, 1, color='black', fill=True)\n        ax.imshow((self.grid[frame] &gt; (self.odor_present_threshold if self.odor_present_threshold is not None else 0)).astype(float), cmap='Greys')\n\n        # Start zone contour\n        start_zone = plt.Rectangle([0,0], 1, 1, color='blue', fill=False)\n        ax.contour(self.start_probabilities, levels=[0.0], colors='blue')\n\n        # Source circle\n        goal_circle = plt.Circle(self.source_position[::-1], self.source_radius, color='r', fill=False)\n        ax.add_patch(goal_circle)\n\n        # Legend\n        ax.legend([odor, start_zone, goal_circle], [f'Frame {frame} odor cues', 'Start zone', 'Source'])\n\n\n    def get_observation(self,\n                        pos:np.ndarray,\n                        time:int|np.ndarray=0\n                        ) -&gt; float|np.ndarray:\n        '''\n        Function to get an observation at a given position on the grid at a given time.\n        A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The position or list of positions to get observations at.\n        time : int or np.ndarray, default=0\n            A timestamp or list of timestamps to get the observations at.\n\n        Returns\n        -------\n        observation : float or np.ndarray\n            A single observation or list of observations.\n        '''\n        # Handling the case of a single point\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            pos = pos[None,:]\n\n        # Time looping\n        time = time % len(self.grid)\n\n        # Handle the case where the agent is allowed to be outside the grid\n        if self.boundary_condition is None or self.boundary_condition == 'no':\n            if is_single_point:\n                return float(self.grid[time, pos[0], pos[1]] ) if  0 &lt;= pos[0] &lt; self.grid.shape[1] and 0 &lt;= pos[1] &lt; self.grid.shape[2] else 0.0\n            mask = (0 &lt;= pos[:, 0]) &amp; (pos[:, 0] &lt; self.grid.shape[1]) &amp; (0 &lt;= pos[:, 1]) &amp; (pos[:, 1] &lt; self.grid.shape[2])\n            observation = np.zeros((mask.shape[0], ))\n            if isinstance(time, int):\n                observation[mask] = self.grid[time, pos[mask,0], pos[mask,1]]\n            else:\n                observation[mask] = self.grid[time[mask], pos[mask,0], pos[mask,1]]\n            return observation\n\n        observation = self.grid[time, pos[0], pos[1]] if len(pos.shape) == 1 else self.grid[time, pos[:,0], pos[:,1]]\n\n        return float(observation[0]) if is_single_point else observation\n\n\n    def source_reached(self,\n                       pos:np.ndarray\n                       ) -&gt; bool | np.ndarray:\n        '''\n        Checks whether a given position is within the source radius.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The position to check whether in the radius of the source.\n\n        Returns\n        -------\n        is_at_source : bool\n            Whether or not the position is within the radius of the source.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Handling the case of a single point\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            pos = pos[None,:]\n\n        is_at_source = (xp.sum((pos - self.source_position) ** 2, axis=1) &lt;= (self.source_radius ** 2))\n\n        return bool(is_at_source[0]) if is_single_point else is_at_source\n\n\n    def random_start_points(self,\n                            n:int=1\n                            ) -&gt; np.ndarray:\n        '''\n        Function to generate n starting positions following the starting probabilities.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many random starting positions to generate\n\n        Returns\n        -------\n        random_states_2d : np.ndarray\n            The n random 2d points in a n x 2 array. \n        '''\n        xp = cp if self.on_gpu else np\n\n        assert n&gt;0, \"n has to be a strictly positive number (&gt;0)\"\n\n        random_states = self.rnd_state.choice(xp.arange(self.padded_height * self.padded_width), size=n, replace=True, p=self.start_probabilities.ravel())\n        random_states_2d = xp.array(xp.unravel_index(random_states, (self.padded_height, self.padded_width))).T\n        return random_states_2d\n\n\n    def move(self,\n             pos:np.ndarray,\n             movement:np.ndarray\n             ) -&gt; np.ndarray:\n        '''\n        Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The start position of the movement.\n        movement : np.ndarray\n            A 2D movement vector.\n\n        Returns\n        -------\n        new_pos : np.ndarray\n            The new position after applying the movement.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Applying the movement vector\n        new_pos = pos + movement\n\n        # Handling the case we are dealing with a single point.\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            new_pos = new_pos[None,:]\n\n        # Wrap condition for vertical axis\n        if self.boundary_condition in ['wrap', 'wrap_vertical']:\n            new_pos[new_pos[:,0] &lt; 0, 0] += self.padded_height\n            new_pos[new_pos[:,0] &gt;= self.padded_height, 0] -= self.padded_height\n\n        # Wrap condition for horizontal axis\n        if self.boundary_condition in ['wrap', 'wrap_horizontal']:\n            new_pos[new_pos[:,1] &lt; 0, 1] += self.padded_width\n            new_pos[new_pos[:,1] &gt;= self.padded_width, 1] -= self.padded_width\n\n        # Stop condition\n        if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_horizontal'):\n            new_pos[:,0] = xp.clip(new_pos[:,0], 0, (self.padded_height-1))\n\n        if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_vertical'):\n            new_pos[:,1] = xp.clip(new_pos[:,1], 0, (self.padded_width-1))\n\n        if is_single_point:\n            new_pos = new_pos[0]\n\n        return new_pos\n\n\n    def distance_to_source(self,\n                           point:np.ndarray,\n                           metric:Literal['manhattan']='manhattan'\n                           ) -&gt; float | np.ndarray:\n        '''\n        Function to compute the distance(s) between given points and the source point.\n\n        Parameters\n        ----------\n        point : np.ndarray\n            A single or an Nx2 array containing N points.\n        metric : 'manhattan'\n            The metric to use to compute the distance.\n\n        Returns\n        -------\n        dist : float or np.ndarray\n            A single distance or a list of distance in a 1D distance array.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Handling the case we have a single point\n        is_single_point = (len(point.shape) == 1)\n        if is_single_point:\n            point = point[None,:]\n\n        # Computing dist\n        dist = None\n        if metric == 'manhattan':\n            dist = xp.sum(xp.abs(self.source_position[None,:] - point), axis=1) - self.source_radius\n        else:\n            raise NotImplementedError('This distance metric has not yet been implemented')\n\n        return float(dist[0]) if is_single_point else dist\n\n\n    def save(self,\n             folder:str|None=None,\n             save_arrays:bool=False,\n             force:bool=False\n             ) -&gt; None:\n        '''\n        Function to save the environment to the memory.\n\n        By default it saved in a new folder at the current path in a new folder with the name 'Env-&lt;name&gt;' where &lt;name&gt; is the name set when initializing an environment.\n        In this folder a file \"METADATA.json\" is created containing all the properties of the environment.\n\n        The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file.\n        The arrays are saved to .npy files along with the METADATA file.\n\n        If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.\n\n        Parameters\n        ----------\n        folder : str, optional\n            The folder to which to save the environment data. If it is not provided, it will be created in the current folder.\n        save_arrays : bool, default=False\n            Whether or not to save the numpy arrays to memory. (The arrays can be heavy)\n        force : bool, default=False\n            In case an environment of the same name is already saved, it will be overwritten.\n        '''\n        # If on gpu, use the cpu version to save\n        if self.on_gpu:\n            self._alternate_version.save(\n                folder=folder,\n                save_arrays=save_arrays,\n                force=force\n            )\n            return\n\n        # Assert either data_file is provided or save_arrays is enabled\n        assert save_arrays or ((self.source_data_file is not None) and (self.start_type is not None)), \"The environment was not created from a data file so 'save_arrays' has to be set to True.\"\n\n        # Adding env name to folder path\n        if folder is None:\n            folder = f'./Env-{self.name}'\n        else:\n            folder += '/Env-' + self.name\n\n        # Checking the folder exists or creates it\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n        elif len(os.listdir(folder)) &gt; 0:\n            if force:\n                shutil.rmtree(folder)\n                os.mkdir(folder)\n            else:\n                raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n        # Generating the metadata arguments dictionary\n        arguments = {}\n        arguments['name'] = self.name\n\n        if self.source_data_file is not None:\n            arguments['source_data_file'] = self.source_data_file\n\n        arguments['width']                 = self.width\n        arguments['height']                = self.height\n        arguments['margins']               = self.margins.tolist()\n        arguments['padded_width']          = int(self.padded_width)\n        arguments['padded_height']         = int(self.padded_height)\n        arguments['shape']                 = [int(s) for s in self.shape]\n        arguments['discretization']        = self.discretization\n        arguments['data_source_position']  = self.data_source_position.tolist()\n        arguments['source_position']       = self.source_position.tolist()\n        arguments['source_radius']         = self.source_radius\n        arguments['boundary_condition']    = self.boundary_condition\n\n        if self.odor_present_threshold is not None:\n            arguments['odor_present_threshold'] = self.odor_present_threshold\n        if self.start_type is not None:\n            arguments['start_type'] = self.start_type\n\n        # Output the arguments to a METADATA file\n        with open(folder + '/METADATA.json', 'w') as json_file:\n            json.dump(arguments, json_file, indent=4)\n\n        # Output the numpy arrays\n        if save_arrays:\n            np.save(folder + '/grid.npy', self.grid)\n            np.save(folder + '/start_probabilities.npy', self.start_probabilities)\n\n        # Success print\n        self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n        print(f'Environment saved to: {folder}')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'Environment':\n        '''\n        Function to load an environment from a given folder.\n\n        Parameters\n        ----------\n        folder : str\n            The folder of the Environment.\n\n        Returns\n        -------\n        loaded_env : Environment\n            The loaded environment.\n        '''\n        assert os.path.exists(folder), \"Folder doesn't exist...\"\n        assert folder.split('/')[-1].startswith('Env-'), \"The folder provided is not the data of en Environment object.\"\n\n        # Load arguments\n        arguments = None\n        with open(folder + '/METADATA.json', 'r') as json_file:\n            arguments = json.load(json_file)\n\n        # Check if numpy arrays are provided, if not, recreate a new environment model\n        if os.path.exists(folder + '/grid.npy') and os.path.exists(folder + '/start_probabilities.npy'):\n            grid = np.load(folder + '/grid.npy')\n            start_probabilities = np.load(folder + '/start_probabilities.npy')\n\n            loaded_env = cls.__new__(cls)\n\n            # Set the arguments\n            loaded_env.width                 = arguments['width']\n            loaded_env.height                = arguments['height']\n            loaded_env.margins               = np.array(arguments['margins'])\n            loaded_env.padded_width          = arguments['padded_width']\n            loaded_env.padded_height         = arguments['padded_height']\n            loaded_env.shape                 = set(arguments['shape'])\n            loaded_env.discretization        = arguments['discretization']\n            loaded_env.data_source_position  = np.array(arguments['data_source_position'])\n            loaded_env.source_position       = np.array(arguments['source_position'])\n            loaded_env.source_radius         = arguments['source_radius']\n            loaded_env.boundary_condition    = arguments['boundary_condition']\n\n            # Optional arguments\n            loaded_env.source_data_file      = arguments.get('source_data_file')\n            loaded_env.odor_present_threshold = arguments.get('odor_present_threshold')\n            loaded_env.start_type            = arguments.get('start_type')\n\n            # Arrays\n            loaded_env.grid = grid\n            loaded_env.start_probabilities = start_probabilities\n\n        else:\n            loaded_env = Environment(\n                data                  = arguments['source_data_file'],\n                source_position       = np.array(arguments['data_source_position']),\n                source_radius         = arguments['source_radius'],\n                discretization        = arguments['discretization'],\n                margins               = np.array(arguments['margins']),\n                boundary_condition    = arguments['boundary_condition'],\n                start_zone            = arguments.get('start_type'),\n                odor_present_threshold = arguments.get('odor_present_threshold'),\n                name                  = arguments['name']\n            )\n\n        # Folder where the environment was pulled from\n        loaded_env.saved_at = os.path.abspath(folder)\n\n        return loaded_env\n\n\n    def to_gpu(self) -&gt; 'Environment':\n        '''\n        Function to send the numpy arrays of the environment to the gpu memory.\n        It returns a new instance of the Environment with the arrays as cupy arrays.\n\n        Returns\n        -------\n        gpu_environment : Environment\n            A new environment instance where the arrays are on the gpu memory.\n        '''\n        assert gpu_support, \"GPU support is not enabled...\"\n\n        # Generating a new instance\n        cls = self.__class__\n        gpu_environment = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_environment, arg, cp.array(val))\n            elif arg == 'rnd_state':\n                setattr(gpu_environment, arg, cp.random.RandomState(self.seed))\n            else:\n                setattr(gpu_environment, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_environment\n        gpu_environment._alternate_version = self\n\n        gpu_environment.on_gpu = True\n        return gpu_environment\n\n\n    def to_cpu(self) -&gt; 'Environment':\n        '''\n        Function to send the numpy arrays of the environment to the cpu memory.\n        It returns a new instance of the Environment with the arrays as numpy arrays.\n\n        Returns\n        -------\n        cpu_environment : Environment\n            A new environment instance where the arrays are on the cpu memory.\n        '''\n        if self.on_gpu:\n            assert self._alternate_version is not None, \"Something went wrong\"\n            return self._alternate_version\n\n        return self\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment--todo-add-support-for-a-real-grid-eg-in-meters","title":"TODO: Add support for a 'real' grid, eg in meters...","text":"<p>margins can be provided as: - An equal margin on each side - A array of 2 elements for x and y margins - A 2D array for each element being [axis, side] where axis is [vertical, horizontal] and side is [L,R]</p> <p>...</p>"},{"location":"reference/#olfactory_navigation.Environment--todo-write-these","title":"TODO Write these","text":""},{"location":"reference/#olfactory_navigation.Environment.distance_to_source","title":"<code>distance_to_source(point, metric='manhattan')</code>","text":"<p>Function to compute the distance(s) between given points and the source point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>ndarray</code> <p>A single or an Nx2 array containing N points.</p> required <code>metric</code> <code>manhattan</code> <p>The metric to use to compute the distance.</p> <code>'manhattan'</code> <p>Returns:</p> Name Type Description <code>dist</code> <code>float or ndarray</code> <p>A single distance or a list of distance in a 1D distance array.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def distance_to_source(self,\n                       point:np.ndarray,\n                       metric:Literal['manhattan']='manhattan'\n                       ) -&gt; float | np.ndarray:\n    '''\n    Function to compute the distance(s) between given points and the source point.\n\n    Parameters\n    ----------\n    point : np.ndarray\n        A single or an Nx2 array containing N points.\n    metric : 'manhattan'\n        The metric to use to compute the distance.\n\n    Returns\n    -------\n    dist : float or np.ndarray\n        A single distance or a list of distance in a 1D distance array.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Handling the case we have a single point\n    is_single_point = (len(point.shape) == 1)\n    if is_single_point:\n        point = point[None,:]\n\n    # Computing dist\n    dist = None\n    if metric == 'manhattan':\n        dist = xp.sum(xp.abs(self.source_position[None,:] - point), axis=1) - self.source_radius\n    else:\n        raise NotImplementedError('This distance metric has not yet been implemented')\n\n    return float(dist[0]) if is_single_point else dist\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.get_observation","title":"<code>get_observation(pos, time=0)</code>","text":"<p>Function to get an observation at a given position on the grid at a given time. A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The position or list of positions to get observations at.</p> required <code>time</code> <code>int or ndarray</code> <p>A timestamp or list of timestamps to get the observations at.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>observation</code> <code>float or ndarray</code> <p>A single observation or list of observations.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def get_observation(self,\n                    pos:np.ndarray,\n                    time:int|np.ndarray=0\n                    ) -&gt; float|np.ndarray:\n    '''\n    Function to get an observation at a given position on the grid at a given time.\n    A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The position or list of positions to get observations at.\n    time : int or np.ndarray, default=0\n        A timestamp or list of timestamps to get the observations at.\n\n    Returns\n    -------\n    observation : float or np.ndarray\n        A single observation or list of observations.\n    '''\n    # Handling the case of a single point\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        pos = pos[None,:]\n\n    # Time looping\n    time = time % len(self.grid)\n\n    # Handle the case where the agent is allowed to be outside the grid\n    if self.boundary_condition is None or self.boundary_condition == 'no':\n        if is_single_point:\n            return float(self.grid[time, pos[0], pos[1]] ) if  0 &lt;= pos[0] &lt; self.grid.shape[1] and 0 &lt;= pos[1] &lt; self.grid.shape[2] else 0.0\n        mask = (0 &lt;= pos[:, 0]) &amp; (pos[:, 0] &lt; self.grid.shape[1]) &amp; (0 &lt;= pos[:, 1]) &amp; (pos[:, 1] &lt; self.grid.shape[2])\n        observation = np.zeros((mask.shape[0], ))\n        if isinstance(time, int):\n            observation[mask] = self.grid[time, pos[mask,0], pos[mask,1]]\n        else:\n            observation[mask] = self.grid[time[mask], pos[mask,0], pos[mask,1]]\n        return observation\n\n    observation = self.grid[time, pos[0], pos[1]] if len(pos.shape) == 1 else self.grid[time, pos[:,0], pos[:,1]]\n\n    return float(observation[0]) if is_single_point else observation\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load an environment from a given folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder of the Environment.</p> required <p>Returns:</p> Name Type Description <code>loaded_env</code> <code>Environment</code> <p>The loaded environment.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'Environment':\n    '''\n    Function to load an environment from a given folder.\n\n    Parameters\n    ----------\n    folder : str\n        The folder of the Environment.\n\n    Returns\n    -------\n    loaded_env : Environment\n        The loaded environment.\n    '''\n    assert os.path.exists(folder), \"Folder doesn't exist...\"\n    assert folder.split('/')[-1].startswith('Env-'), \"The folder provided is not the data of en Environment object.\"\n\n    # Load arguments\n    arguments = None\n    with open(folder + '/METADATA.json', 'r') as json_file:\n        arguments = json.load(json_file)\n\n    # Check if numpy arrays are provided, if not, recreate a new environment model\n    if os.path.exists(folder + '/grid.npy') and os.path.exists(folder + '/start_probabilities.npy'):\n        grid = np.load(folder + '/grid.npy')\n        start_probabilities = np.load(folder + '/start_probabilities.npy')\n\n        loaded_env = cls.__new__(cls)\n\n        # Set the arguments\n        loaded_env.width                 = arguments['width']\n        loaded_env.height                = arguments['height']\n        loaded_env.margins               = np.array(arguments['margins'])\n        loaded_env.padded_width          = arguments['padded_width']\n        loaded_env.padded_height         = arguments['padded_height']\n        loaded_env.shape                 = set(arguments['shape'])\n        loaded_env.discretization        = arguments['discretization']\n        loaded_env.data_source_position  = np.array(arguments['data_source_position'])\n        loaded_env.source_position       = np.array(arguments['source_position'])\n        loaded_env.source_radius         = arguments['source_radius']\n        loaded_env.boundary_condition    = arguments['boundary_condition']\n\n        # Optional arguments\n        loaded_env.source_data_file      = arguments.get('source_data_file')\n        loaded_env.odor_present_threshold = arguments.get('odor_present_threshold')\n        loaded_env.start_type            = arguments.get('start_type')\n\n        # Arrays\n        loaded_env.grid = grid\n        loaded_env.start_probabilities = start_probabilities\n\n    else:\n        loaded_env = Environment(\n            data                  = arguments['source_data_file'],\n            source_position       = np.array(arguments['data_source_position']),\n            source_radius         = arguments['source_radius'],\n            discretization        = arguments['discretization'],\n            margins               = np.array(arguments['margins']),\n            boundary_condition    = arguments['boundary_condition'],\n            start_zone            = arguments.get('start_type'),\n            odor_present_threshold = arguments.get('odor_present_threshold'),\n            name                  = arguments['name']\n        )\n\n    # Folder where the environment was pulled from\n    loaded_env.saved_at = os.path.abspath(folder)\n\n    return loaded_env\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.move","title":"<code>move(pos, movement)</code>","text":"<p>Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The start position of the movement.</p> required <code>movement</code> <code>ndarray</code> <p>A 2D movement vector.</p> required <p>Returns:</p> Name Type Description <code>new_pos</code> <code>ndarray</code> <p>The new position after applying the movement.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def move(self,\n         pos:np.ndarray,\n         movement:np.ndarray\n         ) -&gt; np.ndarray:\n    '''\n    Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The start position of the movement.\n    movement : np.ndarray\n        A 2D movement vector.\n\n    Returns\n    -------\n    new_pos : np.ndarray\n        The new position after applying the movement.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Applying the movement vector\n    new_pos = pos + movement\n\n    # Handling the case we are dealing with a single point.\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        new_pos = new_pos[None,:]\n\n    # Wrap condition for vertical axis\n    if self.boundary_condition in ['wrap', 'wrap_vertical']:\n        new_pos[new_pos[:,0] &lt; 0, 0] += self.padded_height\n        new_pos[new_pos[:,0] &gt;= self.padded_height, 0] -= self.padded_height\n\n    # Wrap condition for horizontal axis\n    if self.boundary_condition in ['wrap', 'wrap_horizontal']:\n        new_pos[new_pos[:,1] &lt; 0, 1] += self.padded_width\n        new_pos[new_pos[:,1] &gt;= self.padded_width, 1] -= self.padded_width\n\n    # Stop condition\n    if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_horizontal'):\n        new_pos[:,0] = xp.clip(new_pos[:,0], 0, (self.padded_height-1))\n\n    if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_vertical'):\n        new_pos[:,1] = xp.clip(new_pos[:,1], 0, (self.padded_width-1))\n\n    if is_single_point:\n        new_pos = new_pos[0]\n\n    return new_pos\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.plot","title":"<code>plot(frame=0, ax=None)</code>","text":"<p>Simple function to plot the environment</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>An ax on which the environment can be plot</p> <code>None</code> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def plot(self, frame:int=0, ax:plt.Axes=None) -&gt; None:\n    '''\n    Simple function to plot the environment\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        An ax on which the environment can be plot\n    '''\n    # If on GPU use the CPU version to plot\n    if self.on_gpu:\n        self._alternate_version.plot(\n            frame=frame,\n            ax=ax\n        )\n        return\n\n    if ax is None:\n        _, ax = plt.subplots(1, figsize=(15,5))\n\n    # Odor grid\n    odor = plt.Rectangle([0,0], 1, 1, color='black', fill=True)\n    ax.imshow((self.grid[frame] &gt; (self.odor_present_threshold if self.odor_present_threshold is not None else 0)).astype(float), cmap='Greys')\n\n    # Start zone contour\n    start_zone = plt.Rectangle([0,0], 1, 1, color='blue', fill=False)\n    ax.contour(self.start_probabilities, levels=[0.0], colors='blue')\n\n    # Source circle\n    goal_circle = plt.Circle(self.source_position[::-1], self.source_radius, color='r', fill=False)\n    ax.add_patch(goal_circle)\n\n    # Legend\n    ax.legend([odor, start_zone, goal_circle], [f'Frame {frame} odor cues', 'Start zone', 'Source'])\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.random_start_points","title":"<code>random_start_points(n=1)</code>","text":"<p>Function to generate n starting positions following the starting probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many random starting positions to generate</p> <code>1</code> <p>Returns:</p> Name Type Description <code>random_states_2d</code> <code>ndarray</code> <p>The n random 2d points in a n x 2 array.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def random_start_points(self,\n                        n:int=1\n                        ) -&gt; np.ndarray:\n    '''\n    Function to generate n starting positions following the starting probabilities.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many random starting positions to generate\n\n    Returns\n    -------\n    random_states_2d : np.ndarray\n        The n random 2d points in a n x 2 array. \n    '''\n    xp = cp if self.on_gpu else np\n\n    assert n&gt;0, \"n has to be a strictly positive number (&gt;0)\"\n\n    random_states = self.rnd_state.choice(xp.arange(self.padded_height * self.padded_width), size=n, replace=True, p=self.start_probabilities.ravel())\n    random_states_2d = xp.array(xp.unravel_index(random_states, (self.padded_height, self.padded_width))).T\n    return random_states_2d\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.save","title":"<code>save(folder=None, save_arrays=False, force=False)</code>","text":"<p>Function to save the environment to the memory.</p> <p>By default it saved in a new folder at the current path in a new folder with the name 'Env-' where  is the name set when initializing an environment. In this folder a file \"METADATA.json\" is created containing all the properties of the environment. <p>The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file. The arrays are saved to .npy files along with the METADATA file.</p> <p>If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder to which to save the environment data. If it is not provided, it will be created in the current folder.</p> <code>None</code> <code>save_arrays</code> <code>bool</code> <p>Whether or not to save the numpy arrays to memory. (The arrays can be heavy)</p> <code>False</code> <code>force</code> <code>bool</code> <p>In case an environment of the same name is already saved, it will be overwritten.</p> <code>False</code> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         save_arrays:bool=False,\n         force:bool=False\n         ) -&gt; None:\n    '''\n    Function to save the environment to the memory.\n\n    By default it saved in a new folder at the current path in a new folder with the name 'Env-&lt;name&gt;' where &lt;name&gt; is the name set when initializing an environment.\n    In this folder a file \"METADATA.json\" is created containing all the properties of the environment.\n\n    The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file.\n    The arrays are saved to .npy files along with the METADATA file.\n\n    If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.\n\n    Parameters\n    ----------\n    folder : str, optional\n        The folder to which to save the environment data. If it is not provided, it will be created in the current folder.\n    save_arrays : bool, default=False\n        Whether or not to save the numpy arrays to memory. (The arrays can be heavy)\n    force : bool, default=False\n        In case an environment of the same name is already saved, it will be overwritten.\n    '''\n    # If on gpu, use the cpu version to save\n    if self.on_gpu:\n        self._alternate_version.save(\n            folder=folder,\n            save_arrays=save_arrays,\n            force=force\n        )\n        return\n\n    # Assert either data_file is provided or save_arrays is enabled\n    assert save_arrays or ((self.source_data_file is not None) and (self.start_type is not None)), \"The environment was not created from a data file so 'save_arrays' has to be set to True.\"\n\n    # Adding env name to folder path\n    if folder is None:\n        folder = f'./Env-{self.name}'\n    else:\n        folder += '/Env-' + self.name\n\n    # Checking the folder exists or creates it\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    elif len(os.listdir(folder)) &gt; 0:\n        if force:\n            shutil.rmtree(folder)\n            os.mkdir(folder)\n        else:\n            raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n    # Generating the metadata arguments dictionary\n    arguments = {}\n    arguments['name'] = self.name\n\n    if self.source_data_file is not None:\n        arguments['source_data_file'] = self.source_data_file\n\n    arguments['width']                 = self.width\n    arguments['height']                = self.height\n    arguments['margins']               = self.margins.tolist()\n    arguments['padded_width']          = int(self.padded_width)\n    arguments['padded_height']         = int(self.padded_height)\n    arguments['shape']                 = [int(s) for s in self.shape]\n    arguments['discretization']        = self.discretization\n    arguments['data_source_position']  = self.data_source_position.tolist()\n    arguments['source_position']       = self.source_position.tolist()\n    arguments['source_radius']         = self.source_radius\n    arguments['boundary_condition']    = self.boundary_condition\n\n    if self.odor_present_threshold is not None:\n        arguments['odor_present_threshold'] = self.odor_present_threshold\n    if self.start_type is not None:\n        arguments['start_type'] = self.start_type\n\n    # Output the arguments to a METADATA file\n    with open(folder + '/METADATA.json', 'w') as json_file:\n        json.dump(arguments, json_file, indent=4)\n\n    # Output the numpy arrays\n    if save_arrays:\n        np.save(folder + '/grid.npy', self.grid)\n        np.save(folder + '/start_probabilities.npy', self.start_probabilities)\n\n    # Success print\n    self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n    print(f'Environment saved to: {folder}')\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.source_reached","title":"<code>source_reached(pos)</code>","text":"<p>Checks whether a given position is within the source radius.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The position to check whether in the radius of the source.</p> required <p>Returns:</p> Name Type Description <code>is_at_source</code> <code>bool</code> <p>Whether or not the position is within the radius of the source.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def source_reached(self,\n                   pos:np.ndarray\n                   ) -&gt; bool | np.ndarray:\n    '''\n    Checks whether a given position is within the source radius.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The position to check whether in the radius of the source.\n\n    Returns\n    -------\n    is_at_source : bool\n        Whether or not the position is within the radius of the source.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Handling the case of a single point\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        pos = pos[None,:]\n\n    is_at_source = (xp.sum((pos - self.source_position) ** 2, axis=1) &lt;= (self.source_radius ** 2))\n\n    return bool(is_at_source[0]) if is_single_point else is_at_source\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function to send the numpy arrays of the environment to the cpu memory. It returns a new instance of the Environment with the arrays as numpy arrays.</p> <p>Returns:</p> Name Type Description <code>cpu_environment</code> <code>Environment</code> <p>A new environment instance where the arrays are on the cpu memory.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def to_cpu(self) -&gt; 'Environment':\n    '''\n    Function to send the numpy arrays of the environment to the cpu memory.\n    It returns a new instance of the Environment with the arrays as numpy arrays.\n\n    Returns\n    -------\n    cpu_environment : Environment\n        A new environment instance where the arrays are on the cpu memory.\n    '''\n    if self.on_gpu:\n        assert self._alternate_version is not None, \"Something went wrong\"\n        return self._alternate_version\n\n    return self\n</code></pre>"},{"location":"reference/#olfactory_navigation.Environment.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the environment to the gpu memory. It returns a new instance of the Environment with the arrays as cupy arrays.</p> <p>Returns:</p> Name Type Description <code>gpu_environment</code> <code>Environment</code> <p>A new environment instance where the arrays are on the gpu memory.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def to_gpu(self) -&gt; 'Environment':\n    '''\n    Function to send the numpy arrays of the environment to the gpu memory.\n    It returns a new instance of the Environment with the arrays as cupy arrays.\n\n    Returns\n    -------\n    gpu_environment : Environment\n        A new environment instance where the arrays are on the gpu memory.\n    '''\n    assert gpu_support, \"GPU support is not enabled...\"\n\n    # Generating a new instance\n    cls = self.__class__\n    gpu_environment = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_environment, arg, cp.array(val))\n        elif arg == 'rnd_state':\n            setattr(gpu_environment, arg, cp.random.RandomState(self.seed))\n        else:\n            setattr(gpu_environment, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_environment\n    gpu_environment._alternate_version = self\n\n    gpu_environment.on_gpu = True\n    return gpu_environment\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory","title":"<code>SimulationHistory</code>","text":"<p>Class to represent a list of the steps that happened during a simulation with:     - the positions the agents pass by     - the actions the agents take     - the observations the agents receive ('observations')</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>start_points</code> <code>ndarray</code> <p>The initial points of the agents in the simulation.</p> required <code>environment</code> <code>Environment</code> <p>The environment on which the simulation is run (can be different from the one associated with the agent).</p> required <code>agent</code> <code>Agent</code> <p>The agent used in the simulation.</p> required <code>time_shift</code> <code>ndarray</code> <p>An array of time shifts in the simulation data.</p> required <code>reward_discount</code> <code>float</code> <p>A discount to be applied to the rewards received by the agent. (eg: reward of 1 received at time n would be: 1 * reward_discount^n)</p> <code>0.99</code> <p>Attributes:</p> Name Type Description <code>start_points</code> <code>ndarray</code> <code>environment</code> <code>Environment</code> <code>agent</code> <code>Agent</code> <code>time_shift</code> <code>ndarray</code> <code>reward_discount</code> <code>float</code> <code>n</code> <code>int</code> <p>The amount of simulations.</p> <code>start_time</code> <code>datetime</code> <p>The datetime the simulations start.</p> <code>actions</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n actions as dy,dx vectors.</p> <code>positions</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n positions as y,x vectors.</p> <code>observations</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n observations received by the agents.</p> <code>done_at_step</code> <code>ndarray</code> <p>A numpy array containing n elements that records when a given simulation reaches the source (-1 is not reached).</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>class SimulationHistory:\n    '''\n    Class to represent a list of the steps that happened during a simulation with:\n        - the positions the agents pass by\n        - the actions the agents take\n        - the observations the agents receive ('observations')\n\n    ...\n\n    Parameters\n    ----------\n    start_points : np.ndarray\n        The initial points of the agents in the simulation.\n    environment : Environment\n        The environment on which the simulation is run (can be different from the one associated with the agent).\n    agent : Agent\n        The agent used in the simulation.\n    time_shift : np.ndarray\n        An array of time shifts in the simulation data.\n    reward_discount : float, default=0.99\n        A discount to be applied to the rewards received by the agent. (eg: reward of 1 received at time n would be: 1 * reward_discount^n)\n\n    Attributes\n    ----------\n    start_points : np.ndarray\n    environment : Environment\n    agent : Agent\n    time_shift : np.ndarray\n    reward_discount : float\n    n : int\n        The amount of simulations.\n    start_time : datetime\n        The datetime the simulations start.\n    actions : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n actions as dy,dx vectors.\n    positions : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n positions as y,x vectors.\n    observations : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n observations received by the agents.\n    done_at_step : np.ndarray\n        A numpy array containing n elements that records when a given simulation reaches the source (-1 is not reached).\n    '''\n    def __init__(self,\n                 start_points:np.ndarray,\n                 environment:Environment,\n                 agent:Agent,\n                 time_shift:np.ndarray,\n                 reward_discount:float=0.99\n                 ) -&gt; None:\n        # If only on state is provided, we make it a 1x2 vector\n        if len(start_points.shape) == 1:\n            start_points = start_points[None,:]\n\n        # Fixed parameters\n        self.n = len(start_points)\n        self.environment = environment.to_cpu()\n        self.agent = agent.to_cpu()\n        self.time_shift = time_shift if gpu_support and cp.get_array_module(time_shift) == np else cp.asnumpy(time_shift)\n        self.reward_discount = reward_discount\n        self.start_time = datetime.now()\n\n        # Simulation Tracking\n        self.start_points = start_points if gpu_support and cp.get_array_module(start_points) == np else cp.asnumpy(start_points)\n        self.actions = []\n        self.positions = []\n        self.observations = []\n        self.timestamps = []\n\n        self._running_sims = np.arange(self.n)\n        self.done_at_step = np.full(self.n, fill_value=-1)\n\n        # Other parameters\n        self._simulation_dfs = None\n\n\n    def add_step(self,\n                 actions:np.ndarray,\n                 next_positions:np.ndarray,\n                 observations:np.ndarray,\n                 is_done:np.ndarray,\n                 interupt:np.ndarray\n                 ) -&gt; None:\n        '''\n        Function to add a step in the simulation history.\n\n        Parameters\n        ----------\n        actions : np.ndarray\n            The actions that were taken by the agents.\n        next_positions : np.ndarray\n            The positions that were reached by the agents after having taken actions.\n        observations : np.ndarray\n            The observations the agents receive after having taken actions.\n        is_done : np.ndarray\n            A boolean array of whether each agent has reached the source or not.\n        interupt : np.ndarray\n            A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.\n        '''\n        self._simulation_dfs = None\n\n        # Time tracking\n        self.timestamps.append(datetime.now())\n\n        # Handle case cupy arrays are provided\n        if gpu_support:\n            actions = actions if cp.get_array_module(actions) == np else cp.asnumpy(actions)\n            next_positions = next_positions if cp.get_array_module(next_positions) == np else cp.asnumpy(next_positions)\n            observations = observations if cp.get_array_module(observations) == np else cp.asnumpy(observations)\n            is_done = is_done if cp.get_array_module(is_done) == np else cp.asnumpy(is_done)\n            interupt = interupt if cp.get_array_module(interupt) == np else cp.asnumpy(interupt)\n\n        # Actions tracking\n        action_all_sims = np.full((self.n,2), fill_value=-1)\n        action_all_sims[self._running_sims] = actions\n        self.actions.append(action_all_sims)\n\n        # Next states tracking\n        next_position_all_sims = np.full((self.n, 2), fill_value=-1)\n        next_position_all_sims[self._running_sims] = next_positions\n        self.positions.append(next_position_all_sims)\n\n        # Observation tracking\n        observation_all_sims = np.full((self.n,), fill_value=-1, dtype=float)\n        observation_all_sims[self._running_sims] = observations\n        self.observations.append(observation_all_sims)\n\n        # Recording at which step the simulation is done if it is done\n        self.done_at_step[self._running_sims[is_done]] = len(self.positions)\n\n        # Updating the list of running sims\n        self._running_sims = self._running_sims[~is_done &amp; ~interupt]\n\n\n    @property\n    def analysis_df(self) -&gt; pd.DataFrame:\n        '''\n        A Pandas DataFrame analyzing the results of the simulations.\n        It aggregates the simulations in single rows, recording:\n         - y_start and x_start: The x, y starting positions\n         - optimal_steps_count: The minimal amount of steps to reach the source\n         - converged:           Whether or not the simulation reached the source\n         - steps_taken:         The amount of steps the agent took to reach the source, (horizon if the simulation did not reach the source)\n         - discounted_rewards:  The discounted reward received by the agent over the course of the simulation\n         - extra_steps:         The amount of extra steps compared to the optimal trajectory\n         - t_min_over_t:         normalized version of the extra steps measure, where it tends to 1 the least amount of time the agent took to reach the source compared to an optimal trajectory.\n\n        For the measures (converged, steps_taken, discounted_rewards, extra_steps, t_min_over_t), the average and standard deviations are computed in rows at the top.\n        '''\n        # Dataframe creation\n        df = pd.DataFrame(self.start_points, columns=['y_start', 'x_start'])\n        df['optimal_steps_count'] = self.environment.distance_to_source(self.start_points)\n        df['converged'] = self.done_at_step &gt;= 0\n        df['steps_taken'] = np.where(df['converged'], self.done_at_step, len(self.positions))\n        df['discounted_rewards'] = self.reward_discount ** df['steps_taken']\n        df['extra_steps'] = df['steps_taken'] - df['optimal_steps_count']\n        df['t_min_over_t'] = df['optimal_steps_count'] / df['steps_taken']\n\n        # Reindex\n        runs_list = [f'run_{i}' for i in range(self.n)]\n        df.index = runs_list\n\n        # Analysis aggregations\n        columns_to_analyze = ['converged', 'steps_taken', 'discounted_rewards', 'extra_steps', 't_min_over_t']\n        success_averages = df.loc[df['converged'], columns_to_analyze].mean()\n        succes_std = df.loc[df['converged'], columns_to_analyze].std()\n\n        df.loc['mean', columns_to_analyze] = df[columns_to_analyze].mean()\n        df.loc['standard_deviation', columns_to_analyze] = df[columns_to_analyze].std()\n\n        df.loc['success_mean', columns_to_analyze] = success_averages\n        df.loc['success_standard_deviation', columns_to_analyze] = succes_std\n\n        # Bringing analysis rows to top\n        df = df.reindex([\n            'mean',\n            'standard_deviation',\n            'success_mean',\n            'success_standard_deviation',\n            *runs_list])\n\n        return df\n\n\n    @property\n    def summary(self) -&gt; str:\n        '''\n        A string summarizing the performances of all the simulations.\n        The metrics used are averages of:\n         - Step count\n         - Extra steps\n         - Discounted rewards\n         - Tmin / T\n\n        Along with the respective the standard deviations and equally for only for the successful simulations.\n        '''\n        done_sim_count = np.sum(self.done_at_step &gt;= 0)\n        summary_str = f'Simulations reached goal: {done_sim_count}/{self.n} ({self.n-done_sim_count} failures) ({(done_sim_count*100)/self.n:.2f}%)'\n\n        if done_sim_count == 0:\n            return summary_str\n\n        # Metrics\n        df = self.analysis_df\n\n        summary_str += f\"\\n\\t- Average step count: {df.loc['mean','steps_taken']:.3f} +- {df.loc['standard_deviation','steps_taken']:.2f} \"\n        summary_str += f\"(Successfull only: {df.loc['success_mean','steps_taken']:.3f} +- {df.loc['success_standard_deviation','steps_taken']:.2f})\"\n\n        summary_str += f\"\\n\\t- Extra steps: {df.loc['mean','extra_steps']:.3f} +- {df.loc['standard_deviation','extra_steps']:.2f} \"\n        summary_str += f\"(Successful only: {df.loc['success_mean','extra_steps']:.3f} +- {df.loc['success_standard_deviation','extra_steps']:.2f})\"\n\n        summary_str += f\"\\n\\t- Average discounted rewards (ADR): {df.loc['mean','discounted_rewards']:.3f} +- {df.loc['standard_deviation','discounted_rewards']:.2f} \"\n        summary_str += f\"(Successfull only: {df.loc['success_mean','discounted_rewards']:.3f} +- {df.loc['success_standard_deviation','discounted_rewards']:.2f})\"\n\n        summary_str += f\"\\n\\t- Tmin/T: {df.loc['mean','t_min_over_t']:.3f} +- {df.loc['standard_deviation','t_min_over_t']:.2f} \"\n        summary_str += f\"(Successful only: {df.loc['success_mean','t_min_over_t']:.3f} +- {df.loc['success_standard_deviation','t_min_over_t']:.2f})\"\n\n        return summary_str\n\n\n    @property\n    def simulation_dfs(self) -&gt; list[pd.DataFrame]:\n        '''\n        A list of the pandas DataFrame where each dataframe is a single simulation history.\n        Each row is a different time instant of simulation process with each column being:\n         - time (of the simulation data)\n         - x\n         - y\n         - dx\n         - dy\n         - o (pure, not thresholded)\n         - done (boolean)\n        '''\n        if self._simulation_dfs is None:\n            self._simulation_dfs = []\n\n            # Converting state, actions and observation to numpy arrays\n            states_array = np.array(self.positions)\n            action_array = np.array(self.actions)\n            observation_array = np.array(self.observations)\n\n            for i in range(self.n):\n                length = self.done_at_step[i] if self.done_at_step[i] &gt;= 0 else len(states_array)\n\n                df = {\n                    'time':  np.arange(length+1) + self.time_shift[i],\n                    'y':     np.hstack([self.start_points[i,0], states_array[:length, i, 0]]),\n                    'x':     np.hstack([self.start_points[i,1], states_array[:length, i, 1]]),\n                    'dy':    np.hstack([[None], action_array[:length, i, 0]]),\n                    'dx':    np.hstack([[None], action_array[:length, i, 1]]),\n                    'o':     np.hstack([[None], observation_array[:length, i]]),\n                    'done':  np.hstack([[None], np.where(np.arange(1,length+1) == self.done_at_step[i], 1, 0)])\n                }\n\n                # Append\n                self._simulation_dfs.append(pd.DataFrame(df))\n\n        return self._simulation_dfs\n\n\n    def save(self,\n             file:str|None=None,\n             folder:str|None=None,\n             save_analysis:bool=True,\n             save_components:bool=False\n             ) -&gt; None:\n        '''\n        Function to save the simulation history to a csv file in a given folder.\n        Additionally, an analysis of the runs can be saved if the save_analysis is enabled.\n        The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.\n\n        Parameters\n        ----------\n        file : str, optional\n            The name of the file the simulation histories will be saved to.\n            If it is not provided, it will be by default \"Simulations-&lt;env_name&gt;-n_&lt;sim_count&gt;-&lt;sim_start_timestamp&gt;-horizon_&lt;max_sim_length&gt;.csv\"\n        folder : str, optional\n            Folder to save the simulation histories to.\n            If the folder name is not provided the current folder will be used.\n        save_analysis : bool, default=True\n            Whether to save an additional csv file with an analysis of the runs of the simulation.\n            It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken.\n            The means and standard deviations of all the runs are also computed.\n            The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.\n        save_components : bool, default=False\n            Whether or not to save the environment and agent along with the simulation histories in the given folder.\n        '''\n        # Handle file name\n        if file is None:\n            env_name = f's_{self.environment.shape[0]}_{self.environment.shape[1]}'\n            file = f'Simulations-{env_name}-n_{self.n}-{self.start_time.strftime(\"%m%d%Y_%H%M%S\")}-horizon_{len(self.positions)}.csv'\n\n        if not file.endswith('.csv'):\n            file += '.csv'\n\n        # Handle folder\n        if folder is None:\n            folder = './'\n\n        if '/' not in folder:\n            folder = './' + folder\n\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n\n        if not folder.endswith('/'):\n            folder += '/'\n\n        # Save components if requested\n        if save_components:\n            if (self.environment.saved_at is None) or (folder not in self.environment.saved_at):\n                self.environment.save(folder=folder)\n\n            if (self.agent.saved_at is None) or (folder not in self.agent.saved_at):\n                self.agent.save(folder=folder)\n\n        # Create csv file\n        combined_df = pd.concat(self.simulation_dfs)\n\n        # Adding Environment and Agent info\n        padding = [None] * len(combined_df)\n        combined_df['timestamps'] = [ts.strftime('%H%M%S%f') for ts in self.timestamps] + padding[:-len(self.timestamps)]\n        combined_df['reward_discount'] = [self.reward_discount] + padding[:-1]\n        combined_df['environment'] = [self.environment.name, self.environment.saved_at] + padding[:-2]\n        combined_df['agent'] = [self.agent.name, self.agent.class_name, self.agent.saved_at] + padding[:-3]\n\n        # Saving csv\n        combined_df.to_csv(folder + file, index=False)\n\n        print(f'Simulations saved to: {folder + file}')\n\n        if save_analysis:\n            analysis_file = file.replace('.csv', '-analysis.csv')\n            self.analysis_df.to_csv(folder + analysis_file)\n\n            print(f\"Simulation's analysis saved to: {folder + analysis_file}\")\n\n\n    @classmethod\n    def load_from_file(cls,\n                       file:str,\n                       environment:Environment|None=None,\n                       agent:Agent|None=None\n                       ) -&gt; 'SimulationHistory':\n        '''\n        Function to load the simulation history from a file.\n        This can be useful to use the plot functions on the simulations saved in succh file.\n\n        The environment and agent can provided as a backup in the case they cannot be loaded from the file.\n\n        Parameters\n        ----------\n        file : str\n            A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)\n        environment : Environment, optional\n            An environment instance to be linked with the simulation history object.\n            If an environment can be loaded from the path found in the file, this parameter will be ignored.\n            But if this loading fails and no environment is provided, the loading will fail.\n        agent : Agent, optional\n            An agent instance to be linked with the simulation history object.\n            If an agent can be loaded from the path found in the file, this parameter will be ignored.\n            But if this loading fails and no agent is provided, the loading will fail.\n\n        Returns\n        -------\n        hist : SimulationHistory\n            The loaded instance of a simulation history object.\n        '''\n        combined_df = pd.read_csv(file, dtype={\n            'time':             int,\n            'y':                float,\n            'x':                float,\n            'dy':               float,\n            'dx':               float,\n            'o':                float,\n            'done':             float,\n            'reward_discount':  str,\n            'environment':      str,\n            'agent':            str\n        })\n\n        # Retrieving reward discount\n        reward_discount = combined_df['reward_discount'][0]\n\n        # Retrieving environment\n        loaded_environment = None\n        environment_name = combined_df['environment'][0]\n        if combined_df['environment'][1] is not None:\n            try:\n                loaded_environment = Environment.load(combined_df['environment'][1])\n            except:\n                print(f'Failed to retrieve \"{environment_name}\" environment from memory')\n\n        if loaded_environment is not None:\n            print(f'Environment \"{environment_name}\" loaded from memory' + (' (Ignoring environment provided as a parameter)' if environment is not None else ''))\n            environment = loaded_environment\n\n        if environment is None:\n            raise Exception('No environment could be linked, the simulation history cannot be instanciated. Provide an environment to resolve this.')\n\n        # Retrieving agent\n        loaded_agent = None\n        agent_name = combined_df['agent'][0]\n        agent_class = combined_df['agent'][1]\n        if combined_df['agent'][2] is not None:\n            try:\n                for (class_name, class_obj) in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n                    if class_name == agent_class:\n                        loaded_agent = class_obj.load(combined_df['agent'][2])\n                        break\n            except:\n                print(f'Failed to retrieve \"{agent_name}\" agent from memory')\n\n        if loaded_agent is not None:\n            print(f'Agent \"{agent_name}\" loaded from memory' + (' (Ignoring agent provided as a parameter)' if agent is not None else ''))\n            agent = loaded_agent\n\n        if agent is None:\n            raise Exception('No agent could be linked, the simulation history cannot be instanciated. Provide an agent to resolve this.')\n\n        # Columns to retrieve\n        columns = [\n            'time',\n            'y',\n            'x',\n            'dy',\n            'dx',\n            'o',\n            'done'\n        ]\n\n        # Recreation of list of simulations\n        simulation_dfs = []\n        sim_start_rows = [None] + np.argwhere(combined_df[['done']].isnull())[1:,0].tolist() + [None]\n        n = len(sim_start_rows)-1\n\n        for i in range(n):\n            simulation_dfs.append(combined_df[columns].iloc[sim_start_rows[i]:sim_start_rows[i+1]])\n\n        # Gathering start states\n        start_points = np.array([sim[['y', 'x']].iloc[0] for sim in simulation_dfs])\n        time_shift = np.array([sim['time'].iloc[0] for sim in simulation_dfs])\n\n        # Generation of SimHist instance\n        hist = SimulationHistory(\n            start_points=start_points,\n            environment=environment,\n            agent=agent,\n            time_shift=time_shift,\n            reward_discount=reward_discount\n        )\n\n        max_length = max(len(sim) for sim in simulation_dfs)\n\n        # Recreating action, state and observations\n        positions = np.full((max_length-1, n, 2), -1)\n        actions = np.full((max_length-1, n, 2), -1)\n        observations = np.full((max_length-1, n), -1)\n        done_at_step = np.full((n,), -1)\n\n        for i, sim in enumerate(simulation_dfs):\n            positions[:len(sim)-1, i, :] = sim[['y','x']].to_numpy()[1:]\n            actions[:len(sim)-1, i, :] = sim[['dy','dx']].to_numpy()[1:]\n            observations[:len(sim)-1, i] = sim[['o']].to_numpy()[1:,0]\n            done_at_step[i] = len(sim)-1 if sim['done'].iloc[-1] == 1 else -1\n\n        hist.positions = [arr for arr in positions]\n        hist.actions = [arr for arr in actions]\n        hist.observations = [arr for arr in observations]\n        hist.done_at_step = done_at_step\n        hist.timestamps = [datetime.strptime(str(int(ts)), '%H%M%S%f') for ts in combined_df['timestamps'][:max_length-1]]\n\n        # Saving simulation dfs back\n        hist._simulation_dfs = simulation_dfs\n\n        return hist\n\n\n    def plot(self,\n             sim_id:int=0,\n             ax:plt.Axes=None\n             ) -&gt; None:\n        '''\n        Function to plot a the trajectory of a given simulation.\n        An ax can be use to plot it on.\n\n        Parameters\n        ----------\n        sim_id : int, default=0\n            The id of the simulation to plot.\n        ax : plt.Axes, optional\n            The ax on which to plot the path. (If not provided, a new axis will be created)\n        '''\n        # Generate ax is not provided\n        if ax is None:\n            _, ax = plt.subplots(figsize=(18,3))\n\n        # Retrieving sim\n        sim = self.simulation_dfs[sim_id]\n\n        # Plot setup\n        env_shape = self.environment.shape\n        ax.imshow(np.zeros(self.environment.shape), cmap='Greys', zorder=-100)\n        ax.set_xlim(0, env_shape[1])\n        ax.set_ylim(env_shape[0], 0)\n\n        # Start\n        start_coord = sim[['x', 'y']].to_numpy()[0]\n        ax.scatter(start_coord[0], start_coord[1], c='green', label='Start')\n\n        # Source circle\n        goal_circle = plt.Circle(self.environment.source_position[::-1], self.environment.source_radius, color='r', fill=False, label='Source')\n        ax.add_patch(goal_circle)\n\n        # Until step\n        seq = sim[['x','y']][1:].to_numpy()\n\n        # Path\n        ax.plot(seq[:,0], seq[:,1], zorder=-1, c='black', label='Path')\n\n        # Something sensed\n        if self.agent is not None:\n            something_sensed = sim['o'][1:].to_numpy() &gt; self.agent.threshold\n            points_obs = seq[something_sensed,:]\n            ax.scatter(points_obs[:,0], points_obs[:,1], zorder=1, label='Something observed')\n        else:\n            print('Agent used is not tracked')\n\n        # Generate legend\n        ax.legend()\n\n\n    def plot_runtimes(self,\n                      ax:plt.Axes=None\n                      ) -&gt; None:\n        '''\n        Function to plot the runtimes over the iterations.\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            The ax on which to plot the path. (If not provided, a new axis will be created)\n        '''\n        # Generate ax is not provided\n        if ax is None:\n            _, ax = plt.subplots(figsize=(18,3))\n\n        # Computing differences\n        timestamp_differences_ms = np.diff(np.array([int(ts.strftime('%H%M%S%f')) for ts in self.timestamps])) / 1000\n\n        # Actual plot\n        ax.plot(timestamp_differences_ms)\n\n        # Axes\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Runtime (ms)')\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory.analysis_df","title":"<code>analysis_df: pd.DataFrame</code>  <code>property</code>","text":"<p>A Pandas DataFrame analyzing the results of the simulations. It aggregates the simulations in single rows, recording:  - y_start and x_start: The x, y starting positions  - optimal_steps_count: The minimal amount of steps to reach the source  - converged:           Whether or not the simulation reached the source  - steps_taken:         The amount of steps the agent took to reach the source, (horizon if the simulation did not reach the source)  - discounted_rewards:  The discounted reward received by the agent over the course of the simulation  - extra_steps:         The amount of extra steps compared to the optimal trajectory  - t_min_over_t:         normalized version of the extra steps measure, where it tends to 1 the least amount of time the agent took to reach the source compared to an optimal trajectory.</p> <p>For the measures (converged, steps_taken, discounted_rewards, extra_steps, t_min_over_t), the average and standard deviations are computed in rows at the top.</p>"},{"location":"reference/#olfactory_navigation.SimulationHistory.simulation_dfs","title":"<code>simulation_dfs: list[pd.DataFrame]</code>  <code>property</code>","text":"<p>A list of the pandas DataFrame where each dataframe is a single simulation history. Each row is a different time instant of simulation process with each column being:  - time (of the simulation data)  - x  - y  - dx  - dy  - o (pure, not thresholded)  - done (boolean)</p>"},{"location":"reference/#olfactory_navigation.SimulationHistory.summary","title":"<code>summary: str</code>  <code>property</code>","text":"<p>A string summarizing the performances of all the simulations. The metrics used are averages of:  - Step count  - Extra steps  - Discounted rewards  - Tmin / T</p> <p>Along with the respective the standard deviations and equally for only for the successful simulations.</p>"},{"location":"reference/#olfactory_navigation.SimulationHistory.add_step","title":"<code>add_step(actions, next_positions, observations, is_done, interupt)</code>","text":"<p>Function to add a step in the simulation history.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>ndarray</code> <p>The actions that were taken by the agents.</p> required <code>next_positions</code> <code>ndarray</code> <p>The positions that were reached by the agents after having taken actions.</p> required <code>observations</code> <code>ndarray</code> <p>The observations the agents receive after having taken actions.</p> required <code>is_done</code> <code>ndarray</code> <p>A boolean array of whether each agent has reached the source or not.</p> required <code>interupt</code> <code>ndarray</code> <p>A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.</p> required Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def add_step(self,\n             actions:np.ndarray,\n             next_positions:np.ndarray,\n             observations:np.ndarray,\n             is_done:np.ndarray,\n             interupt:np.ndarray\n             ) -&gt; None:\n    '''\n    Function to add a step in the simulation history.\n\n    Parameters\n    ----------\n    actions : np.ndarray\n        The actions that were taken by the agents.\n    next_positions : np.ndarray\n        The positions that were reached by the agents after having taken actions.\n    observations : np.ndarray\n        The observations the agents receive after having taken actions.\n    is_done : np.ndarray\n        A boolean array of whether each agent has reached the source or not.\n    interupt : np.ndarray\n        A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.\n    '''\n    self._simulation_dfs = None\n\n    # Time tracking\n    self.timestamps.append(datetime.now())\n\n    # Handle case cupy arrays are provided\n    if gpu_support:\n        actions = actions if cp.get_array_module(actions) == np else cp.asnumpy(actions)\n        next_positions = next_positions if cp.get_array_module(next_positions) == np else cp.asnumpy(next_positions)\n        observations = observations if cp.get_array_module(observations) == np else cp.asnumpy(observations)\n        is_done = is_done if cp.get_array_module(is_done) == np else cp.asnumpy(is_done)\n        interupt = interupt if cp.get_array_module(interupt) == np else cp.asnumpy(interupt)\n\n    # Actions tracking\n    action_all_sims = np.full((self.n,2), fill_value=-1)\n    action_all_sims[self._running_sims] = actions\n    self.actions.append(action_all_sims)\n\n    # Next states tracking\n    next_position_all_sims = np.full((self.n, 2), fill_value=-1)\n    next_position_all_sims[self._running_sims] = next_positions\n    self.positions.append(next_position_all_sims)\n\n    # Observation tracking\n    observation_all_sims = np.full((self.n,), fill_value=-1, dtype=float)\n    observation_all_sims[self._running_sims] = observations\n    self.observations.append(observation_all_sims)\n\n    # Recording at which step the simulation is done if it is done\n    self.done_at_step[self._running_sims[is_done]] = len(self.positions)\n\n    # Updating the list of running sims\n    self._running_sims = self._running_sims[~is_done &amp; ~interupt]\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory.load_from_file","title":"<code>load_from_file(file, environment=None, agent=None)</code>  <code>classmethod</code>","text":"<p>Function to load the simulation history from a file. This can be useful to use the plot functions on the simulations saved in succh file.</p> <p>The environment and agent can provided as a backup in the case they cannot be loaded from the file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)</p> required <code>environment</code> <code>Environment</code> <p>An environment instance to be linked with the simulation history object. If an environment can be loaded from the path found in the file, this parameter will be ignored. But if this loading fails and no environment is provided, the loading will fail.</p> <code>None</code> <code>agent</code> <code>Agent</code> <p>An agent instance to be linked with the simulation history object. If an agent can be loaded from the path found in the file, this parameter will be ignored. But if this loading fails and no agent is provided, the loading will fail.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>The loaded instance of a simulation history object.</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>@classmethod\ndef load_from_file(cls,\n                   file:str,\n                   environment:Environment|None=None,\n                   agent:Agent|None=None\n                   ) -&gt; 'SimulationHistory':\n    '''\n    Function to load the simulation history from a file.\n    This can be useful to use the plot functions on the simulations saved in succh file.\n\n    The environment and agent can provided as a backup in the case they cannot be loaded from the file.\n\n    Parameters\n    ----------\n    file : str\n        A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)\n    environment : Environment, optional\n        An environment instance to be linked with the simulation history object.\n        If an environment can be loaded from the path found in the file, this parameter will be ignored.\n        But if this loading fails and no environment is provided, the loading will fail.\n    agent : Agent, optional\n        An agent instance to be linked with the simulation history object.\n        If an agent can be loaded from the path found in the file, this parameter will be ignored.\n        But if this loading fails and no agent is provided, the loading will fail.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        The loaded instance of a simulation history object.\n    '''\n    combined_df = pd.read_csv(file, dtype={\n        'time':             int,\n        'y':                float,\n        'x':                float,\n        'dy':               float,\n        'dx':               float,\n        'o':                float,\n        'done':             float,\n        'reward_discount':  str,\n        'environment':      str,\n        'agent':            str\n    })\n\n    # Retrieving reward discount\n    reward_discount = combined_df['reward_discount'][0]\n\n    # Retrieving environment\n    loaded_environment = None\n    environment_name = combined_df['environment'][0]\n    if combined_df['environment'][1] is not None:\n        try:\n            loaded_environment = Environment.load(combined_df['environment'][1])\n        except:\n            print(f'Failed to retrieve \"{environment_name}\" environment from memory')\n\n    if loaded_environment is not None:\n        print(f'Environment \"{environment_name}\" loaded from memory' + (' (Ignoring environment provided as a parameter)' if environment is not None else ''))\n        environment = loaded_environment\n\n    if environment is None:\n        raise Exception('No environment could be linked, the simulation history cannot be instanciated. Provide an environment to resolve this.')\n\n    # Retrieving agent\n    loaded_agent = None\n    agent_name = combined_df['agent'][0]\n    agent_class = combined_df['agent'][1]\n    if combined_df['agent'][2] is not None:\n        try:\n            for (class_name, class_obj) in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n                if class_name == agent_class:\n                    loaded_agent = class_obj.load(combined_df['agent'][2])\n                    break\n        except:\n            print(f'Failed to retrieve \"{agent_name}\" agent from memory')\n\n    if loaded_agent is not None:\n        print(f'Agent \"{agent_name}\" loaded from memory' + (' (Ignoring agent provided as a parameter)' if agent is not None else ''))\n        agent = loaded_agent\n\n    if agent is None:\n        raise Exception('No agent could be linked, the simulation history cannot be instanciated. Provide an agent to resolve this.')\n\n    # Columns to retrieve\n    columns = [\n        'time',\n        'y',\n        'x',\n        'dy',\n        'dx',\n        'o',\n        'done'\n    ]\n\n    # Recreation of list of simulations\n    simulation_dfs = []\n    sim_start_rows = [None] + np.argwhere(combined_df[['done']].isnull())[1:,0].tolist() + [None]\n    n = len(sim_start_rows)-1\n\n    for i in range(n):\n        simulation_dfs.append(combined_df[columns].iloc[sim_start_rows[i]:sim_start_rows[i+1]])\n\n    # Gathering start states\n    start_points = np.array([sim[['y', 'x']].iloc[0] for sim in simulation_dfs])\n    time_shift = np.array([sim['time'].iloc[0] for sim in simulation_dfs])\n\n    # Generation of SimHist instance\n    hist = SimulationHistory(\n        start_points=start_points,\n        environment=environment,\n        agent=agent,\n        time_shift=time_shift,\n        reward_discount=reward_discount\n    )\n\n    max_length = max(len(sim) for sim in simulation_dfs)\n\n    # Recreating action, state and observations\n    positions = np.full((max_length-1, n, 2), -1)\n    actions = np.full((max_length-1, n, 2), -1)\n    observations = np.full((max_length-1, n), -1)\n    done_at_step = np.full((n,), -1)\n\n    for i, sim in enumerate(simulation_dfs):\n        positions[:len(sim)-1, i, :] = sim[['y','x']].to_numpy()[1:]\n        actions[:len(sim)-1, i, :] = sim[['dy','dx']].to_numpy()[1:]\n        observations[:len(sim)-1, i] = sim[['o']].to_numpy()[1:,0]\n        done_at_step[i] = len(sim)-1 if sim['done'].iloc[-1] == 1 else -1\n\n    hist.positions = [arr for arr in positions]\n    hist.actions = [arr for arr in actions]\n    hist.observations = [arr for arr in observations]\n    hist.done_at_step = done_at_step\n    hist.timestamps = [datetime.strptime(str(int(ts)), '%H%M%S%f') for ts in combined_df['timestamps'][:max_length-1]]\n\n    # Saving simulation dfs back\n    hist._simulation_dfs = simulation_dfs\n\n    return hist\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory.plot","title":"<code>plot(sim_id=0, ax=None)</code>","text":"<p>Function to plot a the trajectory of a given simulation. An ax can be use to plot it on.</p> <p>Parameters:</p> Name Type Description Default <code>sim_id</code> <code>int</code> <p>The id of the simulation to plot.</p> <code>0</code> <code>ax</code> <code>Axes</code> <p>The ax on which to plot the path. (If not provided, a new axis will be created)</p> <code>None</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def plot(self,\n         sim_id:int=0,\n         ax:plt.Axes=None\n         ) -&gt; None:\n    '''\n    Function to plot a the trajectory of a given simulation.\n    An ax can be use to plot it on.\n\n    Parameters\n    ----------\n    sim_id : int, default=0\n        The id of the simulation to plot.\n    ax : plt.Axes, optional\n        The ax on which to plot the path. (If not provided, a new axis will be created)\n    '''\n    # Generate ax is not provided\n    if ax is None:\n        _, ax = plt.subplots(figsize=(18,3))\n\n    # Retrieving sim\n    sim = self.simulation_dfs[sim_id]\n\n    # Plot setup\n    env_shape = self.environment.shape\n    ax.imshow(np.zeros(self.environment.shape), cmap='Greys', zorder=-100)\n    ax.set_xlim(0, env_shape[1])\n    ax.set_ylim(env_shape[0], 0)\n\n    # Start\n    start_coord = sim[['x', 'y']].to_numpy()[0]\n    ax.scatter(start_coord[0], start_coord[1], c='green', label='Start')\n\n    # Source circle\n    goal_circle = plt.Circle(self.environment.source_position[::-1], self.environment.source_radius, color='r', fill=False, label='Source')\n    ax.add_patch(goal_circle)\n\n    # Until step\n    seq = sim[['x','y']][1:].to_numpy()\n\n    # Path\n    ax.plot(seq[:,0], seq[:,1], zorder=-1, c='black', label='Path')\n\n    # Something sensed\n    if self.agent is not None:\n        something_sensed = sim['o'][1:].to_numpy() &gt; self.agent.threshold\n        points_obs = seq[something_sensed,:]\n        ax.scatter(points_obs[:,0], points_obs[:,1], zorder=1, label='Something observed')\n    else:\n        print('Agent used is not tracked')\n\n    # Generate legend\n    ax.legend()\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory.plot_runtimes","title":"<code>plot_runtimes(ax=None)</code>","text":"<p>Function to plot the runtimes over the iterations.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The ax on which to plot the path. (If not provided, a new axis will be created)</p> <code>None</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def plot_runtimes(self,\n                  ax:plt.Axes=None\n                  ) -&gt; None:\n    '''\n    Function to plot the runtimes over the iterations.\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        The ax on which to plot the path. (If not provided, a new axis will be created)\n    '''\n    # Generate ax is not provided\n    if ax is None:\n        _, ax = plt.subplots(figsize=(18,3))\n\n    # Computing differences\n    timestamp_differences_ms = np.diff(np.array([int(ts.strftime('%H%M%S%f')) for ts in self.timestamps])) / 1000\n\n    # Actual plot\n    ax.plot(timestamp_differences_ms)\n\n    # Axes\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Runtime (ms)')\n</code></pre>"},{"location":"reference/#olfactory_navigation.SimulationHistory.save","title":"<code>save(file=None, folder=None, save_analysis=True, save_components=False)</code>","text":"<p>Function to save the simulation history to a csv file in a given folder. Additionally, an analysis of the runs can be saved if the save_analysis is enabled. The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The name of the file the simulation histories will be saved to. If it is not provided, it will be by default \"Simulations--n_--horizon_.csv\" <code>None</code> <code>folder</code> <code>str</code> <p>Folder to save the simulation histories to. If the folder name is not provided the current folder will be used.</p> <code>None</code> <code>save_analysis</code> <code>bool</code> <p>Whether to save an additional csv file with an analysis of the runs of the simulation. It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken. The means and standard deviations of all the runs are also computed. The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.</p> <code>True</code> <code>save_components</code> <code>bool</code> <p>Whether or not to save the environment and agent along with the simulation histories in the given folder.</p> <code>False</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def save(self,\n         file:str|None=None,\n         folder:str|None=None,\n         save_analysis:bool=True,\n         save_components:bool=False\n         ) -&gt; None:\n    '''\n    Function to save the simulation history to a csv file in a given folder.\n    Additionally, an analysis of the runs can be saved if the save_analysis is enabled.\n    The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.\n\n    Parameters\n    ----------\n    file : str, optional\n        The name of the file the simulation histories will be saved to.\n        If it is not provided, it will be by default \"Simulations-&lt;env_name&gt;-n_&lt;sim_count&gt;-&lt;sim_start_timestamp&gt;-horizon_&lt;max_sim_length&gt;.csv\"\n    folder : str, optional\n        Folder to save the simulation histories to.\n        If the folder name is not provided the current folder will be used.\n    save_analysis : bool, default=True\n        Whether to save an additional csv file with an analysis of the runs of the simulation.\n        It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken.\n        The means and standard deviations of all the runs are also computed.\n        The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.\n    save_components : bool, default=False\n        Whether or not to save the environment and agent along with the simulation histories in the given folder.\n    '''\n    # Handle file name\n    if file is None:\n        env_name = f's_{self.environment.shape[0]}_{self.environment.shape[1]}'\n        file = f'Simulations-{env_name}-n_{self.n}-{self.start_time.strftime(\"%m%d%Y_%H%M%S\")}-horizon_{len(self.positions)}.csv'\n\n    if not file.endswith('.csv'):\n        file += '.csv'\n\n    # Handle folder\n    if folder is None:\n        folder = './'\n\n    if '/' not in folder:\n        folder = './' + folder\n\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n\n    if not folder.endswith('/'):\n        folder += '/'\n\n    # Save components if requested\n    if save_components:\n        if (self.environment.saved_at is None) or (folder not in self.environment.saved_at):\n            self.environment.save(folder=folder)\n\n        if (self.agent.saved_at is None) or (folder not in self.agent.saved_at):\n            self.agent.save(folder=folder)\n\n    # Create csv file\n    combined_df = pd.concat(self.simulation_dfs)\n\n    # Adding Environment and Agent info\n    padding = [None] * len(combined_df)\n    combined_df['timestamps'] = [ts.strftime('%H%M%S%f') for ts in self.timestamps] + padding[:-len(self.timestamps)]\n    combined_df['reward_discount'] = [self.reward_discount] + padding[:-1]\n    combined_df['environment'] = [self.environment.name, self.environment.saved_at] + padding[:-2]\n    combined_df['agent'] = [self.agent.name, self.agent.class_name, self.agent.saved_at] + padding[:-3]\n\n    # Saving csv\n    combined_df.to_csv(folder + file, index=False)\n\n    print(f'Simulations saved to: {folder + file}')\n\n    if save_analysis:\n        analysis_file = file.replace('.csv', '-analysis.csv')\n        self.analysis_df.to_csv(folder + analysis_file)\n\n        print(f\"Simulation's analysis saved to: {folder + analysis_file}\")\n</code></pre>"},{"location":"reference/#olfactory_navigation.run_test","title":"<code>run_test(agent, n=None, start_points=None, environment=None, time_shift=0, time_loop=True, horizon=1000, reward_discount=0.99, print_progress=True, print_stats=True, use_gpu=False)</code>","text":"<p>Function to run n simulations for a given agent in its environment (or a given modified environment). The simulations start either from random start points or provided trough the start_points parameter. The simulation can have shifted initial times (in the olfactory simulation).</p> <p>The simulation will run for at most 'horizon' steps, after which the simulations will be considered failed.</p> <p>Some statistics can be printed at end of the simulation with the 'print_stats' parameter. It will print some performance statisitcs about the simulations such as the average discounter reward. The reward discount can be set by the 'reward_discount' parameter.</p> <p>To speedup the simulations, it can be run on the gpu by toggling the 'use_gpu' parameter. This will have the consequence to send the various arrays to the gpu memory. This will only work if the agent has the support for to work with cupy arrays.</p> <p>This method returns a SimulationHistory object that saves all the positions the agent went through, the actions the agent took, and the observation the agent received. It also provides the possibility the save the results to a csv file and plot the various trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to be tested</p> required <code>n</code> <code>int</code> <p>How many simulation to run in parallel. n is optional but it needs to match with what is provided in start_points.</p> <code>None</code> <code>start_points</code> <code>ndarray</code> <p>The starting points of the simulation in 2d space. If not provided, n random points will be generated based on the start probabilities of the environment. Else, the amount of start_points need to match to n, if it is provided.</p> <code>None</code> <code>environment</code> <code>Environment</code> <p>The environment to run the simulations in. By default, the environment linked to the agent will used. This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.</p> <code>None</code> <code>time_shift</code> <code>int or ndarray</code> <p>The time at which to start the olfactory simulation array. It can be either a single value, or n values.</p> <code>0</code> <code>time_loop</code> <code>bool</code> <p>Whether to loop the time if reaching the end. (starts back at 0)</p> <code>True</code> <code>horizon</code> <code>int</code> <p>The amount of steps to run the simulation for before killing the remaining simulations.</p> <code>1000</code> <code>reward_discount</code> <code>float</code> <p>How much a given reward is discounted based on how long it took to get it. It is purely used to compute the Average Discount Reward (ADR) after the simulation.</p> <code>0.99</code> <code>print_progress</code> <code>bool</code> <p>Wheter to show a progress bar of what step the simulations are at.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Wheter to print the stats at the end of the run.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run the simulations on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>A SimulationHistory object that tracked all the positions, actions and observations.</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def run_test(agent:Agent,\n             n:int|None=None,\n             start_points:np.ndarray|None=None,\n             environment:Environment|None=None,\n             time_shift:int|np.ndarray=0,\n             time_loop:bool=True,\n             horizon:int=1000,\n             reward_discount:float=0.99,\n             print_progress:bool=True,\n             print_stats:bool=True,\n             use_gpu:bool=False\n             ) -&gt; SimulationHistory:\n    '''\n    Function to run n simulations for a given agent in its environment (or a given modified environment).\n    The simulations start either from random start points or provided trough the start_points parameter.\n    The simulation can have shifted initial times (in the olfactory simulation).\n\n    The simulation will run for at most 'horizon' steps, after which the simulations will be considered failed.\n\n    Some statistics can be printed at end of the simulation with the 'print_stats' parameter.\n    It will print some performance statisitcs about the simulations such as the average discounter reward.\n    The reward discount can be set by the 'reward_discount' parameter.\n\n    To speedup the simulations, it can be run on the gpu by toggling the 'use_gpu' parameter.\n    This will have the consequence to send the various arrays to the gpu memory.\n    This will only work if the agent has the support for to work with cupy arrays.\n\n    This method returns a SimulationHistory object that saves all the positions the agent went through,\n    the actions the agent took, and the observation the agent received.\n    It also provides the possibility the save the results to a csv file and plot the various trajectories.\n\n    Parameters\n    ----------\n    agent : Agent\n        The agent to be tested\n    n : int, optional\n        How many simulation to run in parallel.\n        n is optional but it needs to match with what is provided in start_points.\n    start_points : np.ndarray, optional\n        The starting points of the simulation in 2d space.\n        If not provided, n random points will be generated based on the start probabilities of the environment.\n        Else, the amount of start_points need to match to n, if it is provided.\n    environment : Environment, optional\n        The environment to run the simulations in.\n        By default, the environment linked to the agent will used.\n        This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.\n    time_shift : int or np.ndarray, default=0\n        The time at which to start the olfactory simulation array.\n        It can be either a single value, or n values.\n    time_loop : bool, default=True\n        Whether to loop the time if reaching the end. (starts back at 0)\n    horizon : int, default=1000\n        The amount of steps to run the simulation for before killing the remaining simulations.\n    reward_discount : float, default=0.99\n        How much a given reward is discounted based on how long it took to get it.\n        It is purely used to compute the Average Discount Reward (ADR) after the simulation.\n    print_progress : bool, default=True\n        Wheter to show a progress bar of what step the simulations are at.\n    print_stats : bool, default=True\n        Wheter to print the stats at the end of the run.\n    use_gpu : bool, default=False\n        Whether to run the simulations on the GPU or not.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        A SimulationHistory object that tracked all the positions, actions and observations.\n    '''\n    # Gathering n\n    if n is None:\n        if (start_points is None) or (len(start_points.shape) == 1):\n            n = 1\n        else:\n            n = len(start_points)\n\n    # Handle the case an specific environment is given\n    if environment is not None:\n        assert environment.shape == agent.environment.shape, \"The provided environment's shape doesn't match the environment has been trained on...\"\n        print('Using the provided environment, not the agent environment.')\n    else:\n        environment = agent.environment\n\n    # Timeshift\n    if isinstance(time_shift, int):\n        time_shift = np.ones(n) * time_shift\n    else:\n        time_shift = np.array(time_shift)\n        assert time_shift.shape == (n,), f\"time_shift array has a wrong shape (Given: {time_shift.shape}, expected ({n},))\"\n    time_shift = time_shift.astype(int)\n\n    # Move things to GPU if needed\n    if use_gpu:\n        assert gpu_support, f\"GPU support is not enabled, the use_gpu option is not available.\"\n\n        # Move instances to GPU\n        agent = agent.to_gpu()\n        environment = environment.to_gpu()\n        time_shift = cp.array(time_shift)\n\n        if start_points is not None:\n            start_points = cp.array(start_points)\n\n    # Set start positions\n    agent_position = None\n    if start_points is not None:\n        assert start_points.shape == (n, 2), 'The provided start_points are of the wrong shape'\n        agent_position = start_points\n    else:\n        # Generating random starts\n        agent_position = environment.random_start_points(n)\n\n    # Initialize agent's state\n    agent.initialize_state(n)\n\n    # Create simulation history tracker\n    hist = SimulationHistory(\n        start_points=agent_position,\n        environment=environment,\n        agent=agent,\n        time_shift=time_shift,\n        reward_discount=reward_discount\n    )\n\n    # Track begin of simulation ts\n    sim_start_ts = datetime.now()\n\n    # Simulation loop\n    iterator = trange(horizon) if print_progress else range(horizon)\n    for i in iterator:\n        # Letting agent choose the action to take based on it's curent state\n        action = agent.choose_action()\n\n        # Updating the agent's actual position (hidden to him)\n        new_agent_position = environment.move(agent_position, action)\n\n        # Get an observation based on the new position of the agent\n        observation = environment.get_observation(new_agent_position, time=(time_shift + i))\n\n        # Check if the source is reached\n        source_reached = environment.source_reached(new_agent_position)\n\n        # Return the observation to the agent\n        agent.update_state(observation, source_reached)\n\n        # Handling the case where simulations have reached the end\n        sims_at_end = ((time_shift + i + 1) &gt;= (math.inf if time_loop else len(environment.grid)))\n\n        # Interupt agents that reached the end\n        agent_position = new_agent_position[~source_reached &amp; ~sims_at_end]\n        time_shift = time_shift[~source_reached &amp; ~sims_at_end]\n        agent.kill(simulations_to_kill=sims_at_end[~source_reached])\n\n        # Send the values to the tracker\n        hist.add_step(\n            actions=action,\n            next_positions=new_agent_position,\n            observations=observation,\n            is_done=source_reached,\n            interupt=sims_at_end\n        )\n\n        # Early stopping if all agents done\n        if len(agent_position) == 0:\n            break\n\n        # Update progress bar\n        if print_progress:\n            done_count = n-len(agent_position)\n            iterator.set_postfix({'done ': f' {done_count} of {n} ({(done_count*100)/n:.1f}%)'})\n\n    # If requested print the simulation start\n    if print_stats:\n        sim_end_ts = datetime.now()\n        print(f'Simulations done in {(sim_end_ts - sim_start_ts).total_seconds():.3f}s:')\n        print(hist.summary)\n\n    return hist\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>olfactory_navigation<ul> <li>agent</li> <li>agents<ul> <li>fsvi_agent</li> <li>hsvi_agent</li> <li>infotaxis_agent</li> <li>model_based_util<ul> <li>belief</li> <li>belief_value_mapping</li> <li>mdp</li> <li>pomdp</li> <li>value_function</li> <li>vi_solver</li> </ul> </li> <li>pbvi_agent</li> <li>pbvi_ger_agent</li> <li>pbvi_ra_agent</li> <li>pbvi_ssea_agent</li> <li>pbvi_ssga_agent</li> <li>pbvi_ssra_agent</li> <li>perseus_agent</li> <li>q_agent</li> <li>qmdp_agent</li> </ul> </li> <li>environment</li> <li>simulation</li> <li>test_setups</li> </ul> </li> </ul>"},{"location":"reference/agent/","title":"agent","text":""},{"location":"reference/agent/#olfactory_navigation.agent.Agent","title":"<code>Agent</code>","text":"<p>Generic agent class</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>class Agent:\n    '''\n    Generic agent class\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        self.environment = environment\n        self.threshold = threshold\n\n        # setup name\n        if name is None:\n            self.name = self.class_name\n            self.name += f'-tresh_{self.threshold}'\n        else:\n            self.name = name\n\n        self.saved_at = None\n\n        self.on_gpu = False\n        self._alternate_version = None\n\n\n    @property\n    def class_name(self):\n        return self.__class__.__name__\n\n\n    def to_gpu(self) -&gt; 'Agent':\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n        '''\n        raise NotImplementedError('The to_gpu function is not implemented, make an agent subclass to implement the method')\n\n\n    def to_cpu(self) -&gt; 'Agent':\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        cpu_agent : Agent\n            A new environment instance where the arrays are on the cpu memory.\n        '''\n        if self.on_gpu:\n            assert self._alternate_version is not None, \"Something went wrong\"\n            return self._alternate_version\n\n        return self\n\n\n    def train(self) -&gt; None:\n        '''\n        Function to call the particular flavour of training of the agent.\n        '''\n        raise NotImplementedError('The train function is not implemented, make an agent subclass to implement the method')\n\n\n    def save(self,\n             folder:str|None=None,\n             force:bool=False,\n             save_environment:bool=False\n             ) -&gt; None:\n        '''\n        Function to save a trained agent to memory.\n        '''\n        raise NotImplementedError('The save function is not implemented, make an agent subclass to implement the method')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'Agent':\n        '''\n        Function to load a trained agent from memory.\n        '''\n        from olfactory_navigation import agents\n\n        for name, obj in inspect.getmembers(agents):\n            if inspect.isclass(obj) and (name in folder) and issubclass(obj, cls) and (obj != cls):\n                return obj.load(folder)\n\n        raise NotImplementedError('The load function is not implemented, make an agent subclass to implement the method')\n\n\n    def initialize_state(self, n:int=1) -&gt; None:\n        '''\n        Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents to initialize.\n        '''\n        raise NotImplementedError('The initialize_state function is not implemented, make an agent subclass to implement the method')\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to allow for the agent to choose an action to take based on its current state.\n        It then stores this action in its state.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A vector in 2D space of the movement the agent will take\n        '''\n        raise NotImplementedError('The choose_action function is not implemented, make an agent subclass to implement the method')\n\n\n    def update_state(self,\n                     observation:int|np.ndarray,\n                     source_reached:bool|np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state of the agent based on the previous action taken and the observation received.\n        '''\n        raise NotImplementedError('The update_state function is not implemented, make an agent subclass to implement the method')\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that still haven't reached the source\n        '''\n        raise NotImplementedError('The kill function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to allow for the agent to choose an action to take based on its current state. It then stores this action in its state.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A vector in 2D space of the movement the agent will take</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to allow for the agent to choose an action to take based on its current state.\n    It then stores this action in its state.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A vector in 2D space of the movement the agent will take\n    '''\n    raise NotImplementedError('The choose_action function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents to initialize.</p> <code>1</code> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def initialize_state(self, n:int=1) -&gt; None:\n    '''\n    Function to initialize the state of the agent. Which is meant to contain concepts such as the \"memory\" or \"belief\" of the agent.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents to initialize.\n    '''\n    raise NotImplementedError('The initialize_state function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that still haven't reached the source</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that still haven't reached the source\n    '''\n    raise NotImplementedError('The kill function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load a trained agent from memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'Agent':\n    '''\n    Function to load a trained agent from memory.\n    '''\n    from olfactory_navigation import agents\n\n    for name, obj in inspect.getmembers(agents):\n        if inspect.isclass(obj) and (name in folder) and issubclass(obj, cls) and (obj != cls):\n            return obj.load(folder)\n\n    raise NotImplementedError('The load function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.save","title":"<code>save(folder=None, force=False, save_environment=False)</code>","text":"<p>Function to save a trained agent to memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         force:bool=False,\n         save_environment:bool=False\n         ) -&gt; None:\n    '''\n    Function to save a trained agent to memory.\n    '''\n    raise NotImplementedError('The save function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Name Type Description <code>cpu_agent</code> <code>Agent</code> <p>A new environment instance where the arrays are on the cpu memory.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def to_cpu(self) -&gt; 'Agent':\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    cpu_agent : Agent\n        A new environment instance where the arrays are on the cpu memory.\n    '''\n    if self.on_gpu:\n        assert self._alternate_version is not None, \"Something went wrong\"\n        return self._alternate_version\n\n    return self\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def to_gpu(self) -&gt; 'Agent':\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n    '''\n    raise NotImplementedError('The to_gpu function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.train","title":"<code>train()</code>","text":"<p>Function to call the particular flavour of training of the agent.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def train(self) -&gt; None:\n    '''\n    Function to call the particular flavour of training of the agent.\n    '''\n    raise NotImplementedError('The train function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agent/#olfactory_navigation.agent.Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state of the agent based on the previous action taken and the observation received.</p> Source code in <code>olfactory_navigation\\agent.py</code> <pre><code>def update_state(self,\n                 observation:int|np.ndarray,\n                 source_reached:bool|np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state of the agent based on the previous action taken and the observation received.\n    '''\n    raise NotImplementedError('The update_state function is not implemented, make an agent subclass to implement the method')\n</code></pre>"},{"location":"reference/environment/","title":"environment","text":""},{"location":"reference/environment/#olfactory_navigation.environment.Environment","title":"<code>Environment</code>","text":"<p>Class to represent an olfactory environment.</p> <p>It is defined based on an olfactory data set.</p> ----------------- |                       | <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or ndarray</code> <p>The dataset containing the olfactory data. It can be provided as a path to a file containing said array.</p> required <code>source_position</code> <code>list or ndarray</code> <p>The center point of the source provided as a list or a 1D array with the components being x,y.</p> required <code>source_radius</code> <code>int</code> <p>The radius from the center point of the source in which we consider the agent has reached the source.</p> <code>1</code> <code>discretization</code> <code>int</code> <p>How many units should be kept in the final array (a discretization of 2 will retain every other point of the original array).</p> <code>1</code> <code>margins</code> <code>int or list or ndarray</code> <p>How many discretized units have to be added to the data as margins. If a unique element is provided, the margin will be this same value on each side. If a list or array of 2 elements is provided, the first number will be vertical margins (y-axis), while the other will be on the x-axis (horizontal).</p> <code>0</code> <code>boundary_condition</code> <code>stop or wrap or wrap_vertical or wrap_horizontal or clip</code> <p>How the agent should behave at the boundary. Stop means for the agent to stop at the boundary, if the agent tries to move north while being on the top edge, it will stay in the same state. Wrap means for the borders to be like portals, when entering on one side, it reappears on the other side. Wrap can be specified to be only vertically or horizontally</p> <code>'stop'</code> <code>start_zone</code> <code>Literal['odor_present', 'data_zone'] | ndarray</code> <code>'data_zone'</code> <code>odor_present_threshold</code> <code>float | None</code> <code>None</code> <code>name</code> <code>str | None</code> <code>None</code> Arguments Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>class Environment:\n    '''\n    Class to represent an olfactory environment.\n\n    It is defined based on an olfactory data set.\n\n    -------------------------\n    |                       |\n    |   -----------------   |\n    |   |               |   |\n    |   -----------------   |\n    |                       |\n    -------------------------\n\n    # TODO: Add support for a 'real' grid, eg in meters... \n\n    margins can be provided as:\n    - An equal margin on each side\n    - A array of 2 elements for x and y margins\n    - A 2D array for each element being [axis, side] where axis is [vertical, horizontal] and side is [L,R]\n\n    ...\n    # TODO Write these\n    Parameters\n    ----------\n    data : str or np.ndarray\n        The dataset containing the olfactory data. It can be provided as a path to a file containing said array.\n    source_position : list or np.ndarray\n        The center point of the source provided as a list or a 1D array with the components being x,y.\n    source_radius : int, default=1\n        The radius from the center point of the source in which we consider the agent has reached the source.\n    discretization : int, default=1\n        How many units should be kept in the final array (a discretization of 2 will retain every other point of the original array).\n    margins : int or list or np.ndarray, default=0\n        How many discretized units have to be added to the data as margins.\n        If a unique element is provided, the margin will be this same value on each side.\n        If a list or array of 2 elements is provided, the first number will be vertical margins (y-axis), while the other will be on the x-axis (horizontal).\n    boundary_condition : 'stop' or 'wrap' or 'wrap_vertical' or 'wrap_horizontal' or 'clip', default='stop'\n        How the agent should behave at the boundary.\n        Stop means for the agent to stop at the boundary, if the agent tries to move north while being on the top edge, it will stay in the same state.\n        Wrap means for the borders to be like portals, when entering on one side, it reappears on the other side.\n        Wrap can be specified to be only vertically or horizontally\n    start_zone:Literal['odor_present','data_zone']|np.ndarray='data_zone',\n    odor_present_threshold:float|None=None,\n    name:str|None=None\n\n    Arguments\n    ---------\n\n    '''\n    def __init__(self,\n                 data:str|np.ndarray,\n                 source_position:list|np.ndarray,\n                 source_radius:int=1,\n                 discretization:int=1,\n                 margins:int|list|np.ndarray=0,\n                 boundary_condition:Literal['stop', 'wrap', 'wrap_vertical', 'wrap_horizontal', 'clip' ,'no']='stop',\n                 start_zone:Literal['odor_present','data_zone']|np.ndarray='data_zone',\n                 odor_present_threshold:float|None=None,\n                 name:str|None=None,\n                 seed : int = 12131415,\n                 ) -&gt; None:\n        self.saved_at = None\n\n        # Load from file if string provided\n        self.source_data_file = data if isinstance(data, str) else None\n        if isinstance(data, str):\n            data_file = data\n            if data_file.endswith('.npy'):\n                data = np.load(data_file)\n            else:\n                raise NotImplementedError('File format loading not implemented')\n\n        # Making margins a 2x2 array \n        if isinstance(margins, int):\n            self.margins = np.ones((2,2)) * margins\n        elif isinstance(margins, list) or (margins.shape == (2,)):\n            assert len(margins) == 2, 'Margins, if provided as a list must contain only two elements.'\n            margins = np.array(margins)\n            self.margins = np.hstack((margins[:,None], margins[:,None]))\n        elif margins.shape == (2,2):\n            self.margins = margins\n        else:\n            raise ValueError('margins argument should be either an integer or a 1D or 2D array with either shape (2) or (2,2)')\n        assert self.margins.dtype == int, 'margins should be integers'\n\n        # Reading shape of data array\n        timesteps, self.height, self.width = data.shape\n        self.padded_height = self.height + np.sum(self.margins[0])\n        self.padded_width = self.width + np.sum(self.margins[1])\n        self.shape = (self.padded_height, self.padded_width)\n\n        # Preprocess data with discretization\n        self.discretization = discretization\n        if discretization != 1:\n            raise NotImplementedError('Different discretizations have not been implemented yet') # TODO\n        self.grid : np.ndarray = data\n\n        # Apply margins to grid\n        self.grid = np.hstack([np.zeros((timesteps, self.margins[0,0], self.width)), self.grid, np.zeros((timesteps, self.margins[0,1], self.width))])\n        self.grid = np.dstack([np.zeros((timesteps, self.padded_height, self.margins[1,0])), self.grid, np.zeros((timesteps, self.padded_height, self.margins[1,1]))])\n\n        # Saving arguments\n        self.data_source_position = np.array(source_position)\n        self.source_position = self.data_source_position + self.margins[:,0]\n        self.source_radius = source_radius\n        self.boundary_condition = boundary_condition\n\n        # Starting zone\n        self.start_probabilities = np.zeros(self.shape)\n        if start_zone == 'data_zone':\n            self.start_probabilities[self.margins[0,0]:self.margins[0,0]+self.height, self.margins[1,0]:self.margins[1,0]+self.width] = 1.0\n        elif start_zone == 'odor_present':\n            self.start_probabilities = (np.mean((self.grid &gt; (odor_present_threshold if odor_present_threshold is not None else 0)).astype(int), axis=0) &gt; 0).astype(float)\n        elif isinstance(start_zone, np.ndarray):\n            if start_zone.shape == (2,2):\n                self.start_probabilities[start_zone[0,0]:start_zone[0,1], start_zone[1,0]:start_zone[1,1]] = 1.0\n            elif start_zone.shape == self.shape:\n                self.start_probabilities = start_zone\n            else:\n                raise ValueError('If an np.ndarray is provided for the start_zone it has to be 2x2...')\n        else:\n            raise ValueError('start_zone value is wrong')\n\n        self.start_type = start_zone\n\n        # Odor present tresh\n        self.odor_present_threshold = odor_present_threshold\n\n        # Removing the source area from the starting zone\n        source_mask = np.fromfunction(lambda x,y: ((x - self.source_position[0])**2 + (y - self.source_position[1])**2) &lt;= self.source_radius**2, shape=self.shape)\n        self.start_probabilities[source_mask] = 0\n\n        self.start_probabilities /= np.sum(self.start_probabilities)\n\n        # Name\n        self.name = name\n        if self.name is None:\n            self.name =  f'{self.padded_height}_{self.padded_width}' # Size of env\n            self.name += f'-edge_{self.boundary_condition}' # Boundary condition\n            self.name += f'-start_{self.start_type if self.start_type is not None else \"custom\"}' # Start zone\n            self.name += f'-source_{self.source_position[0]}_{self.source_position[1]}_radius{self.source_radius}' # Source\n\n        # gpu support\n        self._alternate_version = None\n        self.on_gpu = False\n\n        # random state\n        self.seed = seed\n        self.rnd_state = np.random.RandomState(seed = seed)\n\n\n    def plot(self, frame:int=0, ax:plt.Axes=None) -&gt; None:\n        '''\n        Simple function to plot the environment\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            An ax on which the environment can be plot\n        '''\n        # If on GPU use the CPU version to plot\n        if self.on_gpu:\n            self._alternate_version.plot(\n                frame=frame,\n                ax=ax\n            )\n            return\n\n        if ax is None:\n            _, ax = plt.subplots(1, figsize=(15,5))\n\n        # Odor grid\n        odor = plt.Rectangle([0,0], 1, 1, color='black', fill=True)\n        ax.imshow((self.grid[frame] &gt; (self.odor_present_threshold if self.odor_present_threshold is not None else 0)).astype(float), cmap='Greys')\n\n        # Start zone contour\n        start_zone = plt.Rectangle([0,0], 1, 1, color='blue', fill=False)\n        ax.contour(self.start_probabilities, levels=[0.0], colors='blue')\n\n        # Source circle\n        goal_circle = plt.Circle(self.source_position[::-1], self.source_radius, color='r', fill=False)\n        ax.add_patch(goal_circle)\n\n        # Legend\n        ax.legend([odor, start_zone, goal_circle], [f'Frame {frame} odor cues', 'Start zone', 'Source'])\n\n\n    def get_observation(self,\n                        pos:np.ndarray,\n                        time:int|np.ndarray=0\n                        ) -&gt; float|np.ndarray:\n        '''\n        Function to get an observation at a given position on the grid at a given time.\n        A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The position or list of positions to get observations at.\n        time : int or np.ndarray, default=0\n            A timestamp or list of timestamps to get the observations at.\n\n        Returns\n        -------\n        observation : float or np.ndarray\n            A single observation or list of observations.\n        '''\n        # Handling the case of a single point\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            pos = pos[None,:]\n\n        # Time looping\n        time = time % len(self.grid)\n\n        # Handle the case where the agent is allowed to be outside the grid\n        if self.boundary_condition is None or self.boundary_condition == 'no':\n            if is_single_point:\n                return float(self.grid[time, pos[0], pos[1]] ) if  0 &lt;= pos[0] &lt; self.grid.shape[1] and 0 &lt;= pos[1] &lt; self.grid.shape[2] else 0.0\n            mask = (0 &lt;= pos[:, 0]) &amp; (pos[:, 0] &lt; self.grid.shape[1]) &amp; (0 &lt;= pos[:, 1]) &amp; (pos[:, 1] &lt; self.grid.shape[2])\n            observation = np.zeros((mask.shape[0], ))\n            if isinstance(time, int):\n                observation[mask] = self.grid[time, pos[mask,0], pos[mask,1]]\n            else:\n                observation[mask] = self.grid[time[mask], pos[mask,0], pos[mask,1]]\n            return observation\n\n        observation = self.grid[time, pos[0], pos[1]] if len(pos.shape) == 1 else self.grid[time, pos[:,0], pos[:,1]]\n\n        return float(observation[0]) if is_single_point else observation\n\n\n    def source_reached(self,\n                       pos:np.ndarray\n                       ) -&gt; bool | np.ndarray:\n        '''\n        Checks whether a given position is within the source radius.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The position to check whether in the radius of the source.\n\n        Returns\n        -------\n        is_at_source : bool\n            Whether or not the position is within the radius of the source.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Handling the case of a single point\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            pos = pos[None,:]\n\n        is_at_source = (xp.sum((pos - self.source_position) ** 2, axis=1) &lt;= (self.source_radius ** 2))\n\n        return bool(is_at_source[0]) if is_single_point else is_at_source\n\n\n    def random_start_points(self,\n                            n:int=1\n                            ) -&gt; np.ndarray:\n        '''\n        Function to generate n starting positions following the starting probabilities.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many random starting positions to generate\n\n        Returns\n        -------\n        random_states_2d : np.ndarray\n            The n random 2d points in a n x 2 array. \n        '''\n        xp = cp if self.on_gpu else np\n\n        assert n&gt;0, \"n has to be a strictly positive number (&gt;0)\"\n\n        random_states = self.rnd_state.choice(xp.arange(self.padded_height * self.padded_width), size=n, replace=True, p=self.start_probabilities.ravel())\n        random_states_2d = xp.array(xp.unravel_index(random_states, (self.padded_height, self.padded_width))).T\n        return random_states_2d\n\n\n    def move(self,\n             pos:np.ndarray,\n             movement:np.ndarray\n             ) -&gt; np.ndarray:\n        '''\n        Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.\n\n        Parameters\n        ----------\n        pos : np.ndarray\n            The start position of the movement.\n        movement : np.ndarray\n            A 2D movement vector.\n\n        Returns\n        -------\n        new_pos : np.ndarray\n            The new position after applying the movement.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Applying the movement vector\n        new_pos = pos + movement\n\n        # Handling the case we are dealing with a single point.\n        is_single_point = (len(pos.shape) == 1)\n        if is_single_point:\n            new_pos = new_pos[None,:]\n\n        # Wrap condition for vertical axis\n        if self.boundary_condition in ['wrap', 'wrap_vertical']:\n            new_pos[new_pos[:,0] &lt; 0, 0] += self.padded_height\n            new_pos[new_pos[:,0] &gt;= self.padded_height, 0] -= self.padded_height\n\n        # Wrap condition for horizontal axis\n        if self.boundary_condition in ['wrap', 'wrap_horizontal']:\n            new_pos[new_pos[:,1] &lt; 0, 1] += self.padded_width\n            new_pos[new_pos[:,1] &gt;= self.padded_width, 1] -= self.padded_width\n\n        # Stop condition\n        if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_horizontal'):\n            new_pos[:,0] = xp.clip(new_pos[:,0], 0, (self.padded_height-1))\n\n        if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_vertical'):\n            new_pos[:,1] = xp.clip(new_pos[:,1], 0, (self.padded_width-1))\n\n        if is_single_point:\n            new_pos = new_pos[0]\n\n        return new_pos\n\n\n    def distance_to_source(self,\n                           point:np.ndarray,\n                           metric:Literal['manhattan']='manhattan'\n                           ) -&gt; float | np.ndarray:\n        '''\n        Function to compute the distance(s) between given points and the source point.\n\n        Parameters\n        ----------\n        point : np.ndarray\n            A single or an Nx2 array containing N points.\n        metric : 'manhattan'\n            The metric to use to compute the distance.\n\n        Returns\n        -------\n        dist : float or np.ndarray\n            A single distance or a list of distance in a 1D distance array.\n        '''\n        xp = cp if self.on_gpu else np\n\n        # Handling the case we have a single point\n        is_single_point = (len(point.shape) == 1)\n        if is_single_point:\n            point = point[None,:]\n\n        # Computing dist\n        dist = None\n        if metric == 'manhattan':\n            dist = xp.sum(xp.abs(self.source_position[None,:] - point), axis=1) - self.source_radius\n        else:\n            raise NotImplementedError('This distance metric has not yet been implemented')\n\n        return float(dist[0]) if is_single_point else dist\n\n\n    def save(self,\n             folder:str|None=None,\n             save_arrays:bool=False,\n             force:bool=False\n             ) -&gt; None:\n        '''\n        Function to save the environment to the memory.\n\n        By default it saved in a new folder at the current path in a new folder with the name 'Env-&lt;name&gt;' where &lt;name&gt; is the name set when initializing an environment.\n        In this folder a file \"METADATA.json\" is created containing all the properties of the environment.\n\n        The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file.\n        The arrays are saved to .npy files along with the METADATA file.\n\n        If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.\n\n        Parameters\n        ----------\n        folder : str, optional\n            The folder to which to save the environment data. If it is not provided, it will be created in the current folder.\n        save_arrays : bool, default=False\n            Whether or not to save the numpy arrays to memory. (The arrays can be heavy)\n        force : bool, default=False\n            In case an environment of the same name is already saved, it will be overwritten.\n        '''\n        # If on gpu, use the cpu version to save\n        if self.on_gpu:\n            self._alternate_version.save(\n                folder=folder,\n                save_arrays=save_arrays,\n                force=force\n            )\n            return\n\n        # Assert either data_file is provided or save_arrays is enabled\n        assert save_arrays or ((self.source_data_file is not None) and (self.start_type is not None)), \"The environment was not created from a data file so 'save_arrays' has to be set to True.\"\n\n        # Adding env name to folder path\n        if folder is None:\n            folder = f'./Env-{self.name}'\n        else:\n            folder += '/Env-' + self.name\n\n        # Checking the folder exists or creates it\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n        elif len(os.listdir(folder)) &gt; 0:\n            if force:\n                shutil.rmtree(folder)\n                os.mkdir(folder)\n            else:\n                raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n        # Generating the metadata arguments dictionary\n        arguments = {}\n        arguments['name'] = self.name\n\n        if self.source_data_file is not None:\n            arguments['source_data_file'] = self.source_data_file\n\n        arguments['width']                 = self.width\n        arguments['height']                = self.height\n        arguments['margins']               = self.margins.tolist()\n        arguments['padded_width']          = int(self.padded_width)\n        arguments['padded_height']         = int(self.padded_height)\n        arguments['shape']                 = [int(s) for s in self.shape]\n        arguments['discretization']        = self.discretization\n        arguments['data_source_position']  = self.data_source_position.tolist()\n        arguments['source_position']       = self.source_position.tolist()\n        arguments['source_radius']         = self.source_radius\n        arguments['boundary_condition']    = self.boundary_condition\n\n        if self.odor_present_threshold is not None:\n            arguments['odor_present_threshold'] = self.odor_present_threshold\n        if self.start_type is not None:\n            arguments['start_type'] = self.start_type\n\n        # Output the arguments to a METADATA file\n        with open(folder + '/METADATA.json', 'w') as json_file:\n            json.dump(arguments, json_file, indent=4)\n\n        # Output the numpy arrays\n        if save_arrays:\n            np.save(folder + '/grid.npy', self.grid)\n            np.save(folder + '/start_probabilities.npy', self.start_probabilities)\n\n        # Success print\n        self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n        print(f'Environment saved to: {folder}')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'Environment':\n        '''\n        Function to load an environment from a given folder.\n\n        Parameters\n        ----------\n        folder : str\n            The folder of the Environment.\n\n        Returns\n        -------\n        loaded_env : Environment\n            The loaded environment.\n        '''\n        assert os.path.exists(folder), \"Folder doesn't exist...\"\n        assert folder.split('/')[-1].startswith('Env-'), \"The folder provided is not the data of en Environment object.\"\n\n        # Load arguments\n        arguments = None\n        with open(folder + '/METADATA.json', 'r') as json_file:\n            arguments = json.load(json_file)\n\n        # Check if numpy arrays are provided, if not, recreate a new environment model\n        if os.path.exists(folder + '/grid.npy') and os.path.exists(folder + '/start_probabilities.npy'):\n            grid = np.load(folder + '/grid.npy')\n            start_probabilities = np.load(folder + '/start_probabilities.npy')\n\n            loaded_env = cls.__new__(cls)\n\n            # Set the arguments\n            loaded_env.width                 = arguments['width']\n            loaded_env.height                = arguments['height']\n            loaded_env.margins               = np.array(arguments['margins'])\n            loaded_env.padded_width          = arguments['padded_width']\n            loaded_env.padded_height         = arguments['padded_height']\n            loaded_env.shape                 = set(arguments['shape'])\n            loaded_env.discretization        = arguments['discretization']\n            loaded_env.data_source_position  = np.array(arguments['data_source_position'])\n            loaded_env.source_position       = np.array(arguments['source_position'])\n            loaded_env.source_radius         = arguments['source_radius']\n            loaded_env.boundary_condition    = arguments['boundary_condition']\n\n            # Optional arguments\n            loaded_env.source_data_file      = arguments.get('source_data_file')\n            loaded_env.odor_present_threshold = arguments.get('odor_present_threshold')\n            loaded_env.start_type            = arguments.get('start_type')\n\n            # Arrays\n            loaded_env.grid = grid\n            loaded_env.start_probabilities = start_probabilities\n\n        else:\n            loaded_env = Environment(\n                data                  = arguments['source_data_file'],\n                source_position       = np.array(arguments['data_source_position']),\n                source_radius         = arguments['source_radius'],\n                discretization        = arguments['discretization'],\n                margins               = np.array(arguments['margins']),\n                boundary_condition    = arguments['boundary_condition'],\n                start_zone            = arguments.get('start_type'),\n                odor_present_threshold = arguments.get('odor_present_threshold'),\n                name                  = arguments['name']\n            )\n\n        # Folder where the environment was pulled from\n        loaded_env.saved_at = os.path.abspath(folder)\n\n        return loaded_env\n\n\n    def to_gpu(self) -&gt; 'Environment':\n        '''\n        Function to send the numpy arrays of the environment to the gpu memory.\n        It returns a new instance of the Environment with the arrays as cupy arrays.\n\n        Returns\n        -------\n        gpu_environment : Environment\n            A new environment instance where the arrays are on the gpu memory.\n        '''\n        assert gpu_support, \"GPU support is not enabled...\"\n\n        # Generating a new instance\n        cls = self.__class__\n        gpu_environment = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_environment, arg, cp.array(val))\n            elif arg == 'rnd_state':\n                setattr(gpu_environment, arg, cp.random.RandomState(self.seed))\n            else:\n                setattr(gpu_environment, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_environment\n        gpu_environment._alternate_version = self\n\n        gpu_environment.on_gpu = True\n        return gpu_environment\n\n\n    def to_cpu(self) -&gt; 'Environment':\n        '''\n        Function to send the numpy arrays of the environment to the cpu memory.\n        It returns a new instance of the Environment with the arrays as numpy arrays.\n\n        Returns\n        -------\n        cpu_environment : Environment\n            A new environment instance where the arrays are on the cpu memory.\n        '''\n        if self.on_gpu:\n            assert self._alternate_version is not None, \"Something went wrong\"\n            return self._alternate_version\n\n        return self\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment--todo-add-support-for-a-real-grid-eg-in-meters","title":"TODO: Add support for a 'real' grid, eg in meters...","text":"<p>margins can be provided as: - An equal margin on each side - A array of 2 elements for x and y margins - A 2D array for each element being [axis, side] where axis is [vertical, horizontal] and side is [L,R]</p> <p>...</p>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment--todo-write-these","title":"TODO Write these","text":""},{"location":"reference/environment/#olfactory_navigation.environment.Environment.distance_to_source","title":"<code>distance_to_source(point, metric='manhattan')</code>","text":"<p>Function to compute the distance(s) between given points and the source point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>ndarray</code> <p>A single or an Nx2 array containing N points.</p> required <code>metric</code> <code>manhattan</code> <p>The metric to use to compute the distance.</p> <code>'manhattan'</code> <p>Returns:</p> Name Type Description <code>dist</code> <code>float or ndarray</code> <p>A single distance or a list of distance in a 1D distance array.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def distance_to_source(self,\n                       point:np.ndarray,\n                       metric:Literal['manhattan']='manhattan'\n                       ) -&gt; float | np.ndarray:\n    '''\n    Function to compute the distance(s) between given points and the source point.\n\n    Parameters\n    ----------\n    point : np.ndarray\n        A single or an Nx2 array containing N points.\n    metric : 'manhattan'\n        The metric to use to compute the distance.\n\n    Returns\n    -------\n    dist : float or np.ndarray\n        A single distance or a list of distance in a 1D distance array.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Handling the case we have a single point\n    is_single_point = (len(point.shape) == 1)\n    if is_single_point:\n        point = point[None,:]\n\n    # Computing dist\n    dist = None\n    if metric == 'manhattan':\n        dist = xp.sum(xp.abs(self.source_position[None,:] - point), axis=1) - self.source_radius\n    else:\n        raise NotImplementedError('This distance metric has not yet been implemented')\n\n    return float(dist[0]) if is_single_point else dist\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.get_observation","title":"<code>get_observation(pos, time=0)</code>","text":"<p>Function to get an observation at a given position on the grid at a given time. A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The position or list of positions to get observations at.</p> required <code>time</code> <code>int or ndarray</code> <p>A timestamp or list of timestamps to get the observations at.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>observation</code> <code>float or ndarray</code> <p>A single observation or list of observations.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def get_observation(self,\n                    pos:np.ndarray,\n                    time:int|np.ndarray=0\n                    ) -&gt; float|np.ndarray:\n    '''\n    Function to get an observation at a given position on the grid at a given time.\n    A set of observations can also be requested, either at a single position for multiple timestamps or with the same amoung of positions as timestamps provided.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The position or list of positions to get observations at.\n    time : int or np.ndarray, default=0\n        A timestamp or list of timestamps to get the observations at.\n\n    Returns\n    -------\n    observation : float or np.ndarray\n        A single observation or list of observations.\n    '''\n    # Handling the case of a single point\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        pos = pos[None,:]\n\n    # Time looping\n    time = time % len(self.grid)\n\n    # Handle the case where the agent is allowed to be outside the grid\n    if self.boundary_condition is None or self.boundary_condition == 'no':\n        if is_single_point:\n            return float(self.grid[time, pos[0], pos[1]] ) if  0 &lt;= pos[0] &lt; self.grid.shape[1] and 0 &lt;= pos[1] &lt; self.grid.shape[2] else 0.0\n        mask = (0 &lt;= pos[:, 0]) &amp; (pos[:, 0] &lt; self.grid.shape[1]) &amp; (0 &lt;= pos[:, 1]) &amp; (pos[:, 1] &lt; self.grid.shape[2])\n        observation = np.zeros((mask.shape[0], ))\n        if isinstance(time, int):\n            observation[mask] = self.grid[time, pos[mask,0], pos[mask,1]]\n        else:\n            observation[mask] = self.grid[time[mask], pos[mask,0], pos[mask,1]]\n        return observation\n\n    observation = self.grid[time, pos[0], pos[1]] if len(pos.shape) == 1 else self.grid[time, pos[:,0], pos[:,1]]\n\n    return float(observation[0]) if is_single_point else observation\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load an environment from a given folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder of the Environment.</p> required <p>Returns:</p> Name Type Description <code>loaded_env</code> <code>Environment</code> <p>The loaded environment.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'Environment':\n    '''\n    Function to load an environment from a given folder.\n\n    Parameters\n    ----------\n    folder : str\n        The folder of the Environment.\n\n    Returns\n    -------\n    loaded_env : Environment\n        The loaded environment.\n    '''\n    assert os.path.exists(folder), \"Folder doesn't exist...\"\n    assert folder.split('/')[-1].startswith('Env-'), \"The folder provided is not the data of en Environment object.\"\n\n    # Load arguments\n    arguments = None\n    with open(folder + '/METADATA.json', 'r') as json_file:\n        arguments = json.load(json_file)\n\n    # Check if numpy arrays are provided, if not, recreate a new environment model\n    if os.path.exists(folder + '/grid.npy') and os.path.exists(folder + '/start_probabilities.npy'):\n        grid = np.load(folder + '/grid.npy')\n        start_probabilities = np.load(folder + '/start_probabilities.npy')\n\n        loaded_env = cls.__new__(cls)\n\n        # Set the arguments\n        loaded_env.width                 = arguments['width']\n        loaded_env.height                = arguments['height']\n        loaded_env.margins               = np.array(arguments['margins'])\n        loaded_env.padded_width          = arguments['padded_width']\n        loaded_env.padded_height         = arguments['padded_height']\n        loaded_env.shape                 = set(arguments['shape'])\n        loaded_env.discretization        = arguments['discretization']\n        loaded_env.data_source_position  = np.array(arguments['data_source_position'])\n        loaded_env.source_position       = np.array(arguments['source_position'])\n        loaded_env.source_radius         = arguments['source_radius']\n        loaded_env.boundary_condition    = arguments['boundary_condition']\n\n        # Optional arguments\n        loaded_env.source_data_file      = arguments.get('source_data_file')\n        loaded_env.odor_present_threshold = arguments.get('odor_present_threshold')\n        loaded_env.start_type            = arguments.get('start_type')\n\n        # Arrays\n        loaded_env.grid = grid\n        loaded_env.start_probabilities = start_probabilities\n\n    else:\n        loaded_env = Environment(\n            data                  = arguments['source_data_file'],\n            source_position       = np.array(arguments['data_source_position']),\n            source_radius         = arguments['source_radius'],\n            discretization        = arguments['discretization'],\n            margins               = np.array(arguments['margins']),\n            boundary_condition    = arguments['boundary_condition'],\n            start_zone            = arguments.get('start_type'),\n            odor_present_threshold = arguments.get('odor_present_threshold'),\n            name                  = arguments['name']\n        )\n\n    # Folder where the environment was pulled from\n    loaded_env.saved_at = os.path.abspath(folder)\n\n    return loaded_env\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.move","title":"<code>move(pos, movement)</code>","text":"<p>Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The start position of the movement.</p> required <code>movement</code> <code>ndarray</code> <p>A 2D movement vector.</p> required <p>Returns:</p> Name Type Description <code>new_pos</code> <code>ndarray</code> <p>The new position after applying the movement.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def move(self,\n         pos:np.ndarray,\n         movement:np.ndarray\n         ) -&gt; np.ndarray:\n    '''\n    Applies a movement vector to a position point and returns a new position point while respecting the boundary conditions.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The start position of the movement.\n    movement : np.ndarray\n        A 2D movement vector.\n\n    Returns\n    -------\n    new_pos : np.ndarray\n        The new position after applying the movement.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Applying the movement vector\n    new_pos = pos + movement\n\n    # Handling the case we are dealing with a single point.\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        new_pos = new_pos[None,:]\n\n    # Wrap condition for vertical axis\n    if self.boundary_condition in ['wrap', 'wrap_vertical']:\n        new_pos[new_pos[:,0] &lt; 0, 0] += self.padded_height\n        new_pos[new_pos[:,0] &gt;= self.padded_height, 0] -= self.padded_height\n\n    # Wrap condition for horizontal axis\n    if self.boundary_condition in ['wrap', 'wrap_horizontal']:\n        new_pos[new_pos[:,1] &lt; 0, 1] += self.padded_width\n        new_pos[new_pos[:,1] &gt;= self.padded_width, 1] -= self.padded_width\n\n    # Stop condition\n    if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_horizontal'):\n        new_pos[:,0] = xp.clip(new_pos[:,0], 0, (self.padded_height-1))\n\n    if (self.boundary_condition == 'stop') or (self.boundary_condition == 'wrap_vertical'):\n        new_pos[:,1] = xp.clip(new_pos[:,1], 0, (self.padded_width-1))\n\n    if is_single_point:\n        new_pos = new_pos[0]\n\n    return new_pos\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.plot","title":"<code>plot(frame=0, ax=None)</code>","text":"<p>Simple function to plot the environment</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>An ax on which the environment can be plot</p> <code>None</code> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def plot(self, frame:int=0, ax:plt.Axes=None) -&gt; None:\n    '''\n    Simple function to plot the environment\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        An ax on which the environment can be plot\n    '''\n    # If on GPU use the CPU version to plot\n    if self.on_gpu:\n        self._alternate_version.plot(\n            frame=frame,\n            ax=ax\n        )\n        return\n\n    if ax is None:\n        _, ax = plt.subplots(1, figsize=(15,5))\n\n    # Odor grid\n    odor = plt.Rectangle([0,0], 1, 1, color='black', fill=True)\n    ax.imshow((self.grid[frame] &gt; (self.odor_present_threshold if self.odor_present_threshold is not None else 0)).astype(float), cmap='Greys')\n\n    # Start zone contour\n    start_zone = plt.Rectangle([0,0], 1, 1, color='blue', fill=False)\n    ax.contour(self.start_probabilities, levels=[0.0], colors='blue')\n\n    # Source circle\n    goal_circle = plt.Circle(self.source_position[::-1], self.source_radius, color='r', fill=False)\n    ax.add_patch(goal_circle)\n\n    # Legend\n    ax.legend([odor, start_zone, goal_circle], [f'Frame {frame} odor cues', 'Start zone', 'Source'])\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.random_start_points","title":"<code>random_start_points(n=1)</code>","text":"<p>Function to generate n starting positions following the starting probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many random starting positions to generate</p> <code>1</code> <p>Returns:</p> Name Type Description <code>random_states_2d</code> <code>ndarray</code> <p>The n random 2d points in a n x 2 array.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def random_start_points(self,\n                        n:int=1\n                        ) -&gt; np.ndarray:\n    '''\n    Function to generate n starting positions following the starting probabilities.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many random starting positions to generate\n\n    Returns\n    -------\n    random_states_2d : np.ndarray\n        The n random 2d points in a n x 2 array. \n    '''\n    xp = cp if self.on_gpu else np\n\n    assert n&gt;0, \"n has to be a strictly positive number (&gt;0)\"\n\n    random_states = self.rnd_state.choice(xp.arange(self.padded_height * self.padded_width), size=n, replace=True, p=self.start_probabilities.ravel())\n    random_states_2d = xp.array(xp.unravel_index(random_states, (self.padded_height, self.padded_width))).T\n    return random_states_2d\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.save","title":"<code>save(folder=None, save_arrays=False, force=False)</code>","text":"<p>Function to save the environment to the memory.</p> <p>By default it saved in a new folder at the current path in a new folder with the name 'Env-' where  is the name set when initializing an environment. In this folder a file \"METADATA.json\" is created containing all the properties of the environment. <p>The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file. The arrays are saved to .npy files along with the METADATA file.</p> <p>If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder to which to save the environment data. If it is not provided, it will be created in the current folder.</p> <code>None</code> <code>save_arrays</code> <code>bool</code> <p>Whether or not to save the numpy arrays to memory. (The arrays can be heavy)</p> <code>False</code> <code>force</code> <code>bool</code> <p>In case an environment of the same name is already saved, it will be overwritten.</p> <code>False</code> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         save_arrays:bool=False,\n         force:bool=False\n         ) -&gt; None:\n    '''\n    Function to save the environment to the memory.\n\n    By default it saved in a new folder at the current path in a new folder with the name 'Env-&lt;name&gt;' where &lt;name&gt; is the name set when initializing an environment.\n    In this folder a file \"METADATA.json\" is created containing all the properties of the environment.\n\n    The numpy arrays of the environment (grid and start_probabilities) can be saved or not. If not, when the environment is loaded it needs to be reconstructed from the original data file.\n    The arrays are saved to .npy files along with the METADATA file.\n\n    If an environment of the same name is already saved, the saving will be interupted. It can however be forced with the force parameter.\n\n    Parameters\n    ----------\n    folder : str, optional\n        The folder to which to save the environment data. If it is not provided, it will be created in the current folder.\n    save_arrays : bool, default=False\n        Whether or not to save the numpy arrays to memory. (The arrays can be heavy)\n    force : bool, default=False\n        In case an environment of the same name is already saved, it will be overwritten.\n    '''\n    # If on gpu, use the cpu version to save\n    if self.on_gpu:\n        self._alternate_version.save(\n            folder=folder,\n            save_arrays=save_arrays,\n            force=force\n        )\n        return\n\n    # Assert either data_file is provided or save_arrays is enabled\n    assert save_arrays or ((self.source_data_file is not None) and (self.start_type is not None)), \"The environment was not created from a data file so 'save_arrays' has to be set to True.\"\n\n    # Adding env name to folder path\n    if folder is None:\n        folder = f'./Env-{self.name}'\n    else:\n        folder += '/Env-' + self.name\n\n    # Checking the folder exists or creates it\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    elif len(os.listdir(folder)) &gt; 0:\n        if force:\n            shutil.rmtree(folder)\n            os.mkdir(folder)\n        else:\n            raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n    # Generating the metadata arguments dictionary\n    arguments = {}\n    arguments['name'] = self.name\n\n    if self.source_data_file is not None:\n        arguments['source_data_file'] = self.source_data_file\n\n    arguments['width']                 = self.width\n    arguments['height']                = self.height\n    arguments['margins']               = self.margins.tolist()\n    arguments['padded_width']          = int(self.padded_width)\n    arguments['padded_height']         = int(self.padded_height)\n    arguments['shape']                 = [int(s) for s in self.shape]\n    arguments['discretization']        = self.discretization\n    arguments['data_source_position']  = self.data_source_position.tolist()\n    arguments['source_position']       = self.source_position.tolist()\n    arguments['source_radius']         = self.source_radius\n    arguments['boundary_condition']    = self.boundary_condition\n\n    if self.odor_present_threshold is not None:\n        arguments['odor_present_threshold'] = self.odor_present_threshold\n    if self.start_type is not None:\n        arguments['start_type'] = self.start_type\n\n    # Output the arguments to a METADATA file\n    with open(folder + '/METADATA.json', 'w') as json_file:\n        json.dump(arguments, json_file, indent=4)\n\n    # Output the numpy arrays\n    if save_arrays:\n        np.save(folder + '/grid.npy', self.grid)\n        np.save(folder + '/start_probabilities.npy', self.start_probabilities)\n\n    # Success print\n    self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n    print(f'Environment saved to: {folder}')\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.source_reached","title":"<code>source_reached(pos)</code>","text":"<p>Checks whether a given position is within the source radius.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>The position to check whether in the radius of the source.</p> required <p>Returns:</p> Name Type Description <code>is_at_source</code> <code>bool</code> <p>Whether or not the position is within the radius of the source.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def source_reached(self,\n                   pos:np.ndarray\n                   ) -&gt; bool | np.ndarray:\n    '''\n    Checks whether a given position is within the source radius.\n\n    Parameters\n    ----------\n    pos : np.ndarray\n        The position to check whether in the radius of the source.\n\n    Returns\n    -------\n    is_at_source : bool\n        Whether or not the position is within the radius of the source.\n    '''\n    xp = cp if self.on_gpu else np\n\n    # Handling the case of a single point\n    is_single_point = (len(pos.shape) == 1)\n    if is_single_point:\n        pos = pos[None,:]\n\n    is_at_source = (xp.sum((pos - self.source_position) ** 2, axis=1) &lt;= (self.source_radius ** 2))\n\n    return bool(is_at_source[0]) if is_single_point else is_at_source\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function to send the numpy arrays of the environment to the cpu memory. It returns a new instance of the Environment with the arrays as numpy arrays.</p> <p>Returns:</p> Name Type Description <code>cpu_environment</code> <code>Environment</code> <p>A new environment instance where the arrays are on the cpu memory.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def to_cpu(self) -&gt; 'Environment':\n    '''\n    Function to send the numpy arrays of the environment to the cpu memory.\n    It returns a new instance of the Environment with the arrays as numpy arrays.\n\n    Returns\n    -------\n    cpu_environment : Environment\n        A new environment instance where the arrays are on the cpu memory.\n    '''\n    if self.on_gpu:\n        assert self._alternate_version is not None, \"Something went wrong\"\n        return self._alternate_version\n\n    return self\n</code></pre>"},{"location":"reference/environment/#olfactory_navigation.environment.Environment.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the environment to the gpu memory. It returns a new instance of the Environment with the arrays as cupy arrays.</p> <p>Returns:</p> Name Type Description <code>gpu_environment</code> <code>Environment</code> <p>A new environment instance where the arrays are on the gpu memory.</p> Source code in <code>olfactory_navigation\\environment.py</code> <pre><code>def to_gpu(self) -&gt; 'Environment':\n    '''\n    Function to send the numpy arrays of the environment to the gpu memory.\n    It returns a new instance of the Environment with the arrays as cupy arrays.\n\n    Returns\n    -------\n    gpu_environment : Environment\n        A new environment instance where the arrays are on the gpu memory.\n    '''\n    assert gpu_support, \"GPU support is not enabled...\"\n\n    # Generating a new instance\n    cls = self.__class__\n    gpu_environment = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_environment, arg, cp.array(val))\n        elif arg == 'rnd_state':\n            setattr(gpu_environment, arg, cp.random.RandomState(self.seed))\n        else:\n            setattr(gpu_environment, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_environment\n    gpu_environment._alternate_version = self\n\n    gpu_environment.on_gpu = True\n    return gpu_environment\n</code></pre>"},{"location":"reference/simulation/","title":"simulation","text":""},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory","title":"<code>SimulationHistory</code>","text":"<p>Class to represent a list of the steps that happened during a simulation with:     - the positions the agents pass by     - the actions the agents take     - the observations the agents receive ('observations')</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>start_points</code> <code>ndarray</code> <p>The initial points of the agents in the simulation.</p> required <code>environment</code> <code>Environment</code> <p>The environment on which the simulation is run (can be different from the one associated with the agent).</p> required <code>agent</code> <code>Agent</code> <p>The agent used in the simulation.</p> required <code>time_shift</code> <code>ndarray</code> <p>An array of time shifts in the simulation data.</p> required <code>reward_discount</code> <code>float</code> <p>A discount to be applied to the rewards received by the agent. (eg: reward of 1 received at time n would be: 1 * reward_discount^n)</p> <code>0.99</code> <p>Attributes:</p> Name Type Description <code>start_points</code> <code>ndarray</code> <code>environment</code> <code>Environment</code> <code>agent</code> <code>Agent</code> <code>time_shift</code> <code>ndarray</code> <code>reward_discount</code> <code>float</code> <code>n</code> <code>int</code> <p>The amount of simulations.</p> <code>start_time</code> <code>datetime</code> <p>The datetime the simulations start.</p> <code>actions</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n actions as dy,dx vectors.</p> <code>positions</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n positions as y,x vectors.</p> <code>observations</code> <code>list[ndarray]</code> <p>A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n observations received by the agents.</p> <code>done_at_step</code> <code>ndarray</code> <p>A numpy array containing n elements that records when a given simulation reaches the source (-1 is not reached).</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>class SimulationHistory:\n    '''\n    Class to represent a list of the steps that happened during a simulation with:\n        - the positions the agents pass by\n        - the actions the agents take\n        - the observations the agents receive ('observations')\n\n    ...\n\n    Parameters\n    ----------\n    start_points : np.ndarray\n        The initial points of the agents in the simulation.\n    environment : Environment\n        The environment on which the simulation is run (can be different from the one associated with the agent).\n    agent : Agent\n        The agent used in the simulation.\n    time_shift : np.ndarray\n        An array of time shifts in the simulation data.\n    reward_discount : float, default=0.99\n        A discount to be applied to the rewards received by the agent. (eg: reward of 1 received at time n would be: 1 * reward_discount^n)\n\n    Attributes\n    ----------\n    start_points : np.ndarray\n    environment : Environment\n    agent : Agent\n    time_shift : np.ndarray\n    reward_discount : float\n    n : int\n        The amount of simulations.\n    start_time : datetime\n        The datetime the simulations start.\n    actions : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n actions as dy,dx vectors.\n    positions : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n positions as y,x vectors.\n    observations : list[np.ndarray]\n        A list of numpy arrays. At each step of the simulation, an array of shape n by 2 is appended to this list representing the n observations received by the agents.\n    done_at_step : np.ndarray\n        A numpy array containing n elements that records when a given simulation reaches the source (-1 is not reached).\n    '''\n    def __init__(self,\n                 start_points:np.ndarray,\n                 environment:Environment,\n                 agent:Agent,\n                 time_shift:np.ndarray,\n                 reward_discount:float=0.99\n                 ) -&gt; None:\n        # If only on state is provided, we make it a 1x2 vector\n        if len(start_points.shape) == 1:\n            start_points = start_points[None,:]\n\n        # Fixed parameters\n        self.n = len(start_points)\n        self.environment = environment.to_cpu()\n        self.agent = agent.to_cpu()\n        self.time_shift = time_shift if gpu_support and cp.get_array_module(time_shift) == np else cp.asnumpy(time_shift)\n        self.reward_discount = reward_discount\n        self.start_time = datetime.now()\n\n        # Simulation Tracking\n        self.start_points = start_points if gpu_support and cp.get_array_module(start_points) == np else cp.asnumpy(start_points)\n        self.actions = []\n        self.positions = []\n        self.observations = []\n        self.timestamps = []\n\n        self._running_sims = np.arange(self.n)\n        self.done_at_step = np.full(self.n, fill_value=-1)\n\n        # Other parameters\n        self._simulation_dfs = None\n\n\n    def add_step(self,\n                 actions:np.ndarray,\n                 next_positions:np.ndarray,\n                 observations:np.ndarray,\n                 is_done:np.ndarray,\n                 interupt:np.ndarray\n                 ) -&gt; None:\n        '''\n        Function to add a step in the simulation history.\n\n        Parameters\n        ----------\n        actions : np.ndarray\n            The actions that were taken by the agents.\n        next_positions : np.ndarray\n            The positions that were reached by the agents after having taken actions.\n        observations : np.ndarray\n            The observations the agents receive after having taken actions.\n        is_done : np.ndarray\n            A boolean array of whether each agent has reached the source or not.\n        interupt : np.ndarray\n            A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.\n        '''\n        self._simulation_dfs = None\n\n        # Time tracking\n        self.timestamps.append(datetime.now())\n\n        # Handle case cupy arrays are provided\n        if gpu_support:\n            actions = actions if cp.get_array_module(actions) == np else cp.asnumpy(actions)\n            next_positions = next_positions if cp.get_array_module(next_positions) == np else cp.asnumpy(next_positions)\n            observations = observations if cp.get_array_module(observations) == np else cp.asnumpy(observations)\n            is_done = is_done if cp.get_array_module(is_done) == np else cp.asnumpy(is_done)\n            interupt = interupt if cp.get_array_module(interupt) == np else cp.asnumpy(interupt)\n\n        # Actions tracking\n        action_all_sims = np.full((self.n,2), fill_value=-1)\n        action_all_sims[self._running_sims] = actions\n        self.actions.append(action_all_sims)\n\n        # Next states tracking\n        next_position_all_sims = np.full((self.n, 2), fill_value=-1)\n        next_position_all_sims[self._running_sims] = next_positions\n        self.positions.append(next_position_all_sims)\n\n        # Observation tracking\n        observation_all_sims = np.full((self.n,), fill_value=-1, dtype=float)\n        observation_all_sims[self._running_sims] = observations\n        self.observations.append(observation_all_sims)\n\n        # Recording at which step the simulation is done if it is done\n        self.done_at_step[self._running_sims[is_done]] = len(self.positions)\n\n        # Updating the list of running sims\n        self._running_sims = self._running_sims[~is_done &amp; ~interupt]\n\n\n    @property\n    def analysis_df(self) -&gt; pd.DataFrame:\n        '''\n        A Pandas DataFrame analyzing the results of the simulations.\n        It aggregates the simulations in single rows, recording:\n         - y_start and x_start: The x, y starting positions\n         - optimal_steps_count: The minimal amount of steps to reach the source\n         - converged:           Whether or not the simulation reached the source\n         - steps_taken:         The amount of steps the agent took to reach the source, (horizon if the simulation did not reach the source)\n         - discounted_rewards:  The discounted reward received by the agent over the course of the simulation\n         - extra_steps:         The amount of extra steps compared to the optimal trajectory\n         - t_min_over_t:         normalized version of the extra steps measure, where it tends to 1 the least amount of time the agent took to reach the source compared to an optimal trajectory.\n\n        For the measures (converged, steps_taken, discounted_rewards, extra_steps, t_min_over_t), the average and standard deviations are computed in rows at the top.\n        '''\n        # Dataframe creation\n        df = pd.DataFrame(self.start_points, columns=['y_start', 'x_start'])\n        df['optimal_steps_count'] = self.environment.distance_to_source(self.start_points)\n        df['converged'] = self.done_at_step &gt;= 0\n        df['steps_taken'] = np.where(df['converged'], self.done_at_step, len(self.positions))\n        df['discounted_rewards'] = self.reward_discount ** df['steps_taken']\n        df['extra_steps'] = df['steps_taken'] - df['optimal_steps_count']\n        df['t_min_over_t'] = df['optimal_steps_count'] / df['steps_taken']\n\n        # Reindex\n        runs_list = [f'run_{i}' for i in range(self.n)]\n        df.index = runs_list\n\n        # Analysis aggregations\n        columns_to_analyze = ['converged', 'steps_taken', 'discounted_rewards', 'extra_steps', 't_min_over_t']\n        success_averages = df.loc[df['converged'], columns_to_analyze].mean()\n        succes_std = df.loc[df['converged'], columns_to_analyze].std()\n\n        df.loc['mean', columns_to_analyze] = df[columns_to_analyze].mean()\n        df.loc['standard_deviation', columns_to_analyze] = df[columns_to_analyze].std()\n\n        df.loc['success_mean', columns_to_analyze] = success_averages\n        df.loc['success_standard_deviation', columns_to_analyze] = succes_std\n\n        # Bringing analysis rows to top\n        df = df.reindex([\n            'mean',\n            'standard_deviation',\n            'success_mean',\n            'success_standard_deviation',\n            *runs_list])\n\n        return df\n\n\n    @property\n    def summary(self) -&gt; str:\n        '''\n        A string summarizing the performances of all the simulations.\n        The metrics used are averages of:\n         - Step count\n         - Extra steps\n         - Discounted rewards\n         - Tmin / T\n\n        Along with the respective the standard deviations and equally for only for the successful simulations.\n        '''\n        done_sim_count = np.sum(self.done_at_step &gt;= 0)\n        summary_str = f'Simulations reached goal: {done_sim_count}/{self.n} ({self.n-done_sim_count} failures) ({(done_sim_count*100)/self.n:.2f}%)'\n\n        if done_sim_count == 0:\n            return summary_str\n\n        # Metrics\n        df = self.analysis_df\n\n        summary_str += f\"\\n\\t- Average step count: {df.loc['mean','steps_taken']:.3f} +- {df.loc['standard_deviation','steps_taken']:.2f} \"\n        summary_str += f\"(Successfull only: {df.loc['success_mean','steps_taken']:.3f} +- {df.loc['success_standard_deviation','steps_taken']:.2f})\"\n\n        summary_str += f\"\\n\\t- Extra steps: {df.loc['mean','extra_steps']:.3f} +- {df.loc['standard_deviation','extra_steps']:.2f} \"\n        summary_str += f\"(Successful only: {df.loc['success_mean','extra_steps']:.3f} +- {df.loc['success_standard_deviation','extra_steps']:.2f})\"\n\n        summary_str += f\"\\n\\t- Average discounted rewards (ADR): {df.loc['mean','discounted_rewards']:.3f} +- {df.loc['standard_deviation','discounted_rewards']:.2f} \"\n        summary_str += f\"(Successfull only: {df.loc['success_mean','discounted_rewards']:.3f} +- {df.loc['success_standard_deviation','discounted_rewards']:.2f})\"\n\n        summary_str += f\"\\n\\t- Tmin/T: {df.loc['mean','t_min_over_t']:.3f} +- {df.loc['standard_deviation','t_min_over_t']:.2f} \"\n        summary_str += f\"(Successful only: {df.loc['success_mean','t_min_over_t']:.3f} +- {df.loc['success_standard_deviation','t_min_over_t']:.2f})\"\n\n        return summary_str\n\n\n    @property\n    def simulation_dfs(self) -&gt; list[pd.DataFrame]:\n        '''\n        A list of the pandas DataFrame where each dataframe is a single simulation history.\n        Each row is a different time instant of simulation process with each column being:\n         - time (of the simulation data)\n         - x\n         - y\n         - dx\n         - dy\n         - o (pure, not thresholded)\n         - done (boolean)\n        '''\n        if self._simulation_dfs is None:\n            self._simulation_dfs = []\n\n            # Converting state, actions and observation to numpy arrays\n            states_array = np.array(self.positions)\n            action_array = np.array(self.actions)\n            observation_array = np.array(self.observations)\n\n            for i in range(self.n):\n                length = self.done_at_step[i] if self.done_at_step[i] &gt;= 0 else len(states_array)\n\n                df = {\n                    'time':  np.arange(length+1) + self.time_shift[i],\n                    'y':     np.hstack([self.start_points[i,0], states_array[:length, i, 0]]),\n                    'x':     np.hstack([self.start_points[i,1], states_array[:length, i, 1]]),\n                    'dy':    np.hstack([[None], action_array[:length, i, 0]]),\n                    'dx':    np.hstack([[None], action_array[:length, i, 1]]),\n                    'o':     np.hstack([[None], observation_array[:length, i]]),\n                    'done':  np.hstack([[None], np.where(np.arange(1,length+1) == self.done_at_step[i], 1, 0)])\n                }\n\n                # Append\n                self._simulation_dfs.append(pd.DataFrame(df))\n\n        return self._simulation_dfs\n\n\n    def save(self,\n             file:str|None=None,\n             folder:str|None=None,\n             save_analysis:bool=True,\n             save_components:bool=False\n             ) -&gt; None:\n        '''\n        Function to save the simulation history to a csv file in a given folder.\n        Additionally, an analysis of the runs can be saved if the save_analysis is enabled.\n        The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.\n\n        Parameters\n        ----------\n        file : str, optional\n            The name of the file the simulation histories will be saved to.\n            If it is not provided, it will be by default \"Simulations-&lt;env_name&gt;-n_&lt;sim_count&gt;-&lt;sim_start_timestamp&gt;-horizon_&lt;max_sim_length&gt;.csv\"\n        folder : str, optional\n            Folder to save the simulation histories to.\n            If the folder name is not provided the current folder will be used.\n        save_analysis : bool, default=True\n            Whether to save an additional csv file with an analysis of the runs of the simulation.\n            It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken.\n            The means and standard deviations of all the runs are also computed.\n            The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.\n        save_components : bool, default=False\n            Whether or not to save the environment and agent along with the simulation histories in the given folder.\n        '''\n        # Handle file name\n        if file is None:\n            env_name = f's_{self.environment.shape[0]}_{self.environment.shape[1]}'\n            file = f'Simulations-{env_name}-n_{self.n}-{self.start_time.strftime(\"%m%d%Y_%H%M%S\")}-horizon_{len(self.positions)}.csv'\n\n        if not file.endswith('.csv'):\n            file += '.csv'\n\n        # Handle folder\n        if folder is None:\n            folder = './'\n\n        if '/' not in folder:\n            folder = './' + folder\n\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n\n        if not folder.endswith('/'):\n            folder += '/'\n\n        # Save components if requested\n        if save_components:\n            if (self.environment.saved_at is None) or (folder not in self.environment.saved_at):\n                self.environment.save(folder=folder)\n\n            if (self.agent.saved_at is None) or (folder not in self.agent.saved_at):\n                self.agent.save(folder=folder)\n\n        # Create csv file\n        combined_df = pd.concat(self.simulation_dfs)\n\n        # Adding Environment and Agent info\n        padding = [None] * len(combined_df)\n        combined_df['timestamps'] = [ts.strftime('%H%M%S%f') for ts in self.timestamps] + padding[:-len(self.timestamps)]\n        combined_df['reward_discount'] = [self.reward_discount] + padding[:-1]\n        combined_df['environment'] = [self.environment.name, self.environment.saved_at] + padding[:-2]\n        combined_df['agent'] = [self.agent.name, self.agent.class_name, self.agent.saved_at] + padding[:-3]\n\n        # Saving csv\n        combined_df.to_csv(folder + file, index=False)\n\n        print(f'Simulations saved to: {folder + file}')\n\n        if save_analysis:\n            analysis_file = file.replace('.csv', '-analysis.csv')\n            self.analysis_df.to_csv(folder + analysis_file)\n\n            print(f\"Simulation's analysis saved to: {folder + analysis_file}\")\n\n\n    @classmethod\n    def load_from_file(cls,\n                       file:str,\n                       environment:Environment|None=None,\n                       agent:Agent|None=None\n                       ) -&gt; 'SimulationHistory':\n        '''\n        Function to load the simulation history from a file.\n        This can be useful to use the plot functions on the simulations saved in succh file.\n\n        The environment and agent can provided as a backup in the case they cannot be loaded from the file.\n\n        Parameters\n        ----------\n        file : str\n            A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)\n        environment : Environment, optional\n            An environment instance to be linked with the simulation history object.\n            If an environment can be loaded from the path found in the file, this parameter will be ignored.\n            But if this loading fails and no environment is provided, the loading will fail.\n        agent : Agent, optional\n            An agent instance to be linked with the simulation history object.\n            If an agent can be loaded from the path found in the file, this parameter will be ignored.\n            But if this loading fails and no agent is provided, the loading will fail.\n\n        Returns\n        -------\n        hist : SimulationHistory\n            The loaded instance of a simulation history object.\n        '''\n        combined_df = pd.read_csv(file, dtype={\n            'time':             int,\n            'y':                float,\n            'x':                float,\n            'dy':               float,\n            'dx':               float,\n            'o':                float,\n            'done':             float,\n            'reward_discount':  str,\n            'environment':      str,\n            'agent':            str\n        })\n\n        # Retrieving reward discount\n        reward_discount = combined_df['reward_discount'][0]\n\n        # Retrieving environment\n        loaded_environment = None\n        environment_name = combined_df['environment'][0]\n        if combined_df['environment'][1] is not None:\n            try:\n                loaded_environment = Environment.load(combined_df['environment'][1])\n            except:\n                print(f'Failed to retrieve \"{environment_name}\" environment from memory')\n\n        if loaded_environment is not None:\n            print(f'Environment \"{environment_name}\" loaded from memory' + (' (Ignoring environment provided as a parameter)' if environment is not None else ''))\n            environment = loaded_environment\n\n        if environment is None:\n            raise Exception('No environment could be linked, the simulation history cannot be instanciated. Provide an environment to resolve this.')\n\n        # Retrieving agent\n        loaded_agent = None\n        agent_name = combined_df['agent'][0]\n        agent_class = combined_df['agent'][1]\n        if combined_df['agent'][2] is not None:\n            try:\n                for (class_name, class_obj) in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n                    if class_name == agent_class:\n                        loaded_agent = class_obj.load(combined_df['agent'][2])\n                        break\n            except:\n                print(f'Failed to retrieve \"{agent_name}\" agent from memory')\n\n        if loaded_agent is not None:\n            print(f'Agent \"{agent_name}\" loaded from memory' + (' (Ignoring agent provided as a parameter)' if agent is not None else ''))\n            agent = loaded_agent\n\n        if agent is None:\n            raise Exception('No agent could be linked, the simulation history cannot be instanciated. Provide an agent to resolve this.')\n\n        # Columns to retrieve\n        columns = [\n            'time',\n            'y',\n            'x',\n            'dy',\n            'dx',\n            'o',\n            'done'\n        ]\n\n        # Recreation of list of simulations\n        simulation_dfs = []\n        sim_start_rows = [None] + np.argwhere(combined_df[['done']].isnull())[1:,0].tolist() + [None]\n        n = len(sim_start_rows)-1\n\n        for i in range(n):\n            simulation_dfs.append(combined_df[columns].iloc[sim_start_rows[i]:sim_start_rows[i+1]])\n\n        # Gathering start states\n        start_points = np.array([sim[['y', 'x']].iloc[0] for sim in simulation_dfs])\n        time_shift = np.array([sim['time'].iloc[0] for sim in simulation_dfs])\n\n        # Generation of SimHist instance\n        hist = SimulationHistory(\n            start_points=start_points,\n            environment=environment,\n            agent=agent,\n            time_shift=time_shift,\n            reward_discount=reward_discount\n        )\n\n        max_length = max(len(sim) for sim in simulation_dfs)\n\n        # Recreating action, state and observations\n        positions = np.full((max_length-1, n, 2), -1)\n        actions = np.full((max_length-1, n, 2), -1)\n        observations = np.full((max_length-1, n), -1)\n        done_at_step = np.full((n,), -1)\n\n        for i, sim in enumerate(simulation_dfs):\n            positions[:len(sim)-1, i, :] = sim[['y','x']].to_numpy()[1:]\n            actions[:len(sim)-1, i, :] = sim[['dy','dx']].to_numpy()[1:]\n            observations[:len(sim)-1, i] = sim[['o']].to_numpy()[1:,0]\n            done_at_step[i] = len(sim)-1 if sim['done'].iloc[-1] == 1 else -1\n\n        hist.positions = [arr for arr in positions]\n        hist.actions = [arr for arr in actions]\n        hist.observations = [arr for arr in observations]\n        hist.done_at_step = done_at_step\n        hist.timestamps = [datetime.strptime(str(int(ts)), '%H%M%S%f') for ts in combined_df['timestamps'][:max_length-1]]\n\n        # Saving simulation dfs back\n        hist._simulation_dfs = simulation_dfs\n\n        return hist\n\n\n    def plot(self,\n             sim_id:int=0,\n             ax:plt.Axes=None\n             ) -&gt; None:\n        '''\n        Function to plot a the trajectory of a given simulation.\n        An ax can be use to plot it on.\n\n        Parameters\n        ----------\n        sim_id : int, default=0\n            The id of the simulation to plot.\n        ax : plt.Axes, optional\n            The ax on which to plot the path. (If not provided, a new axis will be created)\n        '''\n        # Generate ax is not provided\n        if ax is None:\n            _, ax = plt.subplots(figsize=(18,3))\n\n        # Retrieving sim\n        sim = self.simulation_dfs[sim_id]\n\n        # Plot setup\n        env_shape = self.environment.shape\n        ax.imshow(np.zeros(self.environment.shape), cmap='Greys', zorder=-100)\n        ax.set_xlim(0, env_shape[1])\n        ax.set_ylim(env_shape[0], 0)\n\n        # Start\n        start_coord = sim[['x', 'y']].to_numpy()[0]\n        ax.scatter(start_coord[0], start_coord[1], c='green', label='Start')\n\n        # Source circle\n        goal_circle = plt.Circle(self.environment.source_position[::-1], self.environment.source_radius, color='r', fill=False, label='Source')\n        ax.add_patch(goal_circle)\n\n        # Until step\n        seq = sim[['x','y']][1:].to_numpy()\n\n        # Path\n        ax.plot(seq[:,0], seq[:,1], zorder=-1, c='black', label='Path')\n\n        # Something sensed\n        if self.agent is not None:\n            something_sensed = sim['o'][1:].to_numpy() &gt; self.agent.threshold\n            points_obs = seq[something_sensed,:]\n            ax.scatter(points_obs[:,0], points_obs[:,1], zorder=1, label='Something observed')\n        else:\n            print('Agent used is not tracked')\n\n        # Generate legend\n        ax.legend()\n\n\n    def plot_runtimes(self,\n                      ax:plt.Axes=None\n                      ) -&gt; None:\n        '''\n        Function to plot the runtimes over the iterations.\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            The ax on which to plot the path. (If not provided, a new axis will be created)\n        '''\n        # Generate ax is not provided\n        if ax is None:\n            _, ax = plt.subplots(figsize=(18,3))\n\n        # Computing differences\n        timestamp_differences_ms = np.diff(np.array([int(ts.strftime('%H%M%S%f')) for ts in self.timestamps])) / 1000\n\n        # Actual plot\n        ax.plot(timestamp_differences_ms)\n\n        # Axes\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Runtime (ms)')\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.analysis_df","title":"<code>analysis_df: pd.DataFrame</code>  <code>property</code>","text":"<p>A Pandas DataFrame analyzing the results of the simulations. It aggregates the simulations in single rows, recording:  - y_start and x_start: The x, y starting positions  - optimal_steps_count: The minimal amount of steps to reach the source  - converged:           Whether or not the simulation reached the source  - steps_taken:         The amount of steps the agent took to reach the source, (horizon if the simulation did not reach the source)  - discounted_rewards:  The discounted reward received by the agent over the course of the simulation  - extra_steps:         The amount of extra steps compared to the optimal trajectory  - t_min_over_t:         normalized version of the extra steps measure, where it tends to 1 the least amount of time the agent took to reach the source compared to an optimal trajectory.</p> <p>For the measures (converged, steps_taken, discounted_rewards, extra_steps, t_min_over_t), the average and standard deviations are computed in rows at the top.</p>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.simulation_dfs","title":"<code>simulation_dfs: list[pd.DataFrame]</code>  <code>property</code>","text":"<p>A list of the pandas DataFrame where each dataframe is a single simulation history. Each row is a different time instant of simulation process with each column being:  - time (of the simulation data)  - x  - y  - dx  - dy  - o (pure, not thresholded)  - done (boolean)</p>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.summary","title":"<code>summary: str</code>  <code>property</code>","text":"<p>A string summarizing the performances of all the simulations. The metrics used are averages of:  - Step count  - Extra steps  - Discounted rewards  - Tmin / T</p> <p>Along with the respective the standard deviations and equally for only for the successful simulations.</p>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.add_step","title":"<code>add_step(actions, next_positions, observations, is_done, interupt)</code>","text":"<p>Function to add a step in the simulation history.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>ndarray</code> <p>The actions that were taken by the agents.</p> required <code>next_positions</code> <code>ndarray</code> <p>The positions that were reached by the agents after having taken actions.</p> required <code>observations</code> <code>ndarray</code> <p>The observations the agents receive after having taken actions.</p> required <code>is_done</code> <code>ndarray</code> <p>A boolean array of whether each agent has reached the source or not.</p> required <code>interupt</code> <code>ndarray</code> <p>A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.</p> required Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def add_step(self,\n             actions:np.ndarray,\n             next_positions:np.ndarray,\n             observations:np.ndarray,\n             is_done:np.ndarray,\n             interupt:np.ndarray\n             ) -&gt; None:\n    '''\n    Function to add a step in the simulation history.\n\n    Parameters\n    ----------\n    actions : np.ndarray\n        The actions that were taken by the agents.\n    next_positions : np.ndarray\n        The positions that were reached by the agents after having taken actions.\n    observations : np.ndarray\n        The observations the agents receive after having taken actions.\n    is_done : np.ndarray\n        A boolean array of whether each agent has reached the source or not.\n    interupt : np.ndarray\n        A boolean array of whether each agent has to be terminated even if it hasnt reached the source yet.\n    '''\n    self._simulation_dfs = None\n\n    # Time tracking\n    self.timestamps.append(datetime.now())\n\n    # Handle case cupy arrays are provided\n    if gpu_support:\n        actions = actions if cp.get_array_module(actions) == np else cp.asnumpy(actions)\n        next_positions = next_positions if cp.get_array_module(next_positions) == np else cp.asnumpy(next_positions)\n        observations = observations if cp.get_array_module(observations) == np else cp.asnumpy(observations)\n        is_done = is_done if cp.get_array_module(is_done) == np else cp.asnumpy(is_done)\n        interupt = interupt if cp.get_array_module(interupt) == np else cp.asnumpy(interupt)\n\n    # Actions tracking\n    action_all_sims = np.full((self.n,2), fill_value=-1)\n    action_all_sims[self._running_sims] = actions\n    self.actions.append(action_all_sims)\n\n    # Next states tracking\n    next_position_all_sims = np.full((self.n, 2), fill_value=-1)\n    next_position_all_sims[self._running_sims] = next_positions\n    self.positions.append(next_position_all_sims)\n\n    # Observation tracking\n    observation_all_sims = np.full((self.n,), fill_value=-1, dtype=float)\n    observation_all_sims[self._running_sims] = observations\n    self.observations.append(observation_all_sims)\n\n    # Recording at which step the simulation is done if it is done\n    self.done_at_step[self._running_sims[is_done]] = len(self.positions)\n\n    # Updating the list of running sims\n    self._running_sims = self._running_sims[~is_done &amp; ~interupt]\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.load_from_file","title":"<code>load_from_file(file, environment=None, agent=None)</code>  <code>classmethod</code>","text":"<p>Function to load the simulation history from a file. This can be useful to use the plot functions on the simulations saved in succh file.</p> <p>The environment and agent can provided as a backup in the case they cannot be loaded from the file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)</p> required <code>environment</code> <code>Environment</code> <p>An environment instance to be linked with the simulation history object. If an environment can be loaded from the path found in the file, this parameter will be ignored. But if this loading fails and no environment is provided, the loading will fail.</p> <code>None</code> <code>agent</code> <code>Agent</code> <p>An agent instance to be linked with the simulation history object. If an agent can be loaded from the path found in the file, this parameter will be ignored. But if this loading fails and no agent is provided, the loading will fail.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>The loaded instance of a simulation history object.</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>@classmethod\ndef load_from_file(cls,\n                   file:str,\n                   environment:Environment|None=None,\n                   agent:Agent|None=None\n                   ) -&gt; 'SimulationHistory':\n    '''\n    Function to load the simulation history from a file.\n    This can be useful to use the plot functions on the simulations saved in succh file.\n\n    The environment and agent can provided as a backup in the case they cannot be loaded from the file.\n\n    Parameters\n    ----------\n    file : str\n        A file (with the path) of the simulation histories csv. (the analysis file cannot be used for this)\n    environment : Environment, optional\n        An environment instance to be linked with the simulation history object.\n        If an environment can be loaded from the path found in the file, this parameter will be ignored.\n        But if this loading fails and no environment is provided, the loading will fail.\n    agent : Agent, optional\n        An agent instance to be linked with the simulation history object.\n        If an agent can be loaded from the path found in the file, this parameter will be ignored.\n        But if this loading fails and no agent is provided, the loading will fail.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        The loaded instance of a simulation history object.\n    '''\n    combined_df = pd.read_csv(file, dtype={\n        'time':             int,\n        'y':                float,\n        'x':                float,\n        'dy':               float,\n        'dx':               float,\n        'o':                float,\n        'done':             float,\n        'reward_discount':  str,\n        'environment':      str,\n        'agent':            str\n    })\n\n    # Retrieving reward discount\n    reward_discount = combined_df['reward_discount'][0]\n\n    # Retrieving environment\n    loaded_environment = None\n    environment_name = combined_df['environment'][0]\n    if combined_df['environment'][1] is not None:\n        try:\n            loaded_environment = Environment.load(combined_df['environment'][1])\n        except:\n            print(f'Failed to retrieve \"{environment_name}\" environment from memory')\n\n    if loaded_environment is not None:\n        print(f'Environment \"{environment_name}\" loaded from memory' + (' (Ignoring environment provided as a parameter)' if environment is not None else ''))\n        environment = loaded_environment\n\n    if environment is None:\n        raise Exception('No environment could be linked, the simulation history cannot be instanciated. Provide an environment to resolve this.')\n\n    # Retrieving agent\n    loaded_agent = None\n    agent_name = combined_df['agent'][0]\n    agent_class = combined_df['agent'][1]\n    if combined_df['agent'][2] is not None:\n        try:\n            for (class_name, class_obj) in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n                if class_name == agent_class:\n                    loaded_agent = class_obj.load(combined_df['agent'][2])\n                    break\n        except:\n            print(f'Failed to retrieve \"{agent_name}\" agent from memory')\n\n    if loaded_agent is not None:\n        print(f'Agent \"{agent_name}\" loaded from memory' + (' (Ignoring agent provided as a parameter)' if agent is not None else ''))\n        agent = loaded_agent\n\n    if agent is None:\n        raise Exception('No agent could be linked, the simulation history cannot be instanciated. Provide an agent to resolve this.')\n\n    # Columns to retrieve\n    columns = [\n        'time',\n        'y',\n        'x',\n        'dy',\n        'dx',\n        'o',\n        'done'\n    ]\n\n    # Recreation of list of simulations\n    simulation_dfs = []\n    sim_start_rows = [None] + np.argwhere(combined_df[['done']].isnull())[1:,0].tolist() + [None]\n    n = len(sim_start_rows)-1\n\n    for i in range(n):\n        simulation_dfs.append(combined_df[columns].iloc[sim_start_rows[i]:sim_start_rows[i+1]])\n\n    # Gathering start states\n    start_points = np.array([sim[['y', 'x']].iloc[0] for sim in simulation_dfs])\n    time_shift = np.array([sim['time'].iloc[0] for sim in simulation_dfs])\n\n    # Generation of SimHist instance\n    hist = SimulationHistory(\n        start_points=start_points,\n        environment=environment,\n        agent=agent,\n        time_shift=time_shift,\n        reward_discount=reward_discount\n    )\n\n    max_length = max(len(sim) for sim in simulation_dfs)\n\n    # Recreating action, state and observations\n    positions = np.full((max_length-1, n, 2), -1)\n    actions = np.full((max_length-1, n, 2), -1)\n    observations = np.full((max_length-1, n), -1)\n    done_at_step = np.full((n,), -1)\n\n    for i, sim in enumerate(simulation_dfs):\n        positions[:len(sim)-1, i, :] = sim[['y','x']].to_numpy()[1:]\n        actions[:len(sim)-1, i, :] = sim[['dy','dx']].to_numpy()[1:]\n        observations[:len(sim)-1, i] = sim[['o']].to_numpy()[1:,0]\n        done_at_step[i] = len(sim)-1 if sim['done'].iloc[-1] == 1 else -1\n\n    hist.positions = [arr for arr in positions]\n    hist.actions = [arr for arr in actions]\n    hist.observations = [arr for arr in observations]\n    hist.done_at_step = done_at_step\n    hist.timestamps = [datetime.strptime(str(int(ts)), '%H%M%S%f') for ts in combined_df['timestamps'][:max_length-1]]\n\n    # Saving simulation dfs back\n    hist._simulation_dfs = simulation_dfs\n\n    return hist\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.plot","title":"<code>plot(sim_id=0, ax=None)</code>","text":"<p>Function to plot a the trajectory of a given simulation. An ax can be use to plot it on.</p> <p>Parameters:</p> Name Type Description Default <code>sim_id</code> <code>int</code> <p>The id of the simulation to plot.</p> <code>0</code> <code>ax</code> <code>Axes</code> <p>The ax on which to plot the path. (If not provided, a new axis will be created)</p> <code>None</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def plot(self,\n         sim_id:int=0,\n         ax:plt.Axes=None\n         ) -&gt; None:\n    '''\n    Function to plot a the trajectory of a given simulation.\n    An ax can be use to plot it on.\n\n    Parameters\n    ----------\n    sim_id : int, default=0\n        The id of the simulation to plot.\n    ax : plt.Axes, optional\n        The ax on which to plot the path. (If not provided, a new axis will be created)\n    '''\n    # Generate ax is not provided\n    if ax is None:\n        _, ax = plt.subplots(figsize=(18,3))\n\n    # Retrieving sim\n    sim = self.simulation_dfs[sim_id]\n\n    # Plot setup\n    env_shape = self.environment.shape\n    ax.imshow(np.zeros(self.environment.shape), cmap='Greys', zorder=-100)\n    ax.set_xlim(0, env_shape[1])\n    ax.set_ylim(env_shape[0], 0)\n\n    # Start\n    start_coord = sim[['x', 'y']].to_numpy()[0]\n    ax.scatter(start_coord[0], start_coord[1], c='green', label='Start')\n\n    # Source circle\n    goal_circle = plt.Circle(self.environment.source_position[::-1], self.environment.source_radius, color='r', fill=False, label='Source')\n    ax.add_patch(goal_circle)\n\n    # Until step\n    seq = sim[['x','y']][1:].to_numpy()\n\n    # Path\n    ax.plot(seq[:,0], seq[:,1], zorder=-1, c='black', label='Path')\n\n    # Something sensed\n    if self.agent is not None:\n        something_sensed = sim['o'][1:].to_numpy() &gt; self.agent.threshold\n        points_obs = seq[something_sensed,:]\n        ax.scatter(points_obs[:,0], points_obs[:,1], zorder=1, label='Something observed')\n    else:\n        print('Agent used is not tracked')\n\n    # Generate legend\n    ax.legend()\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.plot_runtimes","title":"<code>plot_runtimes(ax=None)</code>","text":"<p>Function to plot the runtimes over the iterations.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The ax on which to plot the path. (If not provided, a new axis will be created)</p> <code>None</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def plot_runtimes(self,\n                  ax:plt.Axes=None\n                  ) -&gt; None:\n    '''\n    Function to plot the runtimes over the iterations.\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        The ax on which to plot the path. (If not provided, a new axis will be created)\n    '''\n    # Generate ax is not provided\n    if ax is None:\n        _, ax = plt.subplots(figsize=(18,3))\n\n    # Computing differences\n    timestamp_differences_ms = np.diff(np.array([int(ts.strftime('%H%M%S%f')) for ts in self.timestamps])) / 1000\n\n    # Actual plot\n    ax.plot(timestamp_differences_ms)\n\n    # Axes\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Runtime (ms)')\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.SimulationHistory.save","title":"<code>save(file=None, folder=None, save_analysis=True, save_components=False)</code>","text":"<p>Function to save the simulation history to a csv file in a given folder. Additionally, an analysis of the runs can be saved if the save_analysis is enabled. The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The name of the file the simulation histories will be saved to. If it is not provided, it will be by default \"Simulations--n_--horizon_.csv\" <code>None</code> <code>folder</code> <code>str</code> <p>Folder to save the simulation histories to. If the folder name is not provided the current folder will be used.</p> <code>None</code> <code>save_analysis</code> <code>bool</code> <p>Whether to save an additional csv file with an analysis of the runs of the simulation. It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken. The means and standard deviations of all the runs are also computed. The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.</p> <code>True</code> <code>save_components</code> <code>bool</code> <p>Whether or not to save the environment and agent along with the simulation histories in the given folder.</p> <code>False</code> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def save(self,\n         file:str|None=None,\n         folder:str|None=None,\n         save_analysis:bool=True,\n         save_components:bool=False\n         ) -&gt; None:\n    '''\n    Function to save the simulation history to a csv file in a given folder.\n    Additionally, an analysis of the runs can be saved if the save_analysis is enabled.\n    The environment and agent used can be saved in the saved folder by enabling the 'save_component' parameter.\n\n    Parameters\n    ----------\n    file : str, optional\n        The name of the file the simulation histories will be saved to.\n        If it is not provided, it will be by default \"Simulations-&lt;env_name&gt;-n_&lt;sim_count&gt;-&lt;sim_start_timestamp&gt;-horizon_&lt;max_sim_length&gt;.csv\"\n    folder : str, optional\n        Folder to save the simulation histories to.\n        If the folder name is not provided the current folder will be used.\n    save_analysis : bool, default=True\n        Whether to save an additional csv file with an analysis of the runs of the simulation.\n        It will contain the amount of steps taken, the amount of extra steps compared to optimality, the discounted rewards and the ratio between optimal trajectory and the steps taken.\n        The means and standard deviations of all the runs are also computed.\n        The file will have the same name as the simulation history file with an additional '-analysis' tag at the end.\n    save_components : bool, default=False\n        Whether or not to save the environment and agent along with the simulation histories in the given folder.\n    '''\n    # Handle file name\n    if file is None:\n        env_name = f's_{self.environment.shape[0]}_{self.environment.shape[1]}'\n        file = f'Simulations-{env_name}-n_{self.n}-{self.start_time.strftime(\"%m%d%Y_%H%M%S\")}-horizon_{len(self.positions)}.csv'\n\n    if not file.endswith('.csv'):\n        file += '.csv'\n\n    # Handle folder\n    if folder is None:\n        folder = './'\n\n    if '/' not in folder:\n        folder = './' + folder\n\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n\n    if not folder.endswith('/'):\n        folder += '/'\n\n    # Save components if requested\n    if save_components:\n        if (self.environment.saved_at is None) or (folder not in self.environment.saved_at):\n            self.environment.save(folder=folder)\n\n        if (self.agent.saved_at is None) or (folder not in self.agent.saved_at):\n            self.agent.save(folder=folder)\n\n    # Create csv file\n    combined_df = pd.concat(self.simulation_dfs)\n\n    # Adding Environment and Agent info\n    padding = [None] * len(combined_df)\n    combined_df['timestamps'] = [ts.strftime('%H%M%S%f') for ts in self.timestamps] + padding[:-len(self.timestamps)]\n    combined_df['reward_discount'] = [self.reward_discount] + padding[:-1]\n    combined_df['environment'] = [self.environment.name, self.environment.saved_at] + padding[:-2]\n    combined_df['agent'] = [self.agent.name, self.agent.class_name, self.agent.saved_at] + padding[:-3]\n\n    # Saving csv\n    combined_df.to_csv(folder + file, index=False)\n\n    print(f'Simulations saved to: {folder + file}')\n\n    if save_analysis:\n        analysis_file = file.replace('.csv', '-analysis.csv')\n        self.analysis_df.to_csv(folder + analysis_file)\n\n        print(f\"Simulation's analysis saved to: {folder + analysis_file}\")\n</code></pre>"},{"location":"reference/simulation/#olfactory_navigation.simulation.run_test","title":"<code>run_test(agent, n=None, start_points=None, environment=None, time_shift=0, time_loop=True, horizon=1000, reward_discount=0.99, print_progress=True, print_stats=True, use_gpu=False)</code>","text":"<p>Function to run n simulations for a given agent in its environment (or a given modified environment). The simulations start either from random start points or provided trough the start_points parameter. The simulation can have shifted initial times (in the olfactory simulation).</p> <p>The simulation will run for at most 'horizon' steps, after which the simulations will be considered failed.</p> <p>Some statistics can be printed at end of the simulation with the 'print_stats' parameter. It will print some performance statisitcs about the simulations such as the average discounter reward. The reward discount can be set by the 'reward_discount' parameter.</p> <p>To speedup the simulations, it can be run on the gpu by toggling the 'use_gpu' parameter. This will have the consequence to send the various arrays to the gpu memory. This will only work if the agent has the support for to work with cupy arrays.</p> <p>This method returns a SimulationHistory object that saves all the positions the agent went through, the actions the agent took, and the observation the agent received. It also provides the possibility the save the results to a csv file and plot the various trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to be tested</p> required <code>n</code> <code>int</code> <p>How many simulation to run in parallel. n is optional but it needs to match with what is provided in start_points.</p> <code>None</code> <code>start_points</code> <code>ndarray</code> <p>The starting points of the simulation in 2d space. If not provided, n random points will be generated based on the start probabilities of the environment. Else, the amount of start_points need to match to n, if it is provided.</p> <code>None</code> <code>environment</code> <code>Environment</code> <p>The environment to run the simulations in. By default, the environment linked to the agent will used. This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.</p> <code>None</code> <code>time_shift</code> <code>int or ndarray</code> <p>The time at which to start the olfactory simulation array. It can be either a single value, or n values.</p> <code>0</code> <code>time_loop</code> <code>bool</code> <p>Whether to loop the time if reaching the end. (starts back at 0)</p> <code>True</code> <code>horizon</code> <code>int</code> <p>The amount of steps to run the simulation for before killing the remaining simulations.</p> <code>1000</code> <code>reward_discount</code> <code>float</code> <p>How much a given reward is discounted based on how long it took to get it. It is purely used to compute the Average Discount Reward (ADR) after the simulation.</p> <code>0.99</code> <code>print_progress</code> <code>bool</code> <p>Wheter to show a progress bar of what step the simulations are at.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Wheter to print the stats at the end of the run.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run the simulations on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>A SimulationHistory object that tracked all the positions, actions and observations.</p> Source code in <code>olfactory_navigation\\simulation.py</code> <pre><code>def run_test(agent:Agent,\n             n:int|None=None,\n             start_points:np.ndarray|None=None,\n             environment:Environment|None=None,\n             time_shift:int|np.ndarray=0,\n             time_loop:bool=True,\n             horizon:int=1000,\n             reward_discount:float=0.99,\n             print_progress:bool=True,\n             print_stats:bool=True,\n             use_gpu:bool=False\n             ) -&gt; SimulationHistory:\n    '''\n    Function to run n simulations for a given agent in its environment (or a given modified environment).\n    The simulations start either from random start points or provided trough the start_points parameter.\n    The simulation can have shifted initial times (in the olfactory simulation).\n\n    The simulation will run for at most 'horizon' steps, after which the simulations will be considered failed.\n\n    Some statistics can be printed at end of the simulation with the 'print_stats' parameter.\n    It will print some performance statisitcs about the simulations such as the average discounter reward.\n    The reward discount can be set by the 'reward_discount' parameter.\n\n    To speedup the simulations, it can be run on the gpu by toggling the 'use_gpu' parameter.\n    This will have the consequence to send the various arrays to the gpu memory.\n    This will only work if the agent has the support for to work with cupy arrays.\n\n    This method returns a SimulationHistory object that saves all the positions the agent went through,\n    the actions the agent took, and the observation the agent received.\n    It also provides the possibility the save the results to a csv file and plot the various trajectories.\n\n    Parameters\n    ----------\n    agent : Agent\n        The agent to be tested\n    n : int, optional\n        How many simulation to run in parallel.\n        n is optional but it needs to match with what is provided in start_points.\n    start_points : np.ndarray, optional\n        The starting points of the simulation in 2d space.\n        If not provided, n random points will be generated based on the start probabilities of the environment.\n        Else, the amount of start_points need to match to n, if it is provided.\n    environment : Environment, optional\n        The environment to run the simulations in.\n        By default, the environment linked to the agent will used.\n        This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.\n    time_shift : int or np.ndarray, default=0\n        The time at which to start the olfactory simulation array.\n        It can be either a single value, or n values.\n    time_loop : bool, default=True\n        Whether to loop the time if reaching the end. (starts back at 0)\n    horizon : int, default=1000\n        The amount of steps to run the simulation for before killing the remaining simulations.\n    reward_discount : float, default=0.99\n        How much a given reward is discounted based on how long it took to get it.\n        It is purely used to compute the Average Discount Reward (ADR) after the simulation.\n    print_progress : bool, default=True\n        Wheter to show a progress bar of what step the simulations are at.\n    print_stats : bool, default=True\n        Wheter to print the stats at the end of the run.\n    use_gpu : bool, default=False\n        Whether to run the simulations on the GPU or not.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        A SimulationHistory object that tracked all the positions, actions and observations.\n    '''\n    # Gathering n\n    if n is None:\n        if (start_points is None) or (len(start_points.shape) == 1):\n            n = 1\n        else:\n            n = len(start_points)\n\n    # Handle the case an specific environment is given\n    if environment is not None:\n        assert environment.shape == agent.environment.shape, \"The provided environment's shape doesn't match the environment has been trained on...\"\n        print('Using the provided environment, not the agent environment.')\n    else:\n        environment = agent.environment\n\n    # Timeshift\n    if isinstance(time_shift, int):\n        time_shift = np.ones(n) * time_shift\n    else:\n        time_shift = np.array(time_shift)\n        assert time_shift.shape == (n,), f\"time_shift array has a wrong shape (Given: {time_shift.shape}, expected ({n},))\"\n    time_shift = time_shift.astype(int)\n\n    # Move things to GPU if needed\n    if use_gpu:\n        assert gpu_support, f\"GPU support is not enabled, the use_gpu option is not available.\"\n\n        # Move instances to GPU\n        agent = agent.to_gpu()\n        environment = environment.to_gpu()\n        time_shift = cp.array(time_shift)\n\n        if start_points is not None:\n            start_points = cp.array(start_points)\n\n    # Set start positions\n    agent_position = None\n    if start_points is not None:\n        assert start_points.shape == (n, 2), 'The provided start_points are of the wrong shape'\n        agent_position = start_points\n    else:\n        # Generating random starts\n        agent_position = environment.random_start_points(n)\n\n    # Initialize agent's state\n    agent.initialize_state(n)\n\n    # Create simulation history tracker\n    hist = SimulationHistory(\n        start_points=agent_position,\n        environment=environment,\n        agent=agent,\n        time_shift=time_shift,\n        reward_discount=reward_discount\n    )\n\n    # Track begin of simulation ts\n    sim_start_ts = datetime.now()\n\n    # Simulation loop\n    iterator = trange(horizon) if print_progress else range(horizon)\n    for i in iterator:\n        # Letting agent choose the action to take based on it's curent state\n        action = agent.choose_action()\n\n        # Updating the agent's actual position (hidden to him)\n        new_agent_position = environment.move(agent_position, action)\n\n        # Get an observation based on the new position of the agent\n        observation = environment.get_observation(new_agent_position, time=(time_shift + i))\n\n        # Check if the source is reached\n        source_reached = environment.source_reached(new_agent_position)\n\n        # Return the observation to the agent\n        agent.update_state(observation, source_reached)\n\n        # Handling the case where simulations have reached the end\n        sims_at_end = ((time_shift + i + 1) &gt;= (math.inf if time_loop else len(environment.grid)))\n\n        # Interupt agents that reached the end\n        agent_position = new_agent_position[~source_reached &amp; ~sims_at_end]\n        time_shift = time_shift[~source_reached &amp; ~sims_at_end]\n        agent.kill(simulations_to_kill=sims_at_end[~source_reached])\n\n        # Send the values to the tracker\n        hist.add_step(\n            actions=action,\n            next_positions=new_agent_position,\n            observations=observation,\n            is_done=source_reached,\n            interupt=sims_at_end\n        )\n\n        # Early stopping if all agents done\n        if len(agent_position) == 0:\n            break\n\n        # Update progress bar\n        if print_progress:\n            done_count = n-len(agent_position)\n            iterator.set_postfix({'done ': f' {done_count} of {n} ({(done_count*100)/n:.1f}%)'})\n\n    # If requested print the simulation start\n    if print_stats:\n        sim_end_ts = datetime.now()\n        print(f'Simulations done in {(sim_end_ts - sim_start_ts).total_seconds():.3f}s:')\n        print(hist.summary)\n\n    return hist\n</code></pre>"},{"location":"reference/test_setups/","title":"test_setups","text":""},{"location":"reference/test_setups/#olfactory_navigation.test_setups.run_all_starts_test","title":"<code>run_all_starts_test(agent, environment=None, time_shift=0, time_loop=True, horizon=1000, reward_discount=0.99, print_progress=True, print_stats=True, use_gpu=False)</code>","text":"<p>Function to run a test with all the available starting positions based on the environment provided (or the environmnent of the agent).</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to be tested</p> required <code>environment</code> <code>Environment</code> <p>The environment to run the simulations in. By default, the environment linked to the agent will used. This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.</p> <code>None</code> <code>time_shift</code> <code>int or ndarray</code> <p>The time at which to start the olfactory simulation array. It can be either a single value, or n values.</p> <code>0</code> <code>time_loop</code> <code>bool</code> <p>Whether to loop the time if reaching the end. (starts back at 0)</p> <code>True</code> <code>horizon</code> <code>int</code> <p>The amount of steps to run the simulation for before killing the remaining simulations.</p> <code>1000</code> <code>reward_discount</code> <code>float</code> <p>How much a given reward is discounted based on how long it took to get it. It is purely used to compute the Average Discount Reward (ADR) after the simulation.</p> <code>0.99</code> <code>print_progress</code> <code>bool</code> <p>Wheter to show a progress bar of what step the simulations are at.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Wheter to print the stats at the end of the run.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run the simulations on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>A SimulationHistory object that tracked all the positions, actions and observations.</p> Source code in <code>olfactory_navigation\\test_setups.py</code> <pre><code>def run_all_starts_test(\n             agent:Agent,\n             environment:Environment|None=None,\n             time_shift:int|np.ndarray=0,\n             time_loop:bool=True,\n             horizon:int=1000,\n             reward_discount:float=0.99,\n             print_progress:bool=True,\n             print_stats:bool=True,\n             use_gpu:bool=False\n             ) -&gt; SimulationHistory:\n    '''\n    Function to run a test with all the available starting positions based on the environment provided (or the environmnent of the agent).\n\n    Parameters\n    ----------\n    agent : Agent\n        The agent to be tested\n    environment : Environment, optional\n        The environment to run the simulations in.\n        By default, the environment linked to the agent will used.\n        This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.\n    time_shift : int or np.ndarray, default=0\n        The time at which to start the olfactory simulation array.\n        It can be either a single value, or n values.\n    time_loop : bool, default=True\n        Whether to loop the time if reaching the end. (starts back at 0)\n    horizon : int, default=1000\n        The amount of steps to run the simulation for before killing the remaining simulations.\n    reward_discount : float, default=0.99\n        How much a given reward is discounted based on how long it took to get it.\n        It is purely used to compute the Average Discount Reward (ADR) after the simulation.\n    print_progress : bool, default=True\n        Wheter to show a progress bar of what step the simulations are at.\n    print_stats : bool, default=True\n        Wheter to print the stats at the end of the run.\n    use_gpu : bool, default=False\n        Whether to run the simulations on the GPU or not.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        A SimulationHistory object that tracked all the positions, actions and observations.\n    '''\n    # Handle the case an specific environment is given\n    environment_provided = environment is not None\n    if environment_provided:\n        assert environment.shape == agent.environment.shape, \"The provided environment's shape doesn't match the environment has been trained on...\"\n    else:\n        environment = agent.environment\n\n    # Gathering starting points\n    start_points = np.argwhere(environment.start_probabilities &gt; 0)\n    n = len(start_points)\n\n    return run_test(\n        agent=agent,\n        n=n,\n        start_points=start_points,\n        environment=environment if environment_provided else None,\n        time_shift=time_shift,\n        time_loop=time_loop,\n        horizon=horizon,\n        reward_discount=reward_discount,\n        print_progress=print_progress,\n        print_stats=print_stats,\n        use_gpu=use_gpu\n    )\n</code></pre>"},{"location":"reference/test_setups/#olfactory_navigation.test_setups.run_n_by_cell_test","title":"<code>run_n_by_cell_test(agent, cell_width=10, n_by_cell=10, environment=None, time_shift=0, time_loop=True, horizon=1000, reward_discount=0.99, print_progress=True, print_stats=True, use_gpu=False)</code>","text":"<p>Function to run a test with simulations starting in different cells across the available starting zones. A number n_by_cell determines how many simulations should start within each cell (the same position can be chosen multiple times).</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent to be tested</p> required <code>cell_width</code> <code>int</code> <p>The size of the sides of each cells to be considered.</p> <code>10</code> <code>n_by_cell</code> <code>int</code> <p>How many simulations should start within each cell.</p> <code>10</code> <code>environment</code> <code>Environment</code> <p>The environment to run the simulations in. By default, the environment linked to the agent will used. This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.</p> <code>None</code> <code>time_shift</code> <code>int or ndarray</code> <p>The time at which to start the olfactory simulation array. It can be either a single value, or n values.</p> <code>0</code> <code>time_loop</code> <code>bool</code> <p>Whether to loop the time if reaching the end. (starts back at 0)</p> <code>True</code> <code>horizon</code> <code>int</code> <p>The amount of steps to run the simulation for before killing the remaining simulations.</p> <code>1000</code> <code>reward_discount</code> <code>float</code> <p>How much a given reward is discounted based on how long it took to get it. It is purely used to compute the Average Discount Reward (ADR) after the simulation.</p> <code>0.99</code> <code>print_progress</code> <code>bool</code> <p>Wheter to show a progress bar of what step the simulations are at.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Wheter to print the stats at the end of the run.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run the simulations on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>hist</code> <code>SimulationHistory</code> <p>A SimulationHistory object that tracked all the positions, actions and observations.</p> Source code in <code>olfactory_navigation\\test_setups.py</code> <pre><code>def run_n_by_cell_test(\n             agent:Agent,\n             cell_width:int=10,\n             n_by_cell:int=10,\n             environment:Environment|None=None,\n             time_shift:int|np.ndarray=0,\n             time_loop:bool=True,\n             horizon:int=1000,\n             reward_discount:float=0.99,\n             print_progress:bool=True,\n             print_stats:bool=True,\n             use_gpu:bool=False\n             ) -&gt; SimulationHistory:\n    '''\n    Function to run a test with simulations starting in different cells across the available starting zones.\n    A number n_by_cell determines how many simulations should start within each cell (the same position can be chosen multiple times).\n\n    Parameters\n    ----------\n    agent : Agent\n        The agent to be tested\n    cell_width : int, default=10\n        The size of the sides of each cells to be considered.\n    n_by_cell : int, default=10\n        How many simulations should start within each cell.\n    environment : Environment, optional\n        The environment to run the simulations in.\n        By default, the environment linked to the agent will used.\n        This parameter is intended if the environment needs to be modified compared to environment the agent was trained on.\n    time_shift : int or np.ndarray, default=0\n        The time at which to start the olfactory simulation array.\n        It can be either a single value, or n values.\n    time_loop : bool, default=True\n        Whether to loop the time if reaching the end. (starts back at 0)\n    horizon : int, default=1000\n        The amount of steps to run the simulation for before killing the remaining simulations.\n    reward_discount : float, default=0.99\n        How much a given reward is discounted based on how long it took to get it.\n        It is purely used to compute the Average Discount Reward (ADR) after the simulation.\n    print_progress : bool, default=True\n        Wheter to show a progress bar of what step the simulations are at.\n    print_stats : bool, default=True\n        Wheter to print the stats at the end of the run.\n    use_gpu : bool, default=False\n        Whether to run the simulations on the GPU or not.\n\n    Returns\n    -------\n    hist : SimulationHistory\n        A SimulationHistory object that tracked all the positions, actions and observations.\n    '''\n    # Handle the case an specific environment is given\n    environment_provided = environment is not None\n    if environment_provided:\n        assert environment.shape == agent.environment.shape, \"The provided environment's shape doesn't match the environment has been trained on...\"\n    else:\n        environment = agent.environment\n\n    # Gathering starting points\n    cells_x = int(environment.shape[0] / cell_width)\n    cells_y = int(environment.shape[1] / cell_width)\n\n    indices = np.arange(np.prod(environment.shape), dtype=int)\n    indices_grid = indices.reshape(environment.shape)\n    all_chosen_indices = []\n\n    for i in range(cells_x):\n        for j in range(cells_y):\n            cell_probs = environment.start_probabilities[(i*cell_width):(i*cell_width)+cell_width, (j*cell_width):(j*cell_width)+cell_width]\n            if np.any(cell_probs &gt; 0):\n                cell_indices = indices_grid[(i*cell_width):(i*cell_width)+cell_width, (j*cell_width):(j*cell_width)+cell_width]\n                cell_probs /= np.sum(cell_probs)\n\n                chosen_indices = np.random.choice(cell_indices.ravel(), size=n_by_cell, replace=True, p=cell_probs.ravel()).tolist()\n                all_chosen_indices += chosen_indices\n\n    n = len(all_chosen_indices)\n    start_points = np.array(np.unravel_index(all_chosen_indices, environment.shape)).T\n\n    return run_test(\n        agent=agent,\n        n=n,\n        start_points=start_points,\n        environment=environment if environment_provided else None,\n        time_shift=time_shift,\n        time_loop=time_loop,\n        horizon=horizon,\n        reward_discount=reward_discount,\n        print_progress=print_progress,\n        print_stats=print_stats,\n        use_gpu=use_gpu\n    )\n</code></pre>"},{"location":"reference/agents/","title":"agents","text":""},{"location":"reference/agents/#olfactory_navigation.agents.FSVI_Agent","title":"<code>FSVI_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A particular flavor of the Point-Based Value Iteration based agent. The general concept relies on Model-Based reinforcement learning as described in: Pineau, J., Gordon, G., &amp; Thrun, S. (2003, August). Point-based value iteration: An anytime algorithm for POMDPs The Forward Search Value Iteration algorithm is described in: Shani, G., Brafman, R. I., &amp; Shimony, S. E. (2007, January). Forward Search Value Iteration for POMDPs</p> <p>The training consist in two steps:</p> <ul> <li> <p>Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).</p> </li> <li> <p>Backup: Using the generated belief points, the value function is updated.</p> </li> </ul> <p>The belief points are probability distributions over the state space and are therefore vectors of |S| elements.</p> <p>Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|. Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action. To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.</p> <p>Forward Search exploration concept: It relies of the solution of the Fully-Observable (MDP) problem to guide the exploration of belief points. It makes an agent start randomly in the environment and makes him take steps following the MDP solution while generating belief points along the way. Each time the expand function is called it starts generated a new set of belief points and the update function uses only the latest generated belief points to make update the value function.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>class FSVI_Agent(PBVI_Agent):\n    '''\n    A particular flavor of the Point-Based Value Iteration based agent.\n    The general concept relies on Model-Based reinforcement learning as described in: Pineau, J., Gordon, G., &amp; Thrun, S. (2003, August). Point-based value iteration: An anytime algorithm for POMDPs\n    The Forward Search Value Iteration algorithm is described in: Shani, G., Brafman, R. I., &amp; Shimony, S. E. (2007, January). Forward Search Value Iteration for POMDPs\n\n    The training consist in two steps:\n\n    - Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).\n\n    - Backup: Using the generated belief points, the value function is updated.\n\n    The belief points are probability distributions over the state space and are therefore vectors of |S| elements.\n\n    Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|.\n    Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action.\n    To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.\n\n    Forward Search exploration concept:\n    It relies of the solution of the Fully-Observable (MDP) problem to guide the exploration of belief points.\n    It makes an agent start randomly in the environment and makes him take steps following the MDP solution while generating belief points along the way.\n    Each time the expand function is called it starts generated a new set of belief points and the update function uses only the latest generated belief points to make update the value function.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               mdp_policy:ValueFunction,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.\n        It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take.\n        Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.\n        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.\n        Once the state is a goal state, the loop is done and the belief sequence is returned.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            A belief set containing a single belief to start the sequence with.\n            A random state will be chosen based on the probability distribution of the belief.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int\n            How many beliefs to be generated at most.\n        mdp_policy : ValueFunction\n            The mdp policy used to choose the action from with the given state 's'.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Getting initial belief\n        b0 = belief_set.belief_list[0]\n        belief_list = [b0]\n\n        # Choose a random starting state\n        s = b0.random_state()\n\n        # Setting the working belief\n        b = b0\n\n        for _ in range(max_generation - 1): #-1 due to a one belief already being present in the set\n            # Choose action based on mdp value function\n            a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])\n\n            # Pick a random next state (weighted by transition probabilities)\n            s_p = model.transition(s, a_star)\n\n            # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star\n            o = model.observe(s_p, a_star)\n\n            # Generate a new belief based on a_star and o\n            b_p = b.update(a_star, o)\n\n            # Record new belief\n            belief_list.append(b_p)\n\n            # Updating s and b\n            s = s_p\n            b = b_p\n\n            # Reset and belief if end state is reached\n            if s in model.end_states:\n                s = b0.random_state()\n                b = b0\n\n        return BeliefSet(model, belief_list)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              mdp_policy:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Foward Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        mdp_policy : ValueFunction, optional\n            The MDP solution to guide the expand process.\n            If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        if mdp_policy is None:\n            log('MDP_policy, not provided. Solving MDP with Value Iteration...')\n            mdp_policy, hist = vi_solver.solve(model = self.model,\n                                               horizon = 1000,\n                                               initial_value_function = initial_value_function,\n                                               gamma = gamma,\n                                               eps = eps,\n                                               use_gpu = use_gpu,\n                                               history_tracking_level = 1,\n                                               print_progress = print_progress)\n\n            if print_stats:\n                print(hist.summary)\n\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats,\n                             mdp_policy = mdp_policy)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.FSVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, mdp_policy, use_gpu=False)</code>","text":"<p>Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles. It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take. Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities. Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence. Once the state is a goal state, the loop is done and the belief sequence is returned.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>A belief set containing a single belief to start the sequence with. A random state will be chosen based on the probability distribution of the belief.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>How many beliefs to be generated at most.</p> required <code>mdp_policy</code> <code>ValueFunction</code> <p>The mdp policy used to choose the action from with the given state 's'.</p> required <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           mdp_policy:ValueFunction,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.\n    It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take.\n    Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.\n    Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.\n    Once the state is a goal state, the loop is done and the belief sequence is returned.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        A belief set containing a single belief to start the sequence with.\n        A random state will be chosen based on the probability distribution of the belief.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int\n        How many beliefs to be generated at most.\n    mdp_policy : ValueFunction\n        The mdp policy used to choose the action from with the given state 's'.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Getting initial belief\n    b0 = belief_set.belief_list[0]\n    belief_list = [b0]\n\n    # Choose a random starting state\n    s = b0.random_state()\n\n    # Setting the working belief\n    b = b0\n\n    for _ in range(max_generation - 1): #-1 due to a one belief already being present in the set\n        # Choose action based on mdp value function\n        a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])\n\n        # Pick a random next state (weighted by transition probabilities)\n        s_p = model.transition(s, a_star)\n\n        # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star\n        o = model.observe(s_p, a_star)\n\n        # Generate a new belief based on a_star and o\n        b_p = b.update(a_star, o)\n\n        # Record new belief\n        belief_list.append(b_p)\n\n        # Updating s and b\n        s = s_p\n        b = b_p\n\n        # Reset and belief if end state is reached\n        if s in model.end_states:\n            s = b0.random_state()\n            b = b0\n\n    return BeliefSet(model, belief_list)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.FSVI_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, mdp_policy=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Foward Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>mdp_policy</code> <code>ValueFunction</code> <p>The MDP solution to guide the expand process. If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          mdp_policy:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Foward Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    mdp_policy : ValueFunction, optional\n        The MDP solution to guide the expand process.\n        If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    if mdp_policy is None:\n        log('MDP_policy, not provided. Solving MDP with Value Iteration...')\n        mdp_policy, hist = vi_solver.solve(model = self.model,\n                                           horizon = 1000,\n                                           initial_value_function = initial_value_function,\n                                           gamma = gamma,\n                                           eps = eps,\n                                           use_gpu = use_gpu,\n                                           history_tracking_level = 1,\n                                           print_progress = print_progress)\n\n        if print_stats:\n            print(hist.summary)\n\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats,\n                         mdp_policy = mdp_policy)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.HSVI_Agent","title":"<code>HSVI_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. </p>"},{"location":"reference/agents/#olfactory_navigation.agents.HSVI_Agent--todo-do-document-of-hsvi-agent","title":"TODO: Do document of HSVI agent","text":""},{"location":"reference/agents/#olfactory_navigation.agents.HSVI_Agent--todo-fix-hsvi-expand","title":"TODO: FIX HSVI expand","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>class HSVI_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. \n\n    # TODO: Do document of HSVI agent\n    # TODO: FIX HSVI expand\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        The expand function of the  Heuristic Search Value Iteration (HSVI) technique.\n        It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.\n\n        It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. Used to compute the value at belief points.\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        if conv_term is None:\n            conv_term = self.eps\n\n        # Update convergence term\n        conv_term /= self.gamma\n\n        # Find best a based on upper bound v\n        max_qv = -xp.inf\n        best_a = -1\n        for a in model.actions:\n            b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n\n            b_prob_val = 0\n            for o in model.observations:\n                b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))\n\n            qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))\n\n            # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)\n            if qva &gt; max_qv:\n                max_qv = qva\n                best_a = a\n\n        # Choose o that max gap between bounds\n        b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,best_a,:,:], b.values)\n\n        max_o_val = -xp.inf\n        best_v_diff = -xp.inf\n        next_b = b\n\n        for o in model.observations:\n            bao = b.update(best_a, o)\n\n            upper_v_bao = upper_bound_belief_value_map.evaluate(bao)\n            lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))\n\n            v_diff = (upper_v_bao - lower_v_bao)\n\n            o_val = b_probs[o] * v_diff\n\n            if o_val &gt; max_o_val:\n                max_o_val = o_val\n                best_v_diff = v_diff\n                next_b = bao\n\n        # if bounds_split &lt; conv_term or max_generation &lt;= 0:\n        if best_v_diff &lt; conv_term or max_generation &lt;= 1:\n            return BeliefSet(model, [next_b])\n\n        # Add the belief point and associated value to the belief-value mapping\n        upper_bound_belief_value_map.add(b, max_qv)\n\n        # Go one step deeper in the recursion\n        b_set = self.expand_hsvi(model=model,\n                                 b=next_b,\n                                 value_function=value_function,\n                                 upper_bound_belief_value_map=upper_bound_belief_value_map,\n                                 conv_term=conv_term,\n                                 max_generation=max_generation-1)\n\n        # Append the nex belief of this iteration to the deeper beliefs\n        new_belief_list = b_set.belief_list\n        new_belief_list.append(next_b)\n\n        return BeliefSet(model, new_belief_list)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Heuristic Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.HSVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>The expand function of the  Heuristic Search Value Iteration (HSVI) technique. It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.</p> <p>It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. Used to compute the value at belief points.</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    The expand function of the  Heuristic Search Value Iteration (HSVI) technique.\n    It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.\n\n    It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. Used to compute the value at belief points.\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    if conv_term is None:\n        conv_term = self.eps\n\n    # Update convergence term\n    conv_term /= self.gamma\n\n    # Find best a based on upper bound v\n    max_qv = -xp.inf\n    best_a = -1\n    for a in model.actions:\n        b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n\n        b_prob_val = 0\n        for o in model.observations:\n            b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))\n\n        qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))\n\n        # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)\n        if qva &gt; max_qv:\n            max_qv = qva\n            best_a = a\n\n    # Choose o that max gap between bounds\n    b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,best_a,:,:], b.values)\n\n    max_o_val = -xp.inf\n    best_v_diff = -xp.inf\n    next_b = b\n\n    for o in model.observations:\n        bao = b.update(best_a, o)\n\n        upper_v_bao = upper_bound_belief_value_map.evaluate(bao)\n        lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))\n\n        v_diff = (upper_v_bao - lower_v_bao)\n\n        o_val = b_probs[o] * v_diff\n\n        if o_val &gt; max_o_val:\n            max_o_val = o_val\n            best_v_diff = v_diff\n            next_b = bao\n\n    # if bounds_split &lt; conv_term or max_generation &lt;= 0:\n    if best_v_diff &lt; conv_term or max_generation &lt;= 1:\n        return BeliefSet(model, [next_b])\n\n    # Add the belief point and associated value to the belief-value mapping\n    upper_bound_belief_value_map.add(b, max_qv)\n\n    # Go one step deeper in the recursion\n    b_set = self.expand_hsvi(model=model,\n                             b=next_b,\n                             value_function=value_function,\n                             upper_bound_belief_value_map=upper_bound_belief_value_map,\n                             conv_term=conv_term,\n                             max_generation=max_generation-1)\n\n    # Append the nex belief of this iteration to the deeper beliefs\n    new_belief_list = b_set.belief_list\n    new_belief_list.append(next_b)\n\n    return BeliefSet(model, new_belief_list)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.HSVI_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Heuristic Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Heuristic Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent","title":"<code>Infotaxis_Agent</code>","text":"<p>             Bases: <code>Agent</code></p> <p>An agent following the Infotaxis principle. It is a Model-Based approach that aims to make steps towards where the agent has the greatest likelihood to minimize the entropy of the belief. The belief is (as for the PBVI agent) a probability distribution over the state space of how much the agent is to be confident in each state. The technique was developped and described in the following article: Vergassola, M., Villermaux, E., &amp; Shraiman, B. I. (2007). 'Infotaxis' as a strategy for searching without gradients.</p> <p>It does not need to be trained to the train(), save() and load() function are not implemented.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>class Infotaxis_Agent(Agent):\n    '''\n    An agent following the Infotaxis principle.\n    It is a Model-Based approach that aims to make steps towards where the agent has the greatest likelihood to minimize the entropy of the belief.\n    The belief is (as for the PBVI agent) a probability distribution over the state space of how much the agent is to be confident in each state.\n    The technique was developped and described in the following article: Vergassola, M., Villermaux, E., &amp; Shraiman, B. I. (2007). 'Infotaxis' as a strategy for searching without gradients.\n\n    It does not need to be trained to the train(), save() and load() function are not implemented.\n\n    ...\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        super().__init__(\n            environment = environment,\n            threshold = threshold,\n            name = name\n        )\n\n        self.model = Model.from_environment(environment, threshold)\n\n        # Status variables\n        self.beliefs = None\n        self.action_played = None\n\n\n    def to_gpu(self) -&gt; Agent:\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        gpu_agent\n        '''\n        # Generating a new instance\n        cls = self.__class__\n        gpu_agent = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_agent, arg, cp.array(val))\n            elif isinstance(val, Model):\n                gpu_agent.model = self.model.gpu_model\n            elif isinstance(val, BeliefSet):\n                gpu_agent.beliefs = self.beliefs.to_gpu()\n            else:\n                setattr(gpu_agent, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_agent\n        gpu_agent._alternate_version = self\n\n        gpu_agent.on_gpu = True\n        return gpu_agent\n\n\n    def initialize_state(self,\n                         n:int=1\n                         ) -&gt; None:\n        '''\n        To use an agent within a simulation, the agent's state needs to be initialized.\n        The initialization consists of setting the agent's initial belief.\n        Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents are to be used during the simulation.\n        '''\n        self.beliefs = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to let the agent or set of agents choose an action based on their current belief.\n        Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A single or a list of actions chosen by the agent(s) based on their belief.\n        '''\n        xp = np if not self.on_gpu else cp\n\n        n = len(self.beliefs)\n\n        best_entropy = xp.ones(n) * -1\n        best_action = xp.ones(n, dtype=int) * -1\n\n        current_entropy = self.beliefs.entropies\n\n        for a in self.model.actions:\n            total_entropy = xp.zeros(n)\n\n            for o in self.model.observations:\n                b_ao = self.beliefs.update(actions=np.ones(n, dtype=int)*a,\n                                           observations=np.ones(n, dtype=int)*o,\n                                           throw_error=False)\n\n                # Computing entropy\n                with warnings.catch_warnings():\n                    warnings.simplefilter('ignore')\n                    b_ao_entropy = b_ao.entropies\n\n                b_prob = xp.dot(self.beliefs.belief_array, xp.sum(self.model.reachable_transitional_observation_table[:,a,o,:], axis=1))\n\n                total_entropy += (b_prob * (current_entropy - b_ao_entropy))\n\n            # Checking if action is superior to previous best\n            superiority_mask = best_entropy &lt; total_entropy\n            best_action[superiority_mask] = a\n            best_entropy[superiority_mask] = total_entropy[superiority_mask]\n\n        # Recording the action played\n        self.action_played = best_action\n\n        # Converting action indexes to movement vectors\n        movemement_vector = self.model.movement_vector[best_action,:]\n\n        return movemement_vector\n\n\n    def update_state(self,\n                     observation:int|np.ndarray,\n                     source_reached:bool|np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n        Parameters\n        ----------\n        observation : np.ndarray\n            The observation(s) the agent(s) made.\n        source_reached : np.ndarray\n            A boolean array of whether the agent(s) have reached the source or not.\n        '''\n        assert self.beliefs is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Binarize observations\n        observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n        observation_ids[source_reached] = 2 # Observe source\n\n        # Update the set of beliefs\n        self.beliefs = self.beliefs.update(actions=self.action_played, observations=observation_ids)\n\n        # Remove the beliefs of the agents having reached the source\n        self.beliefs = BeliefSet(self.model, self.beliefs.belief_array[~source_reached])\n\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that have not reached the source but can't continue further\n\n        Parameters\n        ----------\n        simulations_to_kill : np.ndarray\n            A boolean array of the simulations to kill.\n        '''\n        if all(simulations_to_kill):\n            self.beliefs = None\n        else:\n            self.beliefs = BeliefSet(self.beliefs.model, self.beliefs.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to let the agent or set of agents choose an action based on their current belief. Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A single or a list of actions chosen by the agent(s) based on their belief.</p> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to let the agent or set of agents choose an action based on their current belief.\n    Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A single or a list of actions chosen by the agent(s) based on their belief.\n    '''\n    xp = np if not self.on_gpu else cp\n\n    n = len(self.beliefs)\n\n    best_entropy = xp.ones(n) * -1\n    best_action = xp.ones(n, dtype=int) * -1\n\n    current_entropy = self.beliefs.entropies\n\n    for a in self.model.actions:\n        total_entropy = xp.zeros(n)\n\n        for o in self.model.observations:\n            b_ao = self.beliefs.update(actions=np.ones(n, dtype=int)*a,\n                                       observations=np.ones(n, dtype=int)*o,\n                                       throw_error=False)\n\n            # Computing entropy\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                b_ao_entropy = b_ao.entropies\n\n            b_prob = xp.dot(self.beliefs.belief_array, xp.sum(self.model.reachable_transitional_observation_table[:,a,o,:], axis=1))\n\n            total_entropy += (b_prob * (current_entropy - b_ao_entropy))\n\n        # Checking if action is superior to previous best\n        superiority_mask = best_entropy &lt; total_entropy\n        best_action[superiority_mask] = a\n        best_entropy[superiority_mask] = total_entropy[superiority_mask]\n\n    # Recording the action played\n    self.action_played = best_action\n\n    # Converting action indexes to movement vectors\n    movemement_vector = self.model.movement_vector[best_action,:]\n\n    return movemement_vector\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>To use an agent within a simulation, the agent's state needs to be initialized. The initialization consists of setting the agent's initial belief. Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents are to be used during the simulation.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def initialize_state(self,\n                     n:int=1\n                     ) -&gt; None:\n    '''\n    To use an agent within a simulation, the agent's state needs to be initialized.\n    The initialization consists of setting the agent's initial belief.\n    Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents are to be used during the simulation.\n    '''\n    self.beliefs = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that have not reached the source but can't continue further</p> <p>Parameters:</p> Name Type Description Default <code>simulations_to_kill</code> <code>ndarray</code> <p>A boolean array of the simulations to kill.</p> required Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that have not reached the source but can't continue further\n\n    Parameters\n    ----------\n    simulations_to_kill : np.ndarray\n        A boolean array of the simulations to kill.\n    '''\n    if all(simulations_to_kill):\n        self.beliefs = None\n    else:\n        self.beliefs = BeliefSet(self.beliefs.model, self.beliefs.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Type Description <code>gpu_agent</code> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def to_gpu(self) -&gt; Agent:\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    gpu_agent\n    '''\n    # Generating a new instance\n    cls = self.__class__\n    gpu_agent = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_agent, arg, cp.array(val))\n        elif isinstance(val, Model):\n            gpu_agent.model = self.model.gpu_model\n        elif isinstance(val, BeliefSet):\n            gpu_agent.beliefs = self.beliefs.to_gpu()\n        else:\n            setattr(gpu_agent, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_agent\n    gpu_agent._alternate_version = self\n\n    gpu_agent.on_gpu = True\n    return gpu_agent\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Infotaxis_Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>ndarray</code> <p>The observation(s) the agent(s) made.</p> required <code>source_reached</code> <code>ndarray</code> <p>A boolean array of whether the agent(s) have reached the source or not.</p> required Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def update_state(self,\n                 observation:int|np.ndarray,\n                 source_reached:bool|np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n    Parameters\n    ----------\n    observation : np.ndarray\n        The observation(s) the agent(s) made.\n    source_reached : np.ndarray\n        A boolean array of whether the agent(s) have reached the source or not.\n    '''\n    assert self.beliefs is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Binarize observations\n    observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n    observation_ids[source_reached] = 2 # Observe source\n\n    # Update the set of beliefs\n    self.beliefs = self.beliefs.update(actions=self.action_played, observations=observation_ids)\n\n    # Remove the beliefs of the agents having reached the source\n    self.beliefs = BeliefSet(self.model, self.beliefs.belief_array[~source_reached])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent","title":"<code>PBVI_Agent</code>","text":"<p>             Bases: <code>Agent</code></p> <p>A generic Point-Based Value Iteration based agent. It relies on Model-Based reinforcement learning as described in: Pineau J. et al, Point-based value iteration: An anytime algorithm for POMDPs The training consist in two steps:</p> <ul> <li> <p>Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).</p> </li> <li> <p>Backup: Using the generated belief points, the value function is updated.</p> </li> </ul> <p>The belief points are probability distributions over the state space and are therefore vectors of |S| elements.</p> <p>Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|. Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action. To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>class PBVI_Agent(Agent):\n    '''\n    A generic Point-Based Value Iteration based agent. It relies on Model-Based reinforcement learning as described in: Pineau J. et al, Point-based value iteration: An anytime algorithm for POMDPs\n    The training consist in two steps:\n\n    - Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).\n\n    - Backup: Using the generated belief points, the value function is updated.\n\n    The belief points are probability distributions over the state space and are therefore vectors of |S| elements.\n\n    Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|.\n    Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action.\n    To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        super().__init__(\n            environment=environment,\n            threshold=threshold,\n            name=name\n        )\n\n        self.model = Model.from_environment(environment, threshold)\n\n        # Trainable variables\n        self.trained_at = None\n        self.value_function = None\n\n        # Status variables\n        self.belief = None\n        self.action_played = None\n\n\n    def to_gpu(self) -&gt; Agent:\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        gpu_agent : Agent\n            A copy of the agent with the arrays on the GPU.\n        '''\n        # Generating a new instance\n        cls = self.__class__\n        gpu_agent = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_agent, arg, cp.array(val))\n            elif isinstance(val, Model):\n                gpu_agent.model = self.model.gpu_model\n            elif isinstance(val, ValueFunction):\n                gpu_agent.value_function =self.value_function.to_gpu()\n            elif isinstance(val, BeliefSet):\n                gpu_agent.belief = self.belief.to_gpu()\n            else:\n                setattr(gpu_agent, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_agent\n        gpu_agent._alternate_version = self\n\n        gpu_agent.on_gpu = True\n        return gpu_agent\n\n\n    def save(self,\n             folder:str|None=None,\n             force:bool=False,\n             save_environment:bool=False\n             ) -&gt; None:\n        '''\n        The save function for PBVI Agents consists in recording the value function after the training.\n        It saves the agent in a folder with the name of the agent (class name + training timestamp).\n        In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.\n\n        Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse.\n        If the agent has already been saved, the saving will not happen unless the force parameter is toggled.\n\n        Parameters\n        ----------\n        folder : str, optional\n            The folder under which to save the agent (a subfolder will be created under this folder).\n            The agent will therefore be saved at &lt;folder&gt;/Agent-&lt;agent_name&gt; .\n            By default the current folder is used.\n        force : bool, default=False\n            Whether to overwrite an already saved agent with the same name at the same path.\n        save_environment : bool, default=False\n            Whether to save the environment data along with the agent.\n        '''\n        assert self.trained_at is not None, \"The agent is not trained, there is nothing to save.\"\n\n        # Adding env name to folder path\n        if folder is None:\n            folder = f'./Agent-{self.name}'\n        else:\n            folder += '/Agent-' + self.name\n\n        # Checking the folder exists or creates it\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n        elif len(os.listdir(folder)):\n            if force:\n                shutil.rmtree(folder)\n                os.mkdir(folder)\n            else:\n                raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n        # If requested save environment\n        if save_environment:\n            self.environment.save(folder=folder)\n\n        # Generating the metadata arguments dictionary\n        arguments = {}\n        arguments['name'] = self.name\n        arguments['class'] = self.class_name\n        arguments['threshold'] = self.threshold\n        arguments['environment_name'] = self.environment.name\n        arguments['environment_saved_at'] = self.environment.saved_at\n        arguments['trained_at'] = self.trained_at\n\n        # Output the arguments to a METADATA file\n        with open(folder + '/METADATA.json', 'w') as json_file:\n            json.dump(arguments, json_file, indent=4)\n\n        # Save value function\n        self.value_function.save(folder=folder, file_name='Value_Function.npy')\n\n        # Finalization\n        self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n        print(f'Agent saved to: {folder}')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'PBVI_Agent':\n        '''\n        Function to load a PBVI agent from a given folder it has been saved to.\n        It will load the environment the agent has been trained on along with it.\n\n        If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.\n\n        Parameters\n        ----------\n        folder : str\n            The agent folder.\n\n        Returns\n        -------\n        instance : PBVI_Agent\n            The loaded instance of the PBVI Agent.\n        '''\n        # Load arguments\n        arguments = None\n        with open(folder + '/METADATA.json', 'r') as json_file:\n            arguments = json.load(json_file)\n\n        # Load environment\n        environment = Environment.load(arguments['environment_saved_at'])\n\n        # Load specific class\n        if arguments['class'] != 'PBVI_Agent':\n            from olfactory_navigation import agents\n            cls = {name:obj for name, obj in inspect.getmembers(agents)}[arguments['class']]\n\n        # Build instance\n        instance = cls(\n            environment=environment,\n            threshold=arguments['threshold'],\n            name=arguments['name']\n        )\n\n        # Load and set the value function on the instance\n        instance.value_function = ValueFunction.load(\n            file=folder + '/Value_Function.npy',\n            model=instance.model\n        )\n        instance.trained_at = arguments['trained_at']\n        instance.saved_at = folder\n\n        return instance\n\n\n    def train(self,\n              expansions:int,\n              full_backup:bool=True,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:Union[BeliefSet, Belief, None]=None,\n              initial_value_function:Union[ValueFunction,None]=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False, # TODO rehandle the way things are run on GPU\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True,\n              **expand_arguments\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        full_backup : bool, default=True\n            Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n        expand_arguments : kwargs\n            An arbitrary amount of parameters that will be passed on to the expand function.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Initial belief\n        if initial_belief is None:\n            belief_set = BeliefSet(model, [Belief(model)])\n        elif isinstance(initial_belief, BeliefSet):\n            belief_set = initial_belief.to_gpu() if use_gpu else initial_belief \n        else:\n            initial_belief = Belief(model, xp.array(initial_belief.values))\n            belief_set = BeliefSet(model, [initial_belief])\n\n        # Handeling the case where the agent is already trained\n        if (self.value_function is not None):\n            if not force:\n                raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n            else:\n                self.trained_at = None\n                self.name = '-'.join(self.name.split('-')[:-1])\n                self.value_function = None\n\n        # Initial value function\n        if initial_value_function is None:\n            value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)\n        else:\n            value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function\n\n        # Convergence check boundary\n        max_allowed_change = eps * (gamma / (1-gamma))\n\n        # History tracking\n        training_history = TrainingHistory(tracking_level=history_tracking_level,\n                                           model=model,\n                                           gamma=gamma,\n                                           eps=eps,\n                                           expand_append=full_backup,\n                                           initial_value_function=value_function,\n                                           initial_belief_set=belief_set)\n\n        # Loop\n        iteration = 0\n        expand_value_function = value_function\n        old_value_function = value_function\n\n        try:\n            iterator = trange(expansions, desc='Expansions') if print_progress else range(expansions)\n            iterator_postfix = {}\n            for expansion_i in iterator:\n\n                # 1: Expand belief set\n                start_ts = datetime.now()\n\n                new_belief_set = self.expand(belief_set=belief_set,\n                                             value_function=value_function,\n                                             max_generation=max_belief_growth,\n                                             use_gpu=use_gpu,\n                                             **expand_arguments)\n\n                # Add new beliefs points to the total belief_set\n                belief_set = belief_set.union(new_belief_set)\n\n                expand_time = (datetime.now() - start_ts).total_seconds()\n                training_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)\n\n                # 2: Backup, update value function (alpha vector set)\n                for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f'Backups {expansion_i}'):\n                    start_ts = datetime.now()\n\n                    # Backup step\n                    value_function = self.backup(belief_set if full_backup else new_belief_set,\n                                                 value_function,\n                                                 gamma=gamma,\n                                                 append=(not full_backup),\n                                                 belief_dominance_prune=False,\n                                                 use_gpu=use_gpu)\n                    backup_time = (datetime.now() - start_ts).total_seconds()\n\n                    # Additional pruning\n                    if (iteration % prune_interval) == 0 and iteration &gt; 0:\n                        start_ts = datetime.now()\n                        vf_len = len(value_function)\n\n                        value_function.prune(prune_level)\n\n                        prune_time = (datetime.now() - start_ts).total_seconds()\n                        alpha_vectors_pruned = len(value_function) - vf_len\n                        training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n                    # Check if value function size is above threshold\n                    if limit_value_function_size &gt;= 0 and len(value_function) &gt; limit_value_function_size:\n                        # Compute matrix multiplications between avs and beliefs\n                        alpha_value_per_belief = xp.matmul(value_function.alpha_vector_array, belief_set.belief_array.T)\n\n                        # Select the useful alpha vectors\n                        best_alpha_vector_per_belief = xp.argmax(alpha_value_per_belief, axis=0)\n                        useful_alpha_vectors = xp.unique(best_alpha_vector_per_belief)\n\n                        # Select a random selection of vectors to delete\n                        unuseful_alpha_vectors = xp.delete(xp.arange(len(value_function)), useful_alpha_vectors)\n                        random_vectors_to_delete = xp.random.choice(unuseful_alpha_vectors,\n                                                                    size=max_belief_growth,\n                                                                    p=(xp.arange(len(unuseful_alpha_vectors))[::-1] / xp.sum(xp.arange(len(unuseful_alpha_vectors)))))\n                                                                    # replace=False,\n                                                                    # p=1/len(unuseful_alpha_vectors))\n\n                        value_function = ValueFunction(model=model,\n                                                       alpha_vectors=xp.delete(value_function.alpha_vector_array, random_vectors_to_delete, axis=0),\n                                                       action_list=xp.delete(value_function.actions, random_vectors_to_delete))\n\n                        iterator_postfix['|useful|'] = useful_alpha_vectors.shape[0]\n\n                    # Compute the change between value functions\n                    max_change = self.compute_change(value_function, old_value_function, belief_set)\n\n                    # History tracking\n                    training_history.add_backup_step(backup_time, max_change, value_function)\n\n                    # Convergence check\n                    if max_change &lt; max_allowed_change:\n                        break\n\n                    old_value_function = value_function\n\n                    # Update iteration counter\n                    iteration += 1\n\n                # Compute change with old expansion value function\n                expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)\n\n                if expand_max_change &lt; max_allowed_change:\n                    print('Converged!')\n                    break\n\n                expand_value_function = value_function\n\n                iterator_postfix['|V|'] = len(value_function)\n                iterator_postfix['|B|'] = len(belief_set)\n\n                if print_progress:\n                    iterator.set_postfix(iterator_postfix)\n\n        except MemoryError as e:\n            print(f'Memory full: {e}')\n            print('Returning value function and history as is...\\n')\n\n        # Final pruning\n        start_ts = datetime.now()\n        vf_len = len(value_function)\n\n        value_function.prune(prune_level)\n\n        # History tracking\n        prune_time = (datetime.now() - start_ts).total_seconds()\n        alpha_vectors_pruned = len(value_function) - vf_len\n        training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n        # Record when it was trained\n        self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n        self.name += f'-trained_{self.trained_at}'\n\n        self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n        # Print stats if requested\n        if print_stats:\n            print(training_history.summary)\n\n        return training_history\n\n\n    def compute_change(self,\n                       value_function:ValueFunction,\n                       new_value_function:ValueFunction,\n                       belief_set:BeliefSet\n                       ) -&gt; float:\n        '''\n        Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.\n        It check for each belief, the maximum value and take the max change between believe's value functions.\n        If this max change is lower than eps * (gamma / (1 - gamma)).\n\n        Parameters\n        ----------\n        value_function : ValueFunction\n            The first value function to compare.\n        new_value_function : ValueFunction\n            The second value function to compare.\n        belief_set : BeliefSet\n            The set of believes to check the values on to compute the max change on.\n\n        Returns\n        -------\n        max_change : float\n            The maximum change between value functions at belief points.\n        '''\n        # Get numpy corresponding to the arrays\n        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)\n\n        # Computing Delta for each beliefs\n        max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)\n        new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)\n        max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))\n\n        return max_change\n\n\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False, # TODO Remove this\n               **kwargs\n               ) -&gt; BeliefSet:\n        '''\n        Abstract function!\n        This function should be implemented in subclasses.\n        The expand function consists in the exploration of the belief set.\n        It takes as input a belief set and generates at most 'max_generation' beliefs from it.\n\n        The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            The belief or set of beliefs to be used as a starting point for the exploration.\n        value_function : ValueFunction\n            The current value function. To be used to guide the exploration process.\n        max_generation : int\n            How many beliefs to be generated at most.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n        kwargs\n            Special parameters for the particular flavors of the PBVI Agent.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            A new (or expanded) set of beliefs.\n        '''\n        raise NotImplementedError('PBVI class is abstract so expand function is not implemented, make an PBVI_agent subclass to implement the method')\n\n\n    def backup(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               gamma:float=0.99,\n               append:bool=False,\n               belief_dominance_prune:bool=True,\n               use_gpu:bool=False # TODO Remove this\n               ) -&gt; ValueFunction:\n        '''\n        This function has purpose to update the set of alpha vectors. It does so in 3 steps:\n        1. It creates projections from each alpha vector for each possible action and each possible observation\n        2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.\n        3. Then it further collapses the set to take the best alpha vector and action per belief\n        In the end we have a set of alpha vectors as large as the amount of beliefs.\n\n        The alpha vectors are also pruned to avoid duplicates and remove dominated ones.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            The belief set to use to generate the new alpha vectors with.\n        value_function : ValueFunction\n            The alpha vectors to generate the new set from.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        append : bool, default=False\n            Whether to append the new alpha vectors generated to the old alpha vectors before pruning.\n        belief_dominance_prune : bool, default=True\n            Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.\n        use_gpu : bool, default=False\n\n        Returns\n        -------\n        new_alpha_set : ValueFunction\n            A list of updated alpha vectors.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Step 1\n        vector_array = value_function.alpha_vector_array\n        vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]\n\n        gamma_a_o_t = gamma * xp.einsum('saor,vsar-&gt;aovs', model.reachable_transitional_observation_table, vectors_array_reachable_states)\n\n        # Step 2\n        belief_array = belief_set.belief_array # bs\n        best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao\n\n        best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos\n\n        alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas\n\n        # Step 3\n        best_actions = xp.argmax(xp.einsum('bas,bs-&gt;ba', alpha_a, belief_array), axis=1)\n        alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]\n\n        # Belief domination\n        if belief_dominance_prune:\n            best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)\n            old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)\n            dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief\n\n            best_actions = best_actions[dominating_vectors]\n            alpha_vectors = alpha_vectors[dominating_vectors]\n\n        # Creation of value function\n        new_value_function = ValueFunction(model, alpha_vectors, best_actions)\n\n        # Union with previous value function\n        if append:\n            new_value_function.extend(value_function)\n\n        return new_value_function\n\n\n    def initialize_state(self, n:int=1) -&gt; None:\n        '''\n        To use an agent within a simulation, the agent's state needs to be initialized.\n        The initialization consists of setting the agent's initial belief.\n        Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents are to be used during the simulation.\n        '''\n        assert self.value_function is not None, \"Agent was not trained, run the training function first...\"\n\n        self.belief = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to let the agent or set of agents choose an action based on their current belief.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A single or a list of actions chosen by the agent(s) based on their belief.\n        '''\n        assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Evaluated value function\n        _, action = self.value_function.evaluate_at(self.belief)\n\n        # Recording the action played\n        self.action_played = action\n\n        # Converting action indexes to movement vectors\n        movemement_vector = self.model.movement_vector[action,:]\n\n        return movemement_vector\n\n\n    def update_state(self,\n                     observation:np.ndarray,\n                     source_reached:np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n        Parameters\n        ----------\n        observation : np.ndarray\n            The observation(s) the agent(s) made.\n        source_reached : np.ndarray\n            A boolean array of whether the agent(s) have reached the source or not.\n        '''\n        assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Binarize observations\n        observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n        observation_ids[source_reached] = 2 # Observe source\n\n        # Update the set of beliefs\n        self.belief = self.belief.update(actions=self.action_played, observations=observation_ids)\n\n        # Remove the beliefs of the agents having reached the source\n        self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~source_reached])\n\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that have not reached the source but can't continue further\n\n        Parameters\n        ----------\n        simulations_to_kill : np.ndarray\n            A boolean array of the simulations to kill.\n        '''\n        if all(simulations_to_kill):\n            self.belief = None\n        else:\n            self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.backup","title":"<code>backup(belief_set, value_function, gamma=0.99, append=False, belief_dominance_prune=True, use_gpu=False)</code>","text":"<p>This function has purpose to update the set of alpha vectors. It does so in 3 steps: 1. It creates projections from each alpha vector for each possible action and each possible observation 2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief. 3. Then it further collapses the set to take the best alpha vector and action per belief In the end we have a set of alpha vectors as large as the amount of beliefs.</p> <p>The alpha vectors are also pruned to avoid duplicates and remove dominated ones.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>The belief set to use to generate the new alpha vectors with.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The alpha vectors to generate the new set from.</p> required <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>append</code> <code>bool</code> <p>Whether to append the new alpha vectors generated to the old alpha vectors before pruning.</p> <code>False</code> <code>belief_dominance_prune</code> <code>bool</code> <p>Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <code>False</code> <p>Returns:</p> Name Type Description <code>new_alpha_set</code> <code>ValueFunction</code> <p>A list of updated alpha vectors.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def backup(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           gamma:float=0.99,\n           append:bool=False,\n           belief_dominance_prune:bool=True,\n           use_gpu:bool=False # TODO Remove this\n           ) -&gt; ValueFunction:\n    '''\n    This function has purpose to update the set of alpha vectors. It does so in 3 steps:\n    1. It creates projections from each alpha vector for each possible action and each possible observation\n    2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.\n    3. Then it further collapses the set to take the best alpha vector and action per belief\n    In the end we have a set of alpha vectors as large as the amount of beliefs.\n\n    The alpha vectors are also pruned to avoid duplicates and remove dominated ones.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        The belief set to use to generate the new alpha vectors with.\n    value_function : ValueFunction\n        The alpha vectors to generate the new set from.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    append : bool, default=False\n        Whether to append the new alpha vectors generated to the old alpha vectors before pruning.\n    belief_dominance_prune : bool, default=True\n        Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.\n    use_gpu : bool, default=False\n\n    Returns\n    -------\n    new_alpha_set : ValueFunction\n        A list of updated alpha vectors.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Step 1\n    vector_array = value_function.alpha_vector_array\n    vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]\n\n    gamma_a_o_t = gamma * xp.einsum('saor,vsar-&gt;aovs', model.reachable_transitional_observation_table, vectors_array_reachable_states)\n\n    # Step 2\n    belief_array = belief_set.belief_array # bs\n    best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao\n\n    best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos\n\n    alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas\n\n    # Step 3\n    best_actions = xp.argmax(xp.einsum('bas,bs-&gt;ba', alpha_a, belief_array), axis=1)\n    alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]\n\n    # Belief domination\n    if belief_dominance_prune:\n        best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)\n        old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)\n        dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief\n\n        best_actions = best_actions[dominating_vectors]\n        alpha_vectors = alpha_vectors[dominating_vectors]\n\n    # Creation of value function\n    new_value_function = ValueFunction(model, alpha_vectors, best_actions)\n\n    # Union with previous value function\n    if append:\n        new_value_function.extend(value_function)\n\n    return new_value_function\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to let the agent or set of agents choose an action based on their current belief.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A single or a list of actions chosen by the agent(s) based on their belief.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to let the agent or set of agents choose an action based on their current belief.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A single or a list of actions chosen by the agent(s) based on their belief.\n    '''\n    assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Evaluated value function\n    _, action = self.value_function.evaluate_at(self.belief)\n\n    # Recording the action played\n    self.action_played = action\n\n    # Converting action indexes to movement vectors\n    movemement_vector = self.model.movement_vector[action,:]\n\n    return movemement_vector\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.compute_change","title":"<code>compute_change(value_function, new_value_function, belief_set)</code>","text":"<p>Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver. It check for each belief, the maximum value and take the max change between believe's value functions. If this max change is lower than eps * (gamma / (1 - gamma)).</p> <p>Parameters:</p> Name Type Description Default <code>value_function</code> <code>ValueFunction</code> <p>The first value function to compare.</p> required <code>new_value_function</code> <code>ValueFunction</code> <p>The second value function to compare.</p> required <code>belief_set</code> <code>BeliefSet</code> <p>The set of believes to check the values on to compute the max change on.</p> required <p>Returns:</p> Name Type Description <code>max_change</code> <code>float</code> <p>The maximum change between value functions at belief points.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def compute_change(self,\n                   value_function:ValueFunction,\n                   new_value_function:ValueFunction,\n                   belief_set:BeliefSet\n                   ) -&gt; float:\n    '''\n    Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.\n    It check for each belief, the maximum value and take the max change between believe's value functions.\n    If this max change is lower than eps * (gamma / (1 - gamma)).\n\n    Parameters\n    ----------\n    value_function : ValueFunction\n        The first value function to compare.\n    new_value_function : ValueFunction\n        The second value function to compare.\n    belief_set : BeliefSet\n        The set of believes to check the values on to compute the max change on.\n\n    Returns\n    -------\n    max_change : float\n        The maximum change between value functions at belief points.\n    '''\n    # Get numpy corresponding to the arrays\n    xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)\n\n    # Computing Delta for each beliefs\n    max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)\n    new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)\n    max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))\n\n    return max_change\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False, **kwargs)</code>","text":"<p>Abstract function! This function should be implemented in subclasses. The expand function consists in the exploration of the belief set. It takes as input a belief set and generates at most 'max_generation' beliefs from it.</p> <p>The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>The belief or set of beliefs to be used as a starting point for the exploration.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. To be used to guide the exploration process.</p> required <code>max_generation</code> <code>int</code> <p>How many beliefs to be generated at most.</p> required <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <code>kwargs</code> <p>Special parameters for the particular flavors of the PBVI Agent.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>A new (or expanded) set of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False, # TODO Remove this\n           **kwargs\n           ) -&gt; BeliefSet:\n    '''\n    Abstract function!\n    This function should be implemented in subclasses.\n    The expand function consists in the exploration of the belief set.\n    It takes as input a belief set and generates at most 'max_generation' beliefs from it.\n\n    The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        The belief or set of beliefs to be used as a starting point for the exploration.\n    value_function : ValueFunction\n        The current value function. To be used to guide the exploration process.\n    max_generation : int\n        How many beliefs to be generated at most.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n    kwargs\n        Special parameters for the particular flavors of the PBVI Agent.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        A new (or expanded) set of beliefs.\n    '''\n    raise NotImplementedError('PBVI class is abstract so expand function is not implemented, make an PBVI_agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>To use an agent within a simulation, the agent's state needs to be initialized. The initialization consists of setting the agent's initial belief. Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents are to be used during the simulation.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def initialize_state(self, n:int=1) -&gt; None:\n    '''\n    To use an agent within a simulation, the agent's state needs to be initialized.\n    The initialization consists of setting the agent's initial belief.\n    Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents are to be used during the simulation.\n    '''\n    assert self.value_function is not None, \"Agent was not trained, run the training function first...\"\n\n    self.belief = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that have not reached the source but can't continue further</p> <p>Parameters:</p> Name Type Description Default <code>simulations_to_kill</code> <code>ndarray</code> <p>A boolean array of the simulations to kill.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that have not reached the source but can't continue further\n\n    Parameters\n    ----------\n    simulations_to_kill : np.ndarray\n        A boolean array of the simulations to kill.\n    '''\n    if all(simulations_to_kill):\n        self.belief = None\n    else:\n        self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load a PBVI agent from a given folder it has been saved to. It will load the environment the agent has been trained on along with it.</p> <p>If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The agent folder.</p> required <p>Returns:</p> Name Type Description <code>instance</code> <code>PBVI_Agent</code> <p>The loaded instance of the PBVI Agent.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'PBVI_Agent':\n    '''\n    Function to load a PBVI agent from a given folder it has been saved to.\n    It will load the environment the agent has been trained on along with it.\n\n    If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.\n\n    Parameters\n    ----------\n    folder : str\n        The agent folder.\n\n    Returns\n    -------\n    instance : PBVI_Agent\n        The loaded instance of the PBVI Agent.\n    '''\n    # Load arguments\n    arguments = None\n    with open(folder + '/METADATA.json', 'r') as json_file:\n        arguments = json.load(json_file)\n\n    # Load environment\n    environment = Environment.load(arguments['environment_saved_at'])\n\n    # Load specific class\n    if arguments['class'] != 'PBVI_Agent':\n        from olfactory_navigation import agents\n        cls = {name:obj for name, obj in inspect.getmembers(agents)}[arguments['class']]\n\n    # Build instance\n    instance = cls(\n        environment=environment,\n        threshold=arguments['threshold'],\n        name=arguments['name']\n    )\n\n    # Load and set the value function on the instance\n    instance.value_function = ValueFunction.load(\n        file=folder + '/Value_Function.npy',\n        model=instance.model\n    )\n    instance.trained_at = arguments['trained_at']\n    instance.saved_at = folder\n\n    return instance\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.save","title":"<code>save(folder=None, force=False, save_environment=False)</code>","text":"<p>The save function for PBVI Agents consists in recording the value function after the training. It saves the agent in a folder with the name of the agent (class name + training timestamp). In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.</p> <p>Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse. If the agent has already been saved, the saving will not happen unless the force parameter is toggled.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder under which to save the agent (a subfolder will be created under this folder). The agent will therefore be saved at /Agent- . By default the current folder is used. <code>None</code> <code>force</code> <code>bool</code> <p>Whether to overwrite an already saved agent with the same name at the same path.</p> <code>False</code> <code>save_environment</code> <code>bool</code> <p>Whether to save the environment data along with the agent.</p> <code>False</code> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         force:bool=False,\n         save_environment:bool=False\n         ) -&gt; None:\n    '''\n    The save function for PBVI Agents consists in recording the value function after the training.\n    It saves the agent in a folder with the name of the agent (class name + training timestamp).\n    In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.\n\n    Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse.\n    If the agent has already been saved, the saving will not happen unless the force parameter is toggled.\n\n    Parameters\n    ----------\n    folder : str, optional\n        The folder under which to save the agent (a subfolder will be created under this folder).\n        The agent will therefore be saved at &lt;folder&gt;/Agent-&lt;agent_name&gt; .\n        By default the current folder is used.\n    force : bool, default=False\n        Whether to overwrite an already saved agent with the same name at the same path.\n    save_environment : bool, default=False\n        Whether to save the environment data along with the agent.\n    '''\n    assert self.trained_at is not None, \"The agent is not trained, there is nothing to save.\"\n\n    # Adding env name to folder path\n    if folder is None:\n        folder = f'./Agent-{self.name}'\n    else:\n        folder += '/Agent-' + self.name\n\n    # Checking the folder exists or creates it\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    elif len(os.listdir(folder)):\n        if force:\n            shutil.rmtree(folder)\n            os.mkdir(folder)\n        else:\n            raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n    # If requested save environment\n    if save_environment:\n        self.environment.save(folder=folder)\n\n    # Generating the metadata arguments dictionary\n    arguments = {}\n    arguments['name'] = self.name\n    arguments['class'] = self.class_name\n    arguments['threshold'] = self.threshold\n    arguments['environment_name'] = self.environment.name\n    arguments['environment_saved_at'] = self.environment.saved_at\n    arguments['trained_at'] = self.trained_at\n\n    # Output the arguments to a METADATA file\n    with open(folder + '/METADATA.json', 'w') as json_file:\n        json.dump(arguments, json_file, indent=4)\n\n    # Save value function\n    self.value_function.save(folder=folder, file_name='Value_Function.npy')\n\n    # Finalization\n    self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n    print(f'Agent saved to: {folder}')\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Name Type Description <code>gpu_agent</code> <code>Agent</code> <p>A copy of the agent with the arrays on the GPU.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def to_gpu(self) -&gt; Agent:\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    gpu_agent : Agent\n        A copy of the agent with the arrays on the GPU.\n    '''\n    # Generating a new instance\n    cls = self.__class__\n    gpu_agent = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_agent, arg, cp.array(val))\n        elif isinstance(val, Model):\n            gpu_agent.model = self.model.gpu_model\n        elif isinstance(val, ValueFunction):\n            gpu_agent.value_function =self.value_function.to_gpu()\n        elif isinstance(val, BeliefSet):\n            gpu_agent.belief = self.belief.to_gpu()\n        else:\n            setattr(gpu_agent, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_agent\n    gpu_agent._alternate_version = self\n\n    gpu_agent.on_gpu = True\n    return gpu_agent\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.train","title":"<code>train(expansions, full_backup=True, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True, **expand_arguments)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>full_backup</code> <code>bool</code> <p>Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.</p> <code>True</code> <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <code>expand_arguments</code> <code>kwargs</code> <p>An arbitrary amount of parameters that will be passed on to the expand function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          full_backup:bool=True,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:Union[BeliefSet, Belief, None]=None,\n          initial_value_function:Union[ValueFunction,None]=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False, # TODO rehandle the way things are run on GPU\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True,\n          **expand_arguments\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    full_backup : bool, default=True\n        Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n    expand_arguments : kwargs\n        An arbitrary amount of parameters that will be passed on to the expand function.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Initial belief\n    if initial_belief is None:\n        belief_set = BeliefSet(model, [Belief(model)])\n    elif isinstance(initial_belief, BeliefSet):\n        belief_set = initial_belief.to_gpu() if use_gpu else initial_belief \n    else:\n        initial_belief = Belief(model, xp.array(initial_belief.values))\n        belief_set = BeliefSet(model, [initial_belief])\n\n    # Handeling the case where the agent is already trained\n    if (self.value_function is not None):\n        if not force:\n            raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n        else:\n            self.trained_at = None\n            self.name = '-'.join(self.name.split('-')[:-1])\n            self.value_function = None\n\n    # Initial value function\n    if initial_value_function is None:\n        value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)\n    else:\n        value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function\n\n    # Convergence check boundary\n    max_allowed_change = eps * (gamma / (1-gamma))\n\n    # History tracking\n    training_history = TrainingHistory(tracking_level=history_tracking_level,\n                                       model=model,\n                                       gamma=gamma,\n                                       eps=eps,\n                                       expand_append=full_backup,\n                                       initial_value_function=value_function,\n                                       initial_belief_set=belief_set)\n\n    # Loop\n    iteration = 0\n    expand_value_function = value_function\n    old_value_function = value_function\n\n    try:\n        iterator = trange(expansions, desc='Expansions') if print_progress else range(expansions)\n        iterator_postfix = {}\n        for expansion_i in iterator:\n\n            # 1: Expand belief set\n            start_ts = datetime.now()\n\n            new_belief_set = self.expand(belief_set=belief_set,\n                                         value_function=value_function,\n                                         max_generation=max_belief_growth,\n                                         use_gpu=use_gpu,\n                                         **expand_arguments)\n\n            # Add new beliefs points to the total belief_set\n            belief_set = belief_set.union(new_belief_set)\n\n            expand_time = (datetime.now() - start_ts).total_seconds()\n            training_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)\n\n            # 2: Backup, update value function (alpha vector set)\n            for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f'Backups {expansion_i}'):\n                start_ts = datetime.now()\n\n                # Backup step\n                value_function = self.backup(belief_set if full_backup else new_belief_set,\n                                             value_function,\n                                             gamma=gamma,\n                                             append=(not full_backup),\n                                             belief_dominance_prune=False,\n                                             use_gpu=use_gpu)\n                backup_time = (datetime.now() - start_ts).total_seconds()\n\n                # Additional pruning\n                if (iteration % prune_interval) == 0 and iteration &gt; 0:\n                    start_ts = datetime.now()\n                    vf_len = len(value_function)\n\n                    value_function.prune(prune_level)\n\n                    prune_time = (datetime.now() - start_ts).total_seconds()\n                    alpha_vectors_pruned = len(value_function) - vf_len\n                    training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n                # Check if value function size is above threshold\n                if limit_value_function_size &gt;= 0 and len(value_function) &gt; limit_value_function_size:\n                    # Compute matrix multiplications between avs and beliefs\n                    alpha_value_per_belief = xp.matmul(value_function.alpha_vector_array, belief_set.belief_array.T)\n\n                    # Select the useful alpha vectors\n                    best_alpha_vector_per_belief = xp.argmax(alpha_value_per_belief, axis=0)\n                    useful_alpha_vectors = xp.unique(best_alpha_vector_per_belief)\n\n                    # Select a random selection of vectors to delete\n                    unuseful_alpha_vectors = xp.delete(xp.arange(len(value_function)), useful_alpha_vectors)\n                    random_vectors_to_delete = xp.random.choice(unuseful_alpha_vectors,\n                                                                size=max_belief_growth,\n                                                                p=(xp.arange(len(unuseful_alpha_vectors))[::-1] / xp.sum(xp.arange(len(unuseful_alpha_vectors)))))\n                                                                # replace=False,\n                                                                # p=1/len(unuseful_alpha_vectors))\n\n                    value_function = ValueFunction(model=model,\n                                                   alpha_vectors=xp.delete(value_function.alpha_vector_array, random_vectors_to_delete, axis=0),\n                                                   action_list=xp.delete(value_function.actions, random_vectors_to_delete))\n\n                    iterator_postfix['|useful|'] = useful_alpha_vectors.shape[0]\n\n                # Compute the change between value functions\n                max_change = self.compute_change(value_function, old_value_function, belief_set)\n\n                # History tracking\n                training_history.add_backup_step(backup_time, max_change, value_function)\n\n                # Convergence check\n                if max_change &lt; max_allowed_change:\n                    break\n\n                old_value_function = value_function\n\n                # Update iteration counter\n                iteration += 1\n\n            # Compute change with old expansion value function\n            expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)\n\n            if expand_max_change &lt; max_allowed_change:\n                print('Converged!')\n                break\n\n            expand_value_function = value_function\n\n            iterator_postfix['|V|'] = len(value_function)\n            iterator_postfix['|B|'] = len(belief_set)\n\n            if print_progress:\n                iterator.set_postfix(iterator_postfix)\n\n    except MemoryError as e:\n        print(f'Memory full: {e}')\n        print('Returning value function and history as is...\\n')\n\n    # Final pruning\n    start_ts = datetime.now()\n    vf_len = len(value_function)\n\n    value_function.prune(prune_level)\n\n    # History tracking\n    prune_time = (datetime.now() - start_ts).total_seconds()\n    alpha_vectors_pruned = len(value_function) - vf_len\n    training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n    # Record when it was trained\n    self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n    self.name += f'-trained_{self.trained_at}'\n\n    self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n    # Print stats if requested\n    if print_stats:\n        print(training_history.summary)\n\n    return training_history\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>ndarray</code> <p>The observation(s) the agent(s) made.</p> required <code>source_reached</code> <code>ndarray</code> <p>A boolean array of whether the agent(s) have reached the source or not.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def update_state(self,\n                 observation:np.ndarray,\n                 source_reached:np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n    Parameters\n    ----------\n    observation : np.ndarray\n        The observation(s) the agent(s) made.\n    source_reached : np.ndarray\n        A boolean array of whether the agent(s) have reached the source or not.\n    '''\n    assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Binarize observations\n    observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n    observation_ids[source_reached] = 2 # Observe source\n\n    # Update the set of beliefs\n    self.belief = self.belief.update(actions=self.action_played, observations=observation_ids)\n\n    # Remove the beliefs of the agents having reached the source\n    self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~source_reached])\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_GER_Agent","title":"<code>PBVI_GER_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing belief points that will most decrease the error in the value function (so increasing most the value).</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>class PBVI_GER_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing belief points that will most decrease the error in the value function (so increasing most the value).\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Greedy Error Reduction.\n        It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.\n        The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. Used to compute the value at belief points.\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))\n        new_belief_array[:old_shape[0]] = belief_set.belief_array\n\n        # Finding the min and max rewards for computation of the epsilon\n        r_min = model._min_reward / (1 - self.gamma)\n        r_max = model._max_reward / (1 - self.gamma)\n\n        # Generation of all potential successor beliefs\n        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n        # Finding the alphas associated with each previous beliefs\n        best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)\n        b_alphas = value_function.alpha_vector_array[best_alpha]\n\n        # Difference between beliefs and their successors\n        b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]\n\n        # Computing a 'next' alpha vector made of the max and min\n        alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)\n\n        # Difference between alpha vectors and their successors alpha vector\n        alphas_diffs = alphas_p - b_alphas[:,None,None,:]\n\n        # Computing epsilon for all successor beliefs\n        eps = xp.einsum('baos,baos-&gt;bao', alphas_diffs, b_diffs)\n\n        # Computing the probability of the b and doing action a and receiving observation o\n        bao_probs = xp.einsum('bs,saor-&gt;bao', belief_set.belief_array, model.reachable_transitional_observation_table)\n\n        # Taking the sumproduct of the probs with the epsilons\n        res = xp.einsum('bao,bao-&gt;ba', bao_probs, eps)\n\n        # Picking the correct amount of initial beliefs and ideal actions\n        b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)\n\n        # And picking the ideal observations\n        o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)\n\n        # Selecting the successor beliefs\n        new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Greedy Error Reduction Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_GER_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Greedy Error Reduction. It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error. The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. Used to compute the value at belief points.</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Greedy Error Reduction.\n    It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.\n    The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. Used to compute the value at belief points.\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))\n    new_belief_array[:old_shape[0]] = belief_set.belief_array\n\n    # Finding the min and max rewards for computation of the epsilon\n    r_min = model._min_reward / (1 - self.gamma)\n    r_max = model._max_reward / (1 - self.gamma)\n\n    # Generation of all potential successor beliefs\n    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n    # Finding the alphas associated with each previous beliefs\n    best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)\n    b_alphas = value_function.alpha_vector_array[best_alpha]\n\n    # Difference between beliefs and their successors\n    b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]\n\n    # Computing a 'next' alpha vector made of the max and min\n    alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)\n\n    # Difference between alpha vectors and their successors alpha vector\n    alphas_diffs = alphas_p - b_alphas[:,None,None,:]\n\n    # Computing epsilon for all successor beliefs\n    eps = xp.einsum('baos,baos-&gt;bao', alphas_diffs, b_diffs)\n\n    # Computing the probability of the b and doing action a and receiving observation o\n    bao_probs = xp.einsum('bs,saor-&gt;bao', belief_set.belief_array, model.reachable_transitional_observation_table)\n\n    # Taking the sumproduct of the probs with the epsilons\n    res = xp.einsum('bao,bao-&gt;ba', bao_probs, eps)\n\n    # Picking the correct amount of initial beliefs and ideal actions\n    b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)\n\n    # And picking the ideal observations\n    o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)\n\n    # Selecting the successor beliefs\n    new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_GER_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Greedy Error Reduction Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Greedy Error Reduction Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_RA_Agent","title":"<code>PBVI_RA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing random belief points.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>class PBVI_RA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing random belief points.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # How many new beliefs to add\n        generation_count = min(belief_set.belief_array.shape[0], max_generation)\n\n        # Generation of the new beliefs at random\n        new_beliefs = xp.random.random((generation_count, model.state_count))\n        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]\n\n        return BeliefSet(model, new_beliefs)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Random Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_RA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # How many new beliefs to add\n    generation_count = min(belief_set.belief_array.shape[0], max_generation)\n\n    # Generation of the new beliefs at random\n    new_beliefs = xp.random.random((generation_count, model.state_count))\n    new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]\n\n    return BeliefSet(model, new_beliefs)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_RA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Random Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Random Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSEA_Agent","title":"<code>PBVI_SSEA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing belief points furtest away (L2 distance) from any other belief point already in the belief set based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>class PBVI_SSEA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing belief points furtest away (L2 distance) from any other belief point already in the belief set based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Exploratory Action.\n        Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.\n        These lead to a new state s_p and a observation o for each action.\n        From all these and observation o we can generate updated beliefs. \n        Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        # Generation of successors\n        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n        # Compute the distances between each pair and of successor are source beliefs\n        diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)\n        dist = xp.sqrt(xp.einsum('bnaos,bnaos-&gt;bnao', diff, diff))\n\n        # Taking the min distance for each belief\n        belief_min_dists = xp.min(dist,axis=0)\n\n        # Taking the max distanced successors\n        b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])\n\n        # Selecting successor beliefs\n        new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Exploratory Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSEA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Stochastic Simulation with Exploratory Action. Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability. These lead to a new state s_p and a observation o for each action. From all these and observation o we can generate updated beliefs.  Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Exploratory Action.\n    Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.\n    These lead to a new state s_p and a observation o for each action.\n    From all these and observation o we can generate updated beliefs. \n    Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    # Generation of successors\n    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n    # Compute the distances between each pair and of successor are source beliefs\n    diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)\n    dist = xp.sqrt(xp.einsum('bnaos,bnaos-&gt;bnao', diff, diff))\n\n    # Taking the min distance for each belief\n    belief_min_dists = xp.min(dist,axis=0)\n\n    # Taking the max distanced successors\n    b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])\n\n    # Selecting successor beliefs\n    new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSEA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Exploratory Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Exploratory Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSGA_Agent","title":"<code>PBVI_SSGA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing actions in an epsilon greedy fashion and generating random observations and generating belief points based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>class PBVI_SSGA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing actions in an epsilon greedy fashion and generating random observations and generating belief points based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False,\n               epsilon:float=0.99\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Greedy Action.\n        Simulates running a single-step forward from the beliefs in the \"belief_set\".\n        The step forward is taking assuming we are in a random state s (weighted by the belief),\n        then taking the best action a based on the belief with probability 'epsilon'.\n        These lead to a new state s_p and a observation o.\n        From this action a and observation o we can update our belief. \n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n        epsilon : float, default=0.99\n            The epsilon parameter that determines whether to choose an action greedily or randomly.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n        # Random previous beliefs\n        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n            b = Belief(model, belief_vector)\n            s = b.random_state()\n\n            if random.random() &lt; epsilon:\n                a = random.choice(model.actions)\n            else:\n                best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))\n                a = value_function.actions[best_alpha_index]\n\n            s_p = model.transition(s, a)\n            o = model.observe(s_p, a)\n            b_new = b.update(a, o)\n\n            new_belief_array[i] = b_new.values\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True,\n              epsilon:float=0.99\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Greedy Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n        epsilon : float, default=0.99\n            Expand function parameter. threshold to how often to choose the action greedily to how often randomly.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats,\n                             epsilon = epsilon)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSGA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False, epsilon=0.99)</code>","text":"<p>Stochastic Simulation with Greedy Action. Simulates running a single-step forward from the beliefs in the \"belief_set\". The step forward is taking assuming we are in a random state s (weighted by the belief), then taking the best action a based on the belief with probability 'epsilon'. These lead to a new state s_p and a observation o. From this action a and observation o we can update our belief. </p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon parameter that determines whether to choose an action greedily or randomly.</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False,\n           epsilon:float=0.99\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Greedy Action.\n    Simulates running a single-step forward from the beliefs in the \"belief_set\".\n    The step forward is taking assuming we are in a random state s (weighted by the belief),\n    then taking the best action a based on the belief with probability 'epsilon'.\n    These lead to a new state s_p and a observation o.\n    From this action a and observation o we can update our belief. \n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n    epsilon : float, default=0.99\n        The epsilon parameter that determines whether to choose an action greedily or randomly.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n    # Random previous beliefs\n    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n        b = Belief(model, belief_vector)\n        s = b.random_state()\n\n        if random.random() &lt; epsilon:\n            a = random.choice(model.actions)\n        else:\n            best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))\n            a = value_function.actions[best_alpha_index]\n\n        s_p = model.transition(s, a)\n        o = model.observe(s_p, a)\n        b_new = b.update(a, o)\n\n        new_belief_array[i] = b_new.values\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSGA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True, epsilon=0.99)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Greedy Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <code>epsilon</code> <code>float</code> <p>Expand function parameter. threshold to how often to choose the action greedily to how often randomly.</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True,\n          epsilon:float=0.99\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Greedy Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n    epsilon : float, default=0.99\n        Expand function parameter. threshold to how often to choose the action greedily to how often randomly.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats,\n                         epsilon = epsilon)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSRA_Agent","title":"<code>PBVI_SSRA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing random actions and observations and generating belief points based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>class PBVI_SSRA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing random actions and observations and generating belief points based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Random Action.\n        Simulates running a single-step forward from the beliefs in the \"belief_set\".\n        The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.\n        From this action a and observation o we can update our belief.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n        # Random previous beliefs\n        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n            b = Belief(model, belief_vector)\n            s = b.random_state()\n            a = random.choice(model.actions)\n            s_p = model.transition(s, a)\n            o = model.observe(s_p, a)\n            b_new = b.update(a, o)\n\n            new_belief_array[i] = b_new.values\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Random Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSRA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Stochastic Simulation with Random Action. Simulates running a single-step forward from the beliefs in the \"belief_set\". The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o. From this action a and observation o we can update our belief.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Random Action.\n    Simulates running a single-step forward from the beliefs in the \"belief_set\".\n    The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.\n    From this action a and observation o we can update our belief.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n    # Random previous beliefs\n    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n        b = Belief(model, belief_vector)\n        s = b.random_state()\n        a = random.choice(model.actions)\n        s_p = model.transition(s, a)\n        o = model.observe(s_p, a)\n        b_new = b.update(a, o)\n\n        new_belief_array[i] = b_new.values\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.PBVI_SSRA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Random Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Random Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent","title":"<code>Perseus_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. </p>"},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent--todo-do-document-of-perseus-agent","title":"TODO: Do document of Perseus agent","text":""},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent--todo-fix-perseus-expand","title":"TODO: FIX Perseus expand","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>class Perseus_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. \n\n    # TODO: Do document of Perseus agent\n    # TODO: FIX Perseus expand\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        # TODO\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        b = belief_set.belief_list[0]\n        belief_sequence = []\n\n        for i in range(max_generation):\n            # Choose random action\n            a = int(xp.random.choice(model.actions, size=1)[0])\n\n            # Choose random observation based on prob: P(o|b,a)\n            obs_prob = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n            o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])\n\n            # Update belief\n            bao = b.update(a,o)\n\n            # Finalization\n            belief_sequence.append(bao)\n            b = bao\n\n        return BeliefSet(model, belief_sequence)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Heuristic Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":""},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent.expand--todo","title":"TODO","text":"<p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    # TODO\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    b = belief_set.belief_list[0]\n    belief_sequence = []\n\n    for i in range(max_generation):\n        # Choose random action\n        a = int(xp.random.choice(model.actions, size=1)[0])\n\n        # Choose random observation based on prob: P(o|b,a)\n        obs_prob = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n        o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])\n\n        # Update belief\n        bao = b.update(a,o)\n\n        # Finalization\n        belief_sequence.append(bao)\n        b = bao\n\n    return BeliefSet(model, belief_sequence)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.Perseus_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Heuristic Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Heuristic Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.QMDP_Agent","title":"<code>QMDP_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>An agent that relies on Model-Based Reinforcement Learning. It is a simplified version of the PBVI_Agent. It runs the a Value Iteration solver, assuming full observability. The value function that comes out from this is therefore used to make choices.</p> <p>As stated, during simulations, the agent will choose actions based on an argmax of what action has the highest matrix product of the value function with the belief vector.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\qmdp_agent.py</code> <pre><code>class QMDP_Agent(PBVI_Agent):\n    '''\n    An agent that relies on Model-Based Reinforcement Learning. It is a simplified version of the PBVI_Agent.\n    It runs the a Value Iteration solver, assuming full observability. The value function that comes out from this is therefore used to make choices.\n\n    As stated, during simulations, the agent will choose actions based on an argmax of what action has the highest matrix product of the value function with the belief vector.\n\n    ...\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def train(self,\n              expansions:int,\n              initial_value_function:ValueFunction|None=None,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Simplified version of the training. It consists in running the Value Iteration process.\n\n        Parameters\n        ----------\n        expansions : int\n            How many iterations to run the Value Iteration process for.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        # Handeling the case where the agent is already trained\n        if (self.value_function is not None) and (not force):\n            raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n        else:\n            self.trained_at = None\n            self.name = '-'.join(self.name.split('-')[:-1])\n            self.value_function = None\n\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Value Iteration solving\n        value_function, hist = vi_solver.solve(model = model,\n                                               horizon = expansions,\n                                               initial_value_function = initial_value_function,\n                                               gamma = gamma,\n                                               eps = eps,\n                                               use_gpu = use_gpu,\n                                               history_tracking_level = history_tracking_level,\n                                               print_progress = print_progress)\n\n        # Record when it was trained\n        self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n        self.name += f'-trained_{self.trained_at}'\n\n        self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n        # Print stats if requested\n        if print_stats:\n            print(hist.summary)\n\n        return hist\n</code></pre>"},{"location":"reference/agents/#olfactory_navigation.agents.QMDP_Agent.train","title":"<code>train(expansions, initial_value_function=None, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Simplified version of the training. It consists in running the Value Iteration process.</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many iterations to run the Value Iteration process for.</p> required <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\qmdp_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          initial_value_function:ValueFunction|None=None,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Simplified version of the training. It consists in running the Value Iteration process.\n\n    Parameters\n    ----------\n    expansions : int\n        How many iterations to run the Value Iteration process for.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    # Handeling the case where the agent is already trained\n    if (self.value_function is not None) and (not force):\n        raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n    else:\n        self.trained_at = None\n        self.name = '-'.join(self.name.split('-')[:-1])\n        self.value_function = None\n\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Value Iteration solving\n    value_function, hist = vi_solver.solve(model = model,\n                                           horizon = expansions,\n                                           initial_value_function = initial_value_function,\n                                           gamma = gamma,\n                                           eps = eps,\n                                           use_gpu = use_gpu,\n                                           history_tracking_level = history_tracking_level,\n                                           print_progress = print_progress)\n\n    # Record when it was trained\n    self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n    self.name += f'-trained_{self.trained_at}'\n\n    self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n    # Print stats if requested\n    if print_stats:\n        print(hist.summary)\n\n    return hist\n</code></pre>"},{"location":"reference/agents/fsvi_agent/","title":"fsvi_agent","text":""},{"location":"reference/agents/fsvi_agent/#olfactory_navigation.agents.fsvi_agent.FSVI_Agent","title":"<code>FSVI_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A particular flavor of the Point-Based Value Iteration based agent. The general concept relies on Model-Based reinforcement learning as described in: Pineau, J., Gordon, G., &amp; Thrun, S. (2003, August). Point-based value iteration: An anytime algorithm for POMDPs The Forward Search Value Iteration algorithm is described in: Shani, G., Brafman, R. I., &amp; Shimony, S. E. (2007, January). Forward Search Value Iteration for POMDPs</p> <p>The training consist in two steps:</p> <ul> <li> <p>Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).</p> </li> <li> <p>Backup: Using the generated belief points, the value function is updated.</p> </li> </ul> <p>The belief points are probability distributions over the state space and are therefore vectors of |S| elements.</p> <p>Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|. Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action. To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.</p> <p>Forward Search exploration concept: It relies of the solution of the Fully-Observable (MDP) problem to guide the exploration of belief points. It makes an agent start randomly in the environment and makes him take steps following the MDP solution while generating belief points along the way. Each time the expand function is called it starts generated a new set of belief points and the update function uses only the latest generated belief points to make update the value function.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>class FSVI_Agent(PBVI_Agent):\n    '''\n    A particular flavor of the Point-Based Value Iteration based agent.\n    The general concept relies on Model-Based reinforcement learning as described in: Pineau, J., Gordon, G., &amp; Thrun, S. (2003, August). Point-based value iteration: An anytime algorithm for POMDPs\n    The Forward Search Value Iteration algorithm is described in: Shani, G., Brafman, R. I., &amp; Shimony, S. E. (2007, January). Forward Search Value Iteration for POMDPs\n\n    The training consist in two steps:\n\n    - Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).\n\n    - Backup: Using the generated belief points, the value function is updated.\n\n    The belief points are probability distributions over the state space and are therefore vectors of |S| elements.\n\n    Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|.\n    Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action.\n    To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.\n\n    Forward Search exploration concept:\n    It relies of the solution of the Fully-Observable (MDP) problem to guide the exploration of belief points.\n    It makes an agent start randomly in the environment and makes him take steps following the MDP solution while generating belief points along the way.\n    Each time the expand function is called it starts generated a new set of belief points and the update function uses only the latest generated belief points to make update the value function.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               mdp_policy:ValueFunction,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.\n        It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take.\n        Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.\n        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.\n        Once the state is a goal state, the loop is done and the belief sequence is returned.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            A belief set containing a single belief to start the sequence with.\n            A random state will be chosen based on the probability distribution of the belief.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int\n            How many beliefs to be generated at most.\n        mdp_policy : ValueFunction\n            The mdp policy used to choose the action from with the given state 's'.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Getting initial belief\n        b0 = belief_set.belief_list[0]\n        belief_list = [b0]\n\n        # Choose a random starting state\n        s = b0.random_state()\n\n        # Setting the working belief\n        b = b0\n\n        for _ in range(max_generation - 1): #-1 due to a one belief already being present in the set\n            # Choose action based on mdp value function\n            a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])\n\n            # Pick a random next state (weighted by transition probabilities)\n            s_p = model.transition(s, a_star)\n\n            # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star\n            o = model.observe(s_p, a_star)\n\n            # Generate a new belief based on a_star and o\n            b_p = b.update(a_star, o)\n\n            # Record new belief\n            belief_list.append(b_p)\n\n            # Updating s and b\n            s = s_p\n            b = b_p\n\n            # Reset and belief if end state is reached\n            if s in model.end_states:\n                s = b0.random_state()\n                b = b0\n\n        return BeliefSet(model, belief_list)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              mdp_policy:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Foward Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        mdp_policy : ValueFunction, optional\n            The MDP solution to guide the expand process.\n            If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        if mdp_policy is None:\n            log('MDP_policy, not provided. Solving MDP with Value Iteration...')\n            mdp_policy, hist = vi_solver.solve(model = self.model,\n                                               horizon = 1000,\n                                               initial_value_function = initial_value_function,\n                                               gamma = gamma,\n                                               eps = eps,\n                                               use_gpu = use_gpu,\n                                               history_tracking_level = 1,\n                                               print_progress = print_progress)\n\n            if print_stats:\n                print(hist.summary)\n\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats,\n                             mdp_policy = mdp_policy)\n</code></pre>"},{"location":"reference/agents/fsvi_agent/#olfactory_navigation.agents.fsvi_agent.FSVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, mdp_policy, use_gpu=False)</code>","text":"<p>Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles. It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take. Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities. Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence. Once the state is a goal state, the loop is done and the belief sequence is returned.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>A belief set containing a single belief to start the sequence with. A random state will be chosen based on the probability distribution of the belief.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>How many beliefs to be generated at most.</p> required <code>mdp_policy</code> <code>ValueFunction</code> <p>The mdp policy used to choose the action from with the given state 's'.</p> required <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           mdp_policy:ValueFunction,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.\n    It is a loop is started by a initial state 's' and using the MDP policy, chooses the best action to take.\n    Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.\n    Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.\n    Once the state is a goal state, the loop is done and the belief sequence is returned.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        A belief set containing a single belief to start the sequence with.\n        A random state will be chosen based on the probability distribution of the belief.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int\n        How many beliefs to be generated at most.\n    mdp_policy : ValueFunction\n        The mdp policy used to choose the action from with the given state 's'.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Getting initial belief\n    b0 = belief_set.belief_list[0]\n    belief_list = [b0]\n\n    # Choose a random starting state\n    s = b0.random_state()\n\n    # Setting the working belief\n    b = b0\n\n    for _ in range(max_generation - 1): #-1 due to a one belief already being present in the set\n        # Choose action based on mdp value function\n        a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])\n\n        # Pick a random next state (weighted by transition probabilities)\n        s_p = model.transition(s, a_star)\n\n        # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star\n        o = model.observe(s_p, a_star)\n\n        # Generate a new belief based on a_star and o\n        b_p = b.update(a_star, o)\n\n        # Record new belief\n        belief_list.append(b_p)\n\n        # Updating s and b\n        s = s_p\n        b = b_p\n\n        # Reset and belief if end state is reached\n        if s in model.end_states:\n            s = b0.random_state()\n            b = b0\n\n    return BeliefSet(model, belief_list)\n</code></pre>"},{"location":"reference/agents/fsvi_agent/#olfactory_navigation.agents.fsvi_agent.FSVI_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, mdp_policy=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Foward Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>mdp_policy</code> <code>ValueFunction</code> <p>The MDP solution to guide the expand process. If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\fsvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          mdp_policy:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Foward Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    mdp_policy : ValueFunction, optional\n        The MDP solution to guide the expand process.\n        If it is not provided, the Value Iteration for the MDP version of the problem will be run. (using the same gamma and eps as set here; horizon=1000)\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    if mdp_policy is None:\n        log('MDP_policy, not provided. Solving MDP with Value Iteration...')\n        mdp_policy, hist = vi_solver.solve(model = self.model,\n                                           horizon = 1000,\n                                           initial_value_function = initial_value_function,\n                                           gamma = gamma,\n                                           eps = eps,\n                                           use_gpu = use_gpu,\n                                           history_tracking_level = 1,\n                                           print_progress = print_progress)\n\n        if print_stats:\n            print(hist.summary)\n\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats,\n                         mdp_policy = mdp_policy)\n</code></pre>"},{"location":"reference/agents/hsvi_agent/","title":"hsvi_agent","text":""},{"location":"reference/agents/hsvi_agent/#olfactory_navigation.agents.hsvi_agent.HSVI_Agent","title":"<code>HSVI_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. </p>"},{"location":"reference/agents/hsvi_agent/#olfactory_navigation.agents.hsvi_agent.HSVI_Agent--todo-do-document-of-hsvi-agent","title":"TODO: Do document of HSVI agent","text":""},{"location":"reference/agents/hsvi_agent/#olfactory_navigation.agents.hsvi_agent.HSVI_Agent--todo-fix-hsvi-expand","title":"TODO: FIX HSVI expand","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>class HSVI_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. \n\n    # TODO: Do document of HSVI agent\n    # TODO: FIX HSVI expand\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        The expand function of the  Heuristic Search Value Iteration (HSVI) technique.\n        It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.\n\n        It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. Used to compute the value at belief points.\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        if conv_term is None:\n            conv_term = self.eps\n\n        # Update convergence term\n        conv_term /= self.gamma\n\n        # Find best a based on upper bound v\n        max_qv = -xp.inf\n        best_a = -1\n        for a in model.actions:\n            b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n\n            b_prob_val = 0\n            for o in model.observations:\n                b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))\n\n            qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))\n\n            # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)\n            if qva &gt; max_qv:\n                max_qv = qva\n                best_a = a\n\n        # Choose o that max gap between bounds\n        b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,best_a,:,:], b.values)\n\n        max_o_val = -xp.inf\n        best_v_diff = -xp.inf\n        next_b = b\n\n        for o in model.observations:\n            bao = b.update(best_a, o)\n\n            upper_v_bao = upper_bound_belief_value_map.evaluate(bao)\n            lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))\n\n            v_diff = (upper_v_bao - lower_v_bao)\n\n            o_val = b_probs[o] * v_diff\n\n            if o_val &gt; max_o_val:\n                max_o_val = o_val\n                best_v_diff = v_diff\n                next_b = bao\n\n        # if bounds_split &lt; conv_term or max_generation &lt;= 0:\n        if best_v_diff &lt; conv_term or max_generation &lt;= 1:\n            return BeliefSet(model, [next_b])\n\n        # Add the belief point and associated value to the belief-value mapping\n        upper_bound_belief_value_map.add(b, max_qv)\n\n        # Go one step deeper in the recursion\n        b_set = self.expand_hsvi(model=model,\n                                 b=next_b,\n                                 value_function=value_function,\n                                 upper_bound_belief_value_map=upper_bound_belief_value_map,\n                                 conv_term=conv_term,\n                                 max_generation=max_generation-1)\n\n        # Append the nex belief of this iteration to the deeper beliefs\n        new_belief_list = b_set.belief_list\n        new_belief_list.append(next_b)\n\n        return BeliefSet(model, new_belief_list)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Heuristic Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/hsvi_agent/#olfactory_navigation.agents.hsvi_agent.HSVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>The expand function of the  Heuristic Search Value Iteration (HSVI) technique. It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.</p> <p>It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. Used to compute the value at belief points.</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    The expand function of the  Heuristic Search Value Iteration (HSVI) technique.\n    It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.\n\n    It is developped by Smith T. and Simmons R. and described in the paper \"Heuristic Search Value Iteration for POMDPs\".\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. Used to compute the value at belief points.\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    if conv_term is None:\n        conv_term = self.eps\n\n    # Update convergence term\n    conv_term /= self.gamma\n\n    # Find best a based on upper bound v\n    max_qv = -xp.inf\n    best_a = -1\n    for a in model.actions:\n        b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n\n        b_prob_val = 0\n        for o in model.observations:\n            b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))\n\n        qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))\n\n        # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)\n        if qva &gt; max_qv:\n            max_qv = qva\n            best_a = a\n\n    # Choose o that max gap between bounds\n    b_probs = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,best_a,:,:], b.values)\n\n    max_o_val = -xp.inf\n    best_v_diff = -xp.inf\n    next_b = b\n\n    for o in model.observations:\n        bao = b.update(best_a, o)\n\n        upper_v_bao = upper_bound_belief_value_map.evaluate(bao)\n        lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))\n\n        v_diff = (upper_v_bao - lower_v_bao)\n\n        o_val = b_probs[o] * v_diff\n\n        if o_val &gt; max_o_val:\n            max_o_val = o_val\n            best_v_diff = v_diff\n            next_b = bao\n\n    # if bounds_split &lt; conv_term or max_generation &lt;= 0:\n    if best_v_diff &lt; conv_term or max_generation &lt;= 1:\n        return BeliefSet(model, [next_b])\n\n    # Add the belief point and associated value to the belief-value mapping\n    upper_bound_belief_value_map.add(b, max_qv)\n\n    # Go one step deeper in the recursion\n    b_set = self.expand_hsvi(model=model,\n                             b=next_b,\n                             value_function=value_function,\n                             upper_bound_belief_value_map=upper_bound_belief_value_map,\n                             conv_term=conv_term,\n                             max_generation=max_generation-1)\n\n    # Append the nex belief of this iteration to the deeper beliefs\n    new_belief_list = b_set.belief_list\n    new_belief_list.append(next_b)\n\n    return BeliefSet(model, new_belief_list)\n</code></pre>"},{"location":"reference/agents/hsvi_agent/#olfactory_navigation.agents.hsvi_agent.HSVI_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Heuristic Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\hsvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Heuristic Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/","title":"infotaxis_agent","text":""},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent","title":"<code>Infotaxis_Agent</code>","text":"<p>             Bases: <code>Agent</code></p> <p>An agent following the Infotaxis principle. It is a Model-Based approach that aims to make steps towards where the agent has the greatest likelihood to minimize the entropy of the belief. The belief is (as for the PBVI agent) a probability distribution over the state space of how much the agent is to be confident in each state. The technique was developped and described in the following article: Vergassola, M., Villermaux, E., &amp; Shraiman, B. I. (2007). 'Infotaxis' as a strategy for searching without gradients.</p> <p>It does not need to be trained to the train(), save() and load() function are not implemented.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>class Infotaxis_Agent(Agent):\n    '''\n    An agent following the Infotaxis principle.\n    It is a Model-Based approach that aims to make steps towards where the agent has the greatest likelihood to minimize the entropy of the belief.\n    The belief is (as for the PBVI agent) a probability distribution over the state space of how much the agent is to be confident in each state.\n    The technique was developped and described in the following article: Vergassola, M., Villermaux, E., &amp; Shraiman, B. I. (2007). 'Infotaxis' as a strategy for searching without gradients.\n\n    It does not need to be trained to the train(), save() and load() function are not implemented.\n\n    ...\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        super().__init__(\n            environment = environment,\n            threshold = threshold,\n            name = name\n        )\n\n        self.model = Model.from_environment(environment, threshold)\n\n        # Status variables\n        self.beliefs = None\n        self.action_played = None\n\n\n    def to_gpu(self) -&gt; Agent:\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        gpu_agent\n        '''\n        # Generating a new instance\n        cls = self.__class__\n        gpu_agent = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_agent, arg, cp.array(val))\n            elif isinstance(val, Model):\n                gpu_agent.model = self.model.gpu_model\n            elif isinstance(val, BeliefSet):\n                gpu_agent.beliefs = self.beliefs.to_gpu()\n            else:\n                setattr(gpu_agent, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_agent\n        gpu_agent._alternate_version = self\n\n        gpu_agent.on_gpu = True\n        return gpu_agent\n\n\n    def initialize_state(self,\n                         n:int=1\n                         ) -&gt; None:\n        '''\n        To use an agent within a simulation, the agent's state needs to be initialized.\n        The initialization consists of setting the agent's initial belief.\n        Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents are to be used during the simulation.\n        '''\n        self.beliefs = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to let the agent or set of agents choose an action based on their current belief.\n        Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A single or a list of actions chosen by the agent(s) based on their belief.\n        '''\n        xp = np if not self.on_gpu else cp\n\n        n = len(self.beliefs)\n\n        best_entropy = xp.ones(n) * -1\n        best_action = xp.ones(n, dtype=int) * -1\n\n        current_entropy = self.beliefs.entropies\n\n        for a in self.model.actions:\n            total_entropy = xp.zeros(n)\n\n            for o in self.model.observations:\n                b_ao = self.beliefs.update(actions=np.ones(n, dtype=int)*a,\n                                           observations=np.ones(n, dtype=int)*o,\n                                           throw_error=False)\n\n                # Computing entropy\n                with warnings.catch_warnings():\n                    warnings.simplefilter('ignore')\n                    b_ao_entropy = b_ao.entropies\n\n                b_prob = xp.dot(self.beliefs.belief_array, xp.sum(self.model.reachable_transitional_observation_table[:,a,o,:], axis=1))\n\n                total_entropy += (b_prob * (current_entropy - b_ao_entropy))\n\n            # Checking if action is superior to previous best\n            superiority_mask = best_entropy &lt; total_entropy\n            best_action[superiority_mask] = a\n            best_entropy[superiority_mask] = total_entropy[superiority_mask]\n\n        # Recording the action played\n        self.action_played = best_action\n\n        # Converting action indexes to movement vectors\n        movemement_vector = self.model.movement_vector[best_action,:]\n\n        return movemement_vector\n\n\n    def update_state(self,\n                     observation:int|np.ndarray,\n                     source_reached:bool|np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n        Parameters\n        ----------\n        observation : np.ndarray\n            The observation(s) the agent(s) made.\n        source_reached : np.ndarray\n            A boolean array of whether the agent(s) have reached the source or not.\n        '''\n        assert self.beliefs is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Binarize observations\n        observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n        observation_ids[source_reached] = 2 # Observe source\n\n        # Update the set of beliefs\n        self.beliefs = self.beliefs.update(actions=self.action_played, observations=observation_ids)\n\n        # Remove the beliefs of the agents having reached the source\n        self.beliefs = BeliefSet(self.model, self.beliefs.belief_array[~source_reached])\n\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that have not reached the source but can't continue further\n\n        Parameters\n        ----------\n        simulations_to_kill : np.ndarray\n            A boolean array of the simulations to kill.\n        '''\n        if all(simulations_to_kill):\n            self.beliefs = None\n        else:\n            self.beliefs = BeliefSet(self.beliefs.model, self.beliefs.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to let the agent or set of agents choose an action based on their current belief. Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A single or a list of actions chosen by the agent(s) based on their belief.</p> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to let the agent or set of agents choose an action based on their current belief.\n    Following the Infotaxis principle, it will choose an action that will minimize the sum of next entropies.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A single or a list of actions chosen by the agent(s) based on their belief.\n    '''\n    xp = np if not self.on_gpu else cp\n\n    n = len(self.beliefs)\n\n    best_entropy = xp.ones(n) * -1\n    best_action = xp.ones(n, dtype=int) * -1\n\n    current_entropy = self.beliefs.entropies\n\n    for a in self.model.actions:\n        total_entropy = xp.zeros(n)\n\n        for o in self.model.observations:\n            b_ao = self.beliefs.update(actions=np.ones(n, dtype=int)*a,\n                                       observations=np.ones(n, dtype=int)*o,\n                                       throw_error=False)\n\n            # Computing entropy\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                b_ao_entropy = b_ao.entropies\n\n            b_prob = xp.dot(self.beliefs.belief_array, xp.sum(self.model.reachable_transitional_observation_table[:,a,o,:], axis=1))\n\n            total_entropy += (b_prob * (current_entropy - b_ao_entropy))\n\n        # Checking if action is superior to previous best\n        superiority_mask = best_entropy &lt; total_entropy\n        best_action[superiority_mask] = a\n        best_entropy[superiority_mask] = total_entropy[superiority_mask]\n\n    # Recording the action played\n    self.action_played = best_action\n\n    # Converting action indexes to movement vectors\n    movemement_vector = self.model.movement_vector[best_action,:]\n\n    return movemement_vector\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>To use an agent within a simulation, the agent's state needs to be initialized. The initialization consists of setting the agent's initial belief. Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents are to be used during the simulation.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def initialize_state(self,\n                     n:int=1\n                     ) -&gt; None:\n    '''\n    To use an agent within a simulation, the agent's state needs to be initialized.\n    The initialization consists of setting the agent's initial belief.\n    Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents are to be used during the simulation.\n    '''\n    self.beliefs = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that have not reached the source but can't continue further</p> <p>Parameters:</p> Name Type Description Default <code>simulations_to_kill</code> <code>ndarray</code> <p>A boolean array of the simulations to kill.</p> required Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that have not reached the source but can't continue further\n\n    Parameters\n    ----------\n    simulations_to_kill : np.ndarray\n        A boolean array of the simulations to kill.\n    '''\n    if all(simulations_to_kill):\n        self.beliefs = None\n    else:\n        self.beliefs = BeliefSet(self.beliefs.model, self.beliefs.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Type Description <code>gpu_agent</code> Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def to_gpu(self) -&gt; Agent:\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    gpu_agent\n    '''\n    # Generating a new instance\n    cls = self.__class__\n    gpu_agent = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_agent, arg, cp.array(val))\n        elif isinstance(val, Model):\n            gpu_agent.model = self.model.gpu_model\n        elif isinstance(val, BeliefSet):\n            gpu_agent.beliefs = self.beliefs.to_gpu()\n        else:\n            setattr(gpu_agent, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_agent\n    gpu_agent._alternate_version = self\n\n    gpu_agent.on_gpu = True\n    return gpu_agent\n</code></pre>"},{"location":"reference/agents/infotaxis_agent/#olfactory_navigation.agents.infotaxis_agent.Infotaxis_Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>ndarray</code> <p>The observation(s) the agent(s) made.</p> required <code>source_reached</code> <code>ndarray</code> <p>A boolean array of whether the agent(s) have reached the source or not.</p> required Source code in <code>olfactory_navigation\\agents\\infotaxis_agent.py</code> <pre><code>def update_state(self,\n                 observation:int|np.ndarray,\n                 source_reached:bool|np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n    Parameters\n    ----------\n    observation : np.ndarray\n        The observation(s) the agent(s) made.\n    source_reached : np.ndarray\n        A boolean array of whether the agent(s) have reached the source or not.\n    '''\n    assert self.beliefs is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Binarize observations\n    observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n    observation_ids[source_reached] = 2 # Observe source\n\n    # Update the set of beliefs\n    self.beliefs = self.beliefs.update(actions=self.action_played, observations=observation_ids)\n\n    # Remove the beliefs of the agents having reached the source\n    self.beliefs = BeliefSet(self.model, self.beliefs.belief_array[~source_reached])\n</code></pre>"},{"location":"reference/agents/pbvi_agent/","title":"pbvi_agent","text":""},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent","title":"<code>PBVI_Agent</code>","text":"<p>             Bases: <code>Agent</code></p> <p>A generic Point-Based Value Iteration based agent. It relies on Model-Based reinforcement learning as described in: Pineau J. et al, Point-based value iteration: An anytime algorithm for POMDPs The training consist in two steps:</p> <ul> <li> <p>Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).</p> </li> <li> <p>Backup: Using the generated belief points, the value function is updated.</p> </li> </ul> <p>The belief points are probability distributions over the state space and are therefore vectors of |S| elements.</p> <p>Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|. Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action. To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>class PBVI_Agent(Agent):\n    '''\n    A generic Point-Based Value Iteration based agent. It relies on Model-Based reinforcement learning as described in: Pineau J. et al, Point-based value iteration: An anytime algorithm for POMDPs\n    The training consist in two steps:\n\n    - Expand: Where belief points are explored based on the some strategy (to be defined by subclasses).\n\n    - Backup: Using the generated belief points, the value function is updated.\n\n    The belief points are probability distributions over the state space and are therefore vectors of |S| elements.\n\n    Actions are chosen based on a value function. A value function is a set of alpha vectors of dimentionality |S|.\n    Each alpha vector is associated to a single action but multiple alpha vectors can be associated to the same action.\n    To choose an action at a given belief point, a dot product is taken between each alpha vector and the belief point and the action associated with the highest result is chosen.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def __init__(self,\n                 environment:Environment,\n                 threshold:float|None=3e-6,\n                 name:str|None=None\n                 ) -&gt; None:\n        super().__init__(\n            environment=environment,\n            threshold=threshold,\n            name=name\n        )\n\n        self.model = Model.from_environment(environment, threshold)\n\n        # Trainable variables\n        self.trained_at = None\n        self.value_function = None\n\n        # Status variables\n        self.belief = None\n        self.action_played = None\n\n\n    def to_gpu(self) -&gt; Agent:\n        '''\n        Function to send the numpy arrays of the agent to the gpu.\n        It returns a new instance of the Agent class with the arrays on the gpu\n\n        Returns\n        -------\n        gpu_agent : Agent\n            A copy of the agent with the arrays on the GPU.\n        '''\n        # Generating a new instance\n        cls = self.__class__\n        gpu_agent = cls.__new__(cls)\n\n        # Copying arguments to gpu\n        for arg, val in self.__dict__.items():\n            if isinstance(val, np.ndarray):\n                setattr(gpu_agent, arg, cp.array(val))\n            elif isinstance(val, Model):\n                gpu_agent.model = self.model.gpu_model\n            elif isinstance(val, ValueFunction):\n                gpu_agent.value_function =self.value_function.to_gpu()\n            elif isinstance(val, BeliefSet):\n                gpu_agent.belief = self.belief.to_gpu()\n            else:\n                setattr(gpu_agent, arg, val)\n\n        # Self reference instances\n        self._alternate_version = gpu_agent\n        gpu_agent._alternate_version = self\n\n        gpu_agent.on_gpu = True\n        return gpu_agent\n\n\n    def save(self,\n             folder:str|None=None,\n             force:bool=False,\n             save_environment:bool=False\n             ) -&gt; None:\n        '''\n        The save function for PBVI Agents consists in recording the value function after the training.\n        It saves the agent in a folder with the name of the agent (class name + training timestamp).\n        In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.\n\n        Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse.\n        If the agent has already been saved, the saving will not happen unless the force parameter is toggled.\n\n        Parameters\n        ----------\n        folder : str, optional\n            The folder under which to save the agent (a subfolder will be created under this folder).\n            The agent will therefore be saved at &lt;folder&gt;/Agent-&lt;agent_name&gt; .\n            By default the current folder is used.\n        force : bool, default=False\n            Whether to overwrite an already saved agent with the same name at the same path.\n        save_environment : bool, default=False\n            Whether to save the environment data along with the agent.\n        '''\n        assert self.trained_at is not None, \"The agent is not trained, there is nothing to save.\"\n\n        # Adding env name to folder path\n        if folder is None:\n            folder = f'./Agent-{self.name}'\n        else:\n            folder += '/Agent-' + self.name\n\n        # Checking the folder exists or creates it\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n        elif len(os.listdir(folder)):\n            if force:\n                shutil.rmtree(folder)\n                os.mkdir(folder)\n            else:\n                raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n        # If requested save environment\n        if save_environment:\n            self.environment.save(folder=folder)\n\n        # Generating the metadata arguments dictionary\n        arguments = {}\n        arguments['name'] = self.name\n        arguments['class'] = self.class_name\n        arguments['threshold'] = self.threshold\n        arguments['environment_name'] = self.environment.name\n        arguments['environment_saved_at'] = self.environment.saved_at\n        arguments['trained_at'] = self.trained_at\n\n        # Output the arguments to a METADATA file\n        with open(folder + '/METADATA.json', 'w') as json_file:\n            json.dump(arguments, json_file, indent=4)\n\n        # Save value function\n        self.value_function.save(folder=folder, file_name='Value_Function.npy')\n\n        # Finalization\n        self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n        print(f'Agent saved to: {folder}')\n\n\n    @classmethod\n    def load(cls,\n             folder:str\n             ) -&gt; 'PBVI_Agent':\n        '''\n        Function to load a PBVI agent from a given folder it has been saved to.\n        It will load the environment the agent has been trained on along with it.\n\n        If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.\n\n        Parameters\n        ----------\n        folder : str\n            The agent folder.\n\n        Returns\n        -------\n        instance : PBVI_Agent\n            The loaded instance of the PBVI Agent.\n        '''\n        # Load arguments\n        arguments = None\n        with open(folder + '/METADATA.json', 'r') as json_file:\n            arguments = json.load(json_file)\n\n        # Load environment\n        environment = Environment.load(arguments['environment_saved_at'])\n\n        # Load specific class\n        if arguments['class'] != 'PBVI_Agent':\n            from olfactory_navigation import agents\n            cls = {name:obj for name, obj in inspect.getmembers(agents)}[arguments['class']]\n\n        # Build instance\n        instance = cls(\n            environment=environment,\n            threshold=arguments['threshold'],\n            name=arguments['name']\n        )\n\n        # Load and set the value function on the instance\n        instance.value_function = ValueFunction.load(\n            file=folder + '/Value_Function.npy',\n            model=instance.model\n        )\n        instance.trained_at = arguments['trained_at']\n        instance.saved_at = folder\n\n        return instance\n\n\n    def train(self,\n              expansions:int,\n              full_backup:bool=True,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:Union[BeliefSet, Belief, None]=None,\n              initial_value_function:Union[ValueFunction,None]=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False, # TODO rehandle the way things are run on GPU\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True,\n              **expand_arguments\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        full_backup : bool, default=True\n            Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n        expand_arguments : kwargs\n            An arbitrary amount of parameters that will be passed on to the expand function.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Initial belief\n        if initial_belief is None:\n            belief_set = BeliefSet(model, [Belief(model)])\n        elif isinstance(initial_belief, BeliefSet):\n            belief_set = initial_belief.to_gpu() if use_gpu else initial_belief \n        else:\n            initial_belief = Belief(model, xp.array(initial_belief.values))\n            belief_set = BeliefSet(model, [initial_belief])\n\n        # Handeling the case where the agent is already trained\n        if (self.value_function is not None):\n            if not force:\n                raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n            else:\n                self.trained_at = None\n                self.name = '-'.join(self.name.split('-')[:-1])\n                self.value_function = None\n\n        # Initial value function\n        if initial_value_function is None:\n            value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)\n        else:\n            value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function\n\n        # Convergence check boundary\n        max_allowed_change = eps * (gamma / (1-gamma))\n\n        # History tracking\n        training_history = TrainingHistory(tracking_level=history_tracking_level,\n                                           model=model,\n                                           gamma=gamma,\n                                           eps=eps,\n                                           expand_append=full_backup,\n                                           initial_value_function=value_function,\n                                           initial_belief_set=belief_set)\n\n        # Loop\n        iteration = 0\n        expand_value_function = value_function\n        old_value_function = value_function\n\n        try:\n            iterator = trange(expansions, desc='Expansions') if print_progress else range(expansions)\n            iterator_postfix = {}\n            for expansion_i in iterator:\n\n                # 1: Expand belief set\n                start_ts = datetime.now()\n\n                new_belief_set = self.expand(belief_set=belief_set,\n                                             value_function=value_function,\n                                             max_generation=max_belief_growth,\n                                             use_gpu=use_gpu,\n                                             **expand_arguments)\n\n                # Add new beliefs points to the total belief_set\n                belief_set = belief_set.union(new_belief_set)\n\n                expand_time = (datetime.now() - start_ts).total_seconds()\n                training_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)\n\n                # 2: Backup, update value function (alpha vector set)\n                for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f'Backups {expansion_i}'):\n                    start_ts = datetime.now()\n\n                    # Backup step\n                    value_function = self.backup(belief_set if full_backup else new_belief_set,\n                                                 value_function,\n                                                 gamma=gamma,\n                                                 append=(not full_backup),\n                                                 belief_dominance_prune=False,\n                                                 use_gpu=use_gpu)\n                    backup_time = (datetime.now() - start_ts).total_seconds()\n\n                    # Additional pruning\n                    if (iteration % prune_interval) == 0 and iteration &gt; 0:\n                        start_ts = datetime.now()\n                        vf_len = len(value_function)\n\n                        value_function.prune(prune_level)\n\n                        prune_time = (datetime.now() - start_ts).total_seconds()\n                        alpha_vectors_pruned = len(value_function) - vf_len\n                        training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n                    # Check if value function size is above threshold\n                    if limit_value_function_size &gt;= 0 and len(value_function) &gt; limit_value_function_size:\n                        # Compute matrix multiplications between avs and beliefs\n                        alpha_value_per_belief = xp.matmul(value_function.alpha_vector_array, belief_set.belief_array.T)\n\n                        # Select the useful alpha vectors\n                        best_alpha_vector_per_belief = xp.argmax(alpha_value_per_belief, axis=0)\n                        useful_alpha_vectors = xp.unique(best_alpha_vector_per_belief)\n\n                        # Select a random selection of vectors to delete\n                        unuseful_alpha_vectors = xp.delete(xp.arange(len(value_function)), useful_alpha_vectors)\n                        random_vectors_to_delete = xp.random.choice(unuseful_alpha_vectors,\n                                                                    size=max_belief_growth,\n                                                                    p=(xp.arange(len(unuseful_alpha_vectors))[::-1] / xp.sum(xp.arange(len(unuseful_alpha_vectors)))))\n                                                                    # replace=False,\n                                                                    # p=1/len(unuseful_alpha_vectors))\n\n                        value_function = ValueFunction(model=model,\n                                                       alpha_vectors=xp.delete(value_function.alpha_vector_array, random_vectors_to_delete, axis=0),\n                                                       action_list=xp.delete(value_function.actions, random_vectors_to_delete))\n\n                        iterator_postfix['|useful|'] = useful_alpha_vectors.shape[0]\n\n                    # Compute the change between value functions\n                    max_change = self.compute_change(value_function, old_value_function, belief_set)\n\n                    # History tracking\n                    training_history.add_backup_step(backup_time, max_change, value_function)\n\n                    # Convergence check\n                    if max_change &lt; max_allowed_change:\n                        break\n\n                    old_value_function = value_function\n\n                    # Update iteration counter\n                    iteration += 1\n\n                # Compute change with old expansion value function\n                expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)\n\n                if expand_max_change &lt; max_allowed_change:\n                    print('Converged!')\n                    break\n\n                expand_value_function = value_function\n\n                iterator_postfix['|V|'] = len(value_function)\n                iterator_postfix['|B|'] = len(belief_set)\n\n                if print_progress:\n                    iterator.set_postfix(iterator_postfix)\n\n        except MemoryError as e:\n            print(f'Memory full: {e}')\n            print('Returning value function and history as is...\\n')\n\n        # Final pruning\n        start_ts = datetime.now()\n        vf_len = len(value_function)\n\n        value_function.prune(prune_level)\n\n        # History tracking\n        prune_time = (datetime.now() - start_ts).total_seconds()\n        alpha_vectors_pruned = len(value_function) - vf_len\n        training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n        # Record when it was trained\n        self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n        self.name += f'-trained_{self.trained_at}'\n\n        self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n        # Print stats if requested\n        if print_stats:\n            print(training_history.summary)\n\n        return training_history\n\n\n    def compute_change(self,\n                       value_function:ValueFunction,\n                       new_value_function:ValueFunction,\n                       belief_set:BeliefSet\n                       ) -&gt; float:\n        '''\n        Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.\n        It check for each belief, the maximum value and take the max change between believe's value functions.\n        If this max change is lower than eps * (gamma / (1 - gamma)).\n\n        Parameters\n        ----------\n        value_function : ValueFunction\n            The first value function to compare.\n        new_value_function : ValueFunction\n            The second value function to compare.\n        belief_set : BeliefSet\n            The set of believes to check the values on to compute the max change on.\n\n        Returns\n        -------\n        max_change : float\n            The maximum change between value functions at belief points.\n        '''\n        # Get numpy corresponding to the arrays\n        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)\n\n        # Computing Delta for each beliefs\n        max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)\n        new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)\n        max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))\n\n        return max_change\n\n\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False, # TODO Remove this\n               **kwargs\n               ) -&gt; BeliefSet:\n        '''\n        Abstract function!\n        This function should be implemented in subclasses.\n        The expand function consists in the exploration of the belief set.\n        It takes as input a belief set and generates at most 'max_generation' beliefs from it.\n\n        The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            The belief or set of beliefs to be used as a starting point for the exploration.\n        value_function : ValueFunction\n            The current value function. To be used to guide the exploration process.\n        max_generation : int\n            How many beliefs to be generated at most.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n        kwargs\n            Special parameters for the particular flavors of the PBVI Agent.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            A new (or expanded) set of beliefs.\n        '''\n        raise NotImplementedError('PBVI class is abstract so expand function is not implemented, make an PBVI_agent subclass to implement the method')\n\n\n    def backup(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               gamma:float=0.99,\n               append:bool=False,\n               belief_dominance_prune:bool=True,\n               use_gpu:bool=False # TODO Remove this\n               ) -&gt; ValueFunction:\n        '''\n        This function has purpose to update the set of alpha vectors. It does so in 3 steps:\n        1. It creates projections from each alpha vector for each possible action and each possible observation\n        2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.\n        3. Then it further collapses the set to take the best alpha vector and action per belief\n        In the end we have a set of alpha vectors as large as the amount of beliefs.\n\n        The alpha vectors are also pruned to avoid duplicates and remove dominated ones.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            The belief set to use to generate the new alpha vectors with.\n        value_function : ValueFunction\n            The alpha vectors to generate the new set from.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        append : bool, default=False\n            Whether to append the new alpha vectors generated to the old alpha vectors before pruning.\n        belief_dominance_prune : bool, default=True\n            Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.\n        use_gpu : bool, default=False\n\n        Returns\n        -------\n        new_alpha_set : ValueFunction\n            A list of updated alpha vectors.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Step 1\n        vector_array = value_function.alpha_vector_array\n        vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]\n\n        gamma_a_o_t = gamma * xp.einsum('saor,vsar-&gt;aovs', model.reachable_transitional_observation_table, vectors_array_reachable_states)\n\n        # Step 2\n        belief_array = belief_set.belief_array # bs\n        best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao\n\n        best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos\n\n        alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas\n\n        # Step 3\n        best_actions = xp.argmax(xp.einsum('bas,bs-&gt;ba', alpha_a, belief_array), axis=1)\n        alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]\n\n        # Belief domination\n        if belief_dominance_prune:\n            best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)\n            old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)\n            dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief\n\n            best_actions = best_actions[dominating_vectors]\n            alpha_vectors = alpha_vectors[dominating_vectors]\n\n        # Creation of value function\n        new_value_function = ValueFunction(model, alpha_vectors, best_actions)\n\n        # Union with previous value function\n        if append:\n            new_value_function.extend(value_function)\n\n        return new_value_function\n\n\n    def initialize_state(self, n:int=1) -&gt; None:\n        '''\n        To use an agent within a simulation, the agent's state needs to be initialized.\n        The initialization consists of setting the agent's initial belief.\n        Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n        Parameters\n        ----------\n        n : int, default=1\n            How many agents are to be used during the simulation.\n        '''\n        assert self.value_function is not None, \"Agent was not trained, run the training function first...\"\n\n        self.belief = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n\n\n    def choose_action(self) -&gt; np.ndarray:\n        '''\n        Function to let the agent or set of agents choose an action based on their current belief.\n\n        Returns\n        -------\n        movement_vector : np.ndarray\n            A single or a list of actions chosen by the agent(s) based on their belief.\n        '''\n        assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Evaluated value function\n        _, action = self.value_function.evaluate_at(self.belief)\n\n        # Recording the action played\n        self.action_played = action\n\n        # Converting action indexes to movement vectors\n        movemement_vector = self.model.movement_vector[action,:]\n\n        return movemement_vector\n\n\n    def update_state(self,\n                     observation:np.ndarray,\n                     source_reached:np.ndarray\n                     ) -&gt; None:\n        '''\n        Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n        Parameters\n        ----------\n        observation : np.ndarray\n            The observation(s) the agent(s) made.\n        source_reached : np.ndarray\n            A boolean array of whether the agent(s) have reached the source or not.\n        '''\n        assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n        # Binarize observations\n        observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n        observation_ids[source_reached] = 2 # Observe source\n\n        # Update the set of beliefs\n        self.belief = self.belief.update(actions=self.action_played, observations=observation_ids)\n\n        # Remove the beliefs of the agents having reached the source\n        self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~source_reached])\n\n\n    def kill(self,\n             simulations_to_kill:np.ndarray\n             ) -&gt; None:\n        '''\n        Function to kill any simulations that have not reached the source but can't continue further\n\n        Parameters\n        ----------\n        simulations_to_kill : np.ndarray\n            A boolean array of the simulations to kill.\n        '''\n        if all(simulations_to_kill):\n            self.belief = None\n        else:\n            self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.backup","title":"<code>backup(belief_set, value_function, gamma=0.99, append=False, belief_dominance_prune=True, use_gpu=False)</code>","text":"<p>This function has purpose to update the set of alpha vectors. It does so in 3 steps: 1. It creates projections from each alpha vector for each possible action and each possible observation 2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief. 3. Then it further collapses the set to take the best alpha vector and action per belief In the end we have a set of alpha vectors as large as the amount of beliefs.</p> <p>The alpha vectors are also pruned to avoid duplicates and remove dominated ones.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>The belief set to use to generate the new alpha vectors with.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The alpha vectors to generate the new set from.</p> required <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>append</code> <code>bool</code> <p>Whether to append the new alpha vectors generated to the old alpha vectors before pruning.</p> <code>False</code> <code>belief_dominance_prune</code> <code>bool</code> <p>Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.</p> <code>True</code> <code>use_gpu</code> <code>bool</code> <code>False</code> <p>Returns:</p> Name Type Description <code>new_alpha_set</code> <code>ValueFunction</code> <p>A list of updated alpha vectors.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def backup(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           gamma:float=0.99,\n           append:bool=False,\n           belief_dominance_prune:bool=True,\n           use_gpu:bool=False # TODO Remove this\n           ) -&gt; ValueFunction:\n    '''\n    This function has purpose to update the set of alpha vectors. It does so in 3 steps:\n    1. It creates projections from each alpha vector for each possible action and each possible observation\n    2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.\n    3. Then it further collapses the set to take the best alpha vector and action per belief\n    In the end we have a set of alpha vectors as large as the amount of beliefs.\n\n    The alpha vectors are also pruned to avoid duplicates and remove dominated ones.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        The belief set to use to generate the new alpha vectors with.\n    value_function : ValueFunction\n        The alpha vectors to generate the new set from.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    append : bool, default=False\n        Whether to append the new alpha vectors generated to the old alpha vectors before pruning.\n    belief_dominance_prune : bool, default=True\n        Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.\n    use_gpu : bool, default=False\n\n    Returns\n    -------\n    new_alpha_set : ValueFunction\n        A list of updated alpha vectors.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Step 1\n    vector_array = value_function.alpha_vector_array\n    vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]\n\n    gamma_a_o_t = gamma * xp.einsum('saor,vsar-&gt;aovs', model.reachable_transitional_observation_table, vectors_array_reachable_states)\n\n    # Step 2\n    belief_array = belief_set.belief_array # bs\n    best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao\n\n    best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos\n\n    alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas\n\n    # Step 3\n    best_actions = xp.argmax(xp.einsum('bas,bs-&gt;ba', alpha_a, belief_array), axis=1)\n    alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]\n\n    # Belief domination\n    if belief_dominance_prune:\n        best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)\n        old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)\n        dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief\n\n        best_actions = best_actions[dominating_vectors]\n        alpha_vectors = alpha_vectors[dominating_vectors]\n\n    # Creation of value function\n    new_value_function = ValueFunction(model, alpha_vectors, best_actions)\n\n    # Union with previous value function\n    if append:\n        new_value_function.extend(value_function)\n\n    return new_value_function\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.choose_action","title":"<code>choose_action()</code>","text":"<p>Function to let the agent or set of agents choose an action based on their current belief.</p> <p>Returns:</p> Name Type Description <code>movement_vector</code> <code>ndarray</code> <p>A single or a list of actions chosen by the agent(s) based on their belief.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def choose_action(self) -&gt; np.ndarray:\n    '''\n    Function to let the agent or set of agents choose an action based on their current belief.\n\n    Returns\n    -------\n    movement_vector : np.ndarray\n        A single or a list of actions chosen by the agent(s) based on their belief.\n    '''\n    assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Evaluated value function\n    _, action = self.value_function.evaluate_at(self.belief)\n\n    # Recording the action played\n    self.action_played = action\n\n    # Converting action indexes to movement vectors\n    movemement_vector = self.model.movement_vector[action,:]\n\n    return movemement_vector\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.compute_change","title":"<code>compute_change(value_function, new_value_function, belief_set)</code>","text":"<p>Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver. It check for each belief, the maximum value and take the max change between believe's value functions. If this max change is lower than eps * (gamma / (1 - gamma)).</p> <p>Parameters:</p> Name Type Description Default <code>value_function</code> <code>ValueFunction</code> <p>The first value function to compare.</p> required <code>new_value_function</code> <code>ValueFunction</code> <p>The second value function to compare.</p> required <code>belief_set</code> <code>BeliefSet</code> <p>The set of believes to check the values on to compute the max change on.</p> required <p>Returns:</p> Name Type Description <code>max_change</code> <code>float</code> <p>The maximum change between value functions at belief points.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def compute_change(self,\n                   value_function:ValueFunction,\n                   new_value_function:ValueFunction,\n                   belief_set:BeliefSet\n                   ) -&gt; float:\n    '''\n    Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.\n    It check for each belief, the maximum value and take the max change between believe's value functions.\n    If this max change is lower than eps * (gamma / (1 - gamma)).\n\n    Parameters\n    ----------\n    value_function : ValueFunction\n        The first value function to compare.\n    new_value_function : ValueFunction\n        The second value function to compare.\n    belief_set : BeliefSet\n        The set of believes to check the values on to compute the max change on.\n\n    Returns\n    -------\n    max_change : float\n        The maximum change between value functions at belief points.\n    '''\n    # Get numpy corresponding to the arrays\n    xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)\n\n    # Computing Delta for each beliefs\n    max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)\n    new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)\n    max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))\n\n    return max_change\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False, **kwargs)</code>","text":"<p>Abstract function! This function should be implemented in subclasses. The expand function consists in the exploration of the belief set. It takes as input a belief set and generates at most 'max_generation' beliefs from it.</p> <p>The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>The belief or set of beliefs to be used as a starting point for the exploration.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. To be used to guide the exploration process.</p> required <code>max_generation</code> <code>int</code> <p>How many beliefs to be generated at most.</p> required <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <code>kwargs</code> <p>Special parameters for the particular flavors of the PBVI Agent.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>A new (or expanded) set of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False, # TODO Remove this\n           **kwargs\n           ) -&gt; BeliefSet:\n    '''\n    Abstract function!\n    This function should be implemented in subclasses.\n    The expand function consists in the exploration of the belief set.\n    It takes as input a belief set and generates at most 'max_generation' beliefs from it.\n\n    The current value function is also passed as an argument as it is used in some PBVI techniques to guide the belief exploration.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        The belief or set of beliefs to be used as a starting point for the exploration.\n    value_function : ValueFunction\n        The current value function. To be used to guide the exploration process.\n    max_generation : int\n        How many beliefs to be generated at most.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n    kwargs\n        Special parameters for the particular flavors of the PBVI Agent.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        A new (or expanded) set of beliefs.\n    '''\n    raise NotImplementedError('PBVI class is abstract so expand function is not implemented, make an PBVI_agent subclass to implement the method')\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.initialize_state","title":"<code>initialize_state(n=1)</code>","text":"<p>To use an agent within a simulation, the agent's state needs to be initialized. The initialization consists of setting the agent's initial belief. Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>How many agents are to be used during the simulation.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def initialize_state(self, n:int=1) -&gt; None:\n    '''\n    To use an agent within a simulation, the agent's state needs to be initialized.\n    The initialization consists of setting the agent's initial belief.\n    Multiple agents can be used at once for simulations, for this reason, the belief parameter is a BeliefSet by default.\n\n    Parameters\n    ----------\n    n : int, default=1\n        How many agents are to be used during the simulation.\n    '''\n    assert self.value_function is not None, \"Agent was not trained, run the training function first...\"\n\n    self.belief = BeliefSet(self.model, [Belief(self.model) for _ in range(n)])\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.kill","title":"<code>kill(simulations_to_kill)</code>","text":"<p>Function to kill any simulations that have not reached the source but can't continue further</p> <p>Parameters:</p> Name Type Description Default <code>simulations_to_kill</code> <code>ndarray</code> <p>A boolean array of the simulations to kill.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def kill(self,\n         simulations_to_kill:np.ndarray\n         ) -&gt; None:\n    '''\n    Function to kill any simulations that have not reached the source but can't continue further\n\n    Parameters\n    ----------\n    simulations_to_kill : np.ndarray\n        A boolean array of the simulations to kill.\n    '''\n    if all(simulations_to_kill):\n        self.belief = None\n    else:\n        self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~simulations_to_kill])\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.load","title":"<code>load(folder)</code>  <code>classmethod</code>","text":"<p>Function to load a PBVI agent from a given folder it has been saved to. It will load the environment the agent has been trained on along with it.</p> <p>If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The agent folder.</p> required <p>Returns:</p> Name Type Description <code>instance</code> <code>PBVI_Agent</code> <p>The loaded instance of the PBVI Agent.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>@classmethod\ndef load(cls,\n         folder:str\n         ) -&gt; 'PBVI_Agent':\n    '''\n    Function to load a PBVI agent from a given folder it has been saved to.\n    It will load the environment the agent has been trained on along with it.\n\n    If it is a subclass of the PBVI_Agent, an instance of that specific subclass will be returned.\n\n    Parameters\n    ----------\n    folder : str\n        The agent folder.\n\n    Returns\n    -------\n    instance : PBVI_Agent\n        The loaded instance of the PBVI Agent.\n    '''\n    # Load arguments\n    arguments = None\n    with open(folder + '/METADATA.json', 'r') as json_file:\n        arguments = json.load(json_file)\n\n    # Load environment\n    environment = Environment.load(arguments['environment_saved_at'])\n\n    # Load specific class\n    if arguments['class'] != 'PBVI_Agent':\n        from olfactory_navigation import agents\n        cls = {name:obj for name, obj in inspect.getmembers(agents)}[arguments['class']]\n\n    # Build instance\n    instance = cls(\n        environment=environment,\n        threshold=arguments['threshold'],\n        name=arguments['name']\n    )\n\n    # Load and set the value function on the instance\n    instance.value_function = ValueFunction.load(\n        file=folder + '/Value_Function.npy',\n        model=instance.model\n    )\n    instance.trained_at = arguments['trained_at']\n    instance.saved_at = folder\n\n    return instance\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.save","title":"<code>save(folder=None, force=False, save_environment=False)</code>","text":"<p>The save function for PBVI Agents consists in recording the value function after the training. It saves the agent in a folder with the name of the agent (class name + training timestamp). In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.</p> <p>Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse. If the agent has already been saved, the saving will not happen unless the force parameter is toggled.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder under which to save the agent (a subfolder will be created under this folder). The agent will therefore be saved at /Agent- . By default the current folder is used. <code>None</code> <code>force</code> <code>bool</code> <p>Whether to overwrite an already saved agent with the same name at the same path.</p> <code>False</code> <code>save_environment</code> <code>bool</code> <p>Whether to save the environment data along with the agent.</p> <code>False</code> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def save(self,\n         folder:str|None=None,\n         force:bool=False,\n         save_environment:bool=False\n         ) -&gt; None:\n    '''\n    The save function for PBVI Agents consists in recording the value function after the training.\n    It saves the agent in a folder with the name of the agent (class name + training timestamp).\n    In this folder, there will be the metadata of the agent (all the attributes) in a json format and the value function.\n\n    Optionally, the environment can be saved too to be able to load it alongside the agent for future reuse.\n    If the agent has already been saved, the saving will not happen unless the force parameter is toggled.\n\n    Parameters\n    ----------\n    folder : str, optional\n        The folder under which to save the agent (a subfolder will be created under this folder).\n        The agent will therefore be saved at &lt;folder&gt;/Agent-&lt;agent_name&gt; .\n        By default the current folder is used.\n    force : bool, default=False\n        Whether to overwrite an already saved agent with the same name at the same path.\n    save_environment : bool, default=False\n        Whether to save the environment data along with the agent.\n    '''\n    assert self.trained_at is not None, \"The agent is not trained, there is nothing to save.\"\n\n    # Adding env name to folder path\n    if folder is None:\n        folder = f'./Agent-{self.name}'\n    else:\n        folder += '/Agent-' + self.name\n\n    # Checking the folder exists or creates it\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    elif len(os.listdir(folder)):\n        if force:\n            shutil.rmtree(folder)\n            os.mkdir(folder)\n        else:\n            raise Exception(f'{folder} is not empty. If you want to overwrite the saved model, enable \"force\".')\n\n    # If requested save environment\n    if save_environment:\n        self.environment.save(folder=folder)\n\n    # Generating the metadata arguments dictionary\n    arguments = {}\n    arguments['name'] = self.name\n    arguments['class'] = self.class_name\n    arguments['threshold'] = self.threshold\n    arguments['environment_name'] = self.environment.name\n    arguments['environment_saved_at'] = self.environment.saved_at\n    arguments['trained_at'] = self.trained_at\n\n    # Output the arguments to a METADATA file\n    with open(folder + '/METADATA.json', 'w') as json_file:\n        json.dump(arguments, json_file, indent=4)\n\n    # Save value function\n    self.value_function.save(folder=folder, file_name='Value_Function.npy')\n\n    # Finalization\n    self.saved_at = os.path.abspath(folder).replace('\\\\', '/')\n    print(f'Agent saved to: {folder}')\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function to send the numpy arrays of the agent to the gpu. It returns a new instance of the Agent class with the arrays on the gpu</p> <p>Returns:</p> Name Type Description <code>gpu_agent</code> <code>Agent</code> <p>A copy of the agent with the arrays on the GPU.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def to_gpu(self) -&gt; Agent:\n    '''\n    Function to send the numpy arrays of the agent to the gpu.\n    It returns a new instance of the Agent class with the arrays on the gpu\n\n    Returns\n    -------\n    gpu_agent : Agent\n        A copy of the agent with the arrays on the GPU.\n    '''\n    # Generating a new instance\n    cls = self.__class__\n    gpu_agent = cls.__new__(cls)\n\n    # Copying arguments to gpu\n    for arg, val in self.__dict__.items():\n        if isinstance(val, np.ndarray):\n            setattr(gpu_agent, arg, cp.array(val))\n        elif isinstance(val, Model):\n            gpu_agent.model = self.model.gpu_model\n        elif isinstance(val, ValueFunction):\n            gpu_agent.value_function =self.value_function.to_gpu()\n        elif isinstance(val, BeliefSet):\n            gpu_agent.belief = self.belief.to_gpu()\n        else:\n            setattr(gpu_agent, arg, val)\n\n    # Self reference instances\n    self._alternate_version = gpu_agent\n    gpu_agent._alternate_version = self\n\n    gpu_agent.on_gpu = True\n    return gpu_agent\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.train","title":"<code>train(expansions, full_backup=True, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True, **expand_arguments)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>full_backup</code> <code>bool</code> <p>Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.</p> <code>True</code> <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <code>expand_arguments</code> <code>kwargs</code> <p>An arbitrary amount of parameters that will be passed on to the expand function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          full_backup:bool=True,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:Union[BeliefSet, Belief, None]=None,\n          initial_value_function:Union[ValueFunction,None]=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False, # TODO rehandle the way things are run on GPU\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True,\n          **expand_arguments\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    full_backup : bool, default=True\n        Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n    expand_arguments : kwargs\n        An arbitrary amount of parameters that will be passed on to the expand function.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Initial belief\n    if initial_belief is None:\n        belief_set = BeliefSet(model, [Belief(model)])\n    elif isinstance(initial_belief, BeliefSet):\n        belief_set = initial_belief.to_gpu() if use_gpu else initial_belief \n    else:\n        initial_belief = Belief(model, xp.array(initial_belief.values))\n        belief_set = BeliefSet(model, [initial_belief])\n\n    # Handeling the case where the agent is already trained\n    if (self.value_function is not None):\n        if not force:\n            raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n        else:\n            self.trained_at = None\n            self.name = '-'.join(self.name.split('-')[:-1])\n            self.value_function = None\n\n    # Initial value function\n    if initial_value_function is None:\n        value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)\n    else:\n        value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function\n\n    # Convergence check boundary\n    max_allowed_change = eps * (gamma / (1-gamma))\n\n    # History tracking\n    training_history = TrainingHistory(tracking_level=history_tracking_level,\n                                       model=model,\n                                       gamma=gamma,\n                                       eps=eps,\n                                       expand_append=full_backup,\n                                       initial_value_function=value_function,\n                                       initial_belief_set=belief_set)\n\n    # Loop\n    iteration = 0\n    expand_value_function = value_function\n    old_value_function = value_function\n\n    try:\n        iterator = trange(expansions, desc='Expansions') if print_progress else range(expansions)\n        iterator_postfix = {}\n        for expansion_i in iterator:\n\n            # 1: Expand belief set\n            start_ts = datetime.now()\n\n            new_belief_set = self.expand(belief_set=belief_set,\n                                         value_function=value_function,\n                                         max_generation=max_belief_growth,\n                                         use_gpu=use_gpu,\n                                         **expand_arguments)\n\n            # Add new beliefs points to the total belief_set\n            belief_set = belief_set.union(new_belief_set)\n\n            expand_time = (datetime.now() - start_ts).total_seconds()\n            training_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)\n\n            # 2: Backup, update value function (alpha vector set)\n            for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f'Backups {expansion_i}'):\n                start_ts = datetime.now()\n\n                # Backup step\n                value_function = self.backup(belief_set if full_backup else new_belief_set,\n                                             value_function,\n                                             gamma=gamma,\n                                             append=(not full_backup),\n                                             belief_dominance_prune=False,\n                                             use_gpu=use_gpu)\n                backup_time = (datetime.now() - start_ts).total_seconds()\n\n                # Additional pruning\n                if (iteration % prune_interval) == 0 and iteration &gt; 0:\n                    start_ts = datetime.now()\n                    vf_len = len(value_function)\n\n                    value_function.prune(prune_level)\n\n                    prune_time = (datetime.now() - start_ts).total_seconds()\n                    alpha_vectors_pruned = len(value_function) - vf_len\n                    training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n                # Check if value function size is above threshold\n                if limit_value_function_size &gt;= 0 and len(value_function) &gt; limit_value_function_size:\n                    # Compute matrix multiplications between avs and beliefs\n                    alpha_value_per_belief = xp.matmul(value_function.alpha_vector_array, belief_set.belief_array.T)\n\n                    # Select the useful alpha vectors\n                    best_alpha_vector_per_belief = xp.argmax(alpha_value_per_belief, axis=0)\n                    useful_alpha_vectors = xp.unique(best_alpha_vector_per_belief)\n\n                    # Select a random selection of vectors to delete\n                    unuseful_alpha_vectors = xp.delete(xp.arange(len(value_function)), useful_alpha_vectors)\n                    random_vectors_to_delete = xp.random.choice(unuseful_alpha_vectors,\n                                                                size=max_belief_growth,\n                                                                p=(xp.arange(len(unuseful_alpha_vectors))[::-1] / xp.sum(xp.arange(len(unuseful_alpha_vectors)))))\n                                                                # replace=False,\n                                                                # p=1/len(unuseful_alpha_vectors))\n\n                    value_function = ValueFunction(model=model,\n                                                   alpha_vectors=xp.delete(value_function.alpha_vector_array, random_vectors_to_delete, axis=0),\n                                                   action_list=xp.delete(value_function.actions, random_vectors_to_delete))\n\n                    iterator_postfix['|useful|'] = useful_alpha_vectors.shape[0]\n\n                # Compute the change between value functions\n                max_change = self.compute_change(value_function, old_value_function, belief_set)\n\n                # History tracking\n                training_history.add_backup_step(backup_time, max_change, value_function)\n\n                # Convergence check\n                if max_change &lt; max_allowed_change:\n                    break\n\n                old_value_function = value_function\n\n                # Update iteration counter\n                iteration += 1\n\n            # Compute change with old expansion value function\n            expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)\n\n            if expand_max_change &lt; max_allowed_change:\n                print('Converged!')\n                break\n\n            expand_value_function = value_function\n\n            iterator_postfix['|V|'] = len(value_function)\n            iterator_postfix['|B|'] = len(belief_set)\n\n            if print_progress:\n                iterator.set_postfix(iterator_postfix)\n\n    except MemoryError as e:\n        print(f'Memory full: {e}')\n        print('Returning value function and history as is...\\n')\n\n    # Final pruning\n    start_ts = datetime.now()\n    vf_len = len(value_function)\n\n    value_function.prune(prune_level)\n\n    # History tracking\n    prune_time = (datetime.now() - start_ts).total_seconds()\n    alpha_vectors_pruned = len(value_function) - vf_len\n    training_history.add_prune_step(prune_time, alpha_vectors_pruned)\n\n    # Record when it was trained\n    self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n    self.name += f'-trained_{self.trained_at}'\n\n    self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n    # Print stats if requested\n    if print_stats:\n        print(training_history.summary)\n\n    return training_history\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.PBVI_Agent.update_state","title":"<code>update_state(observation, source_reached)</code>","text":"<p>Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>ndarray</code> <p>The observation(s) the agent(s) made.</p> required <code>source_reached</code> <code>ndarray</code> <p>A boolean array of whether the agent(s) have reached the source or not.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def update_state(self,\n                 observation:np.ndarray,\n                 source_reached:np.ndarray\n                 ) -&gt; None:\n    '''\n    Function to update the internal state(s) of the agent(s) based on the previous action(s) taken and the observation(s) received.\n\n    Parameters\n    ----------\n    observation : np.ndarray\n        The observation(s) the agent(s) made.\n    source_reached : np.ndarray\n        A boolean array of whether the agent(s) have reached the source or not.\n    '''\n    assert self.belief is not None, \"Agent was not initialized yet, run the initialize_state function first\"\n\n    # Binarize observations\n    observation_ids = np.where(observation &gt; self.threshold, 1, 0).astype(int)\n    observation_ids[source_reached] = 2 # Observe source\n\n    # Update the set of beliefs\n    self.belief = self.belief.update(actions=self.action_played, observations=observation_ids)\n\n    # Remove the beliefs of the agents having reached the source\n    self.belief = BeliefSet(self.belief.model, self.belief.belief_array[~source_reached])\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory","title":"<code>TrainingHistory</code>","text":"<p>Class to represent the history of a solver for a POMDP solver. It has mainly the purpose to have visualizations for the solution, belief set and the whole solving history. The visualizations available are:     - Belief set plot     - Solution plot     - Video of value function and belief set evolution over training.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>tracking_level</code> <code>int</code> <p>The tracking level of the solver.</p> required <code>model</code> <code>Model</code> <p>The model the solver has solved.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter used by the solver (learning rate).</p> required <code>eps</code> <code>float</code> <p>The epsilon parameter used by the solver (covergence bound).</p> required <code>expand_append</code> <code>bool</code> <p>Whether the expand function appends new belief points to the belief set of reloads it all.</p> required <code>initial_value_function</code> <code>ValueFunction</code> <p>The initial value function the solver will use to start the solving process.</p> required <code>initial_belief_set</code> <code>BeliefSet</code> <p>The initial belief set the solver will use to start the solving process.</p> required <p>Attributes:</p> Name Type Description <code>tracking_level</code> <code>int</code> <code>model</code> <code>Model</code> <code>gamma</code> <code>float</code> <code>eps</code> <code>float</code> <code>expand_append</code> <code>bool</code> <code>run_ts</code> <code>datetime</code> <p>The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.</p> <code>expansion_times</code> <code>list[float]</code> <p>A list of recorded times of the expand function.</p> <code>backup_times</code> <code>list[float]</code> <p>A list of recorded times of the backup function.</p> <code>alpha_vector_counts</code> <code>list[int]</code> <p>A list of recorded alpha vector count making up the value function over the solving process.</p> <code>beliefs_counts</code> <code>list[int]</code> <p>A list of recorded belief count making up the belief set over the solving process.</p> <code>value_function_changes</code> <code>list[float]</code> <p>A list of recorded value function changes (the maximum changed value between 2 value functions).</p> <code>value_functions</code> <code>list[ValueFunction]</code> <p>A list of recorded value functions.</p> <code>belief_sets</code> <code>list[BeliefSet]</code> <p>A list of recorded belief sets.</p> <code>solution</code> <code>ValueFunction</code> <code>explored_beliefs</code> <code>BeliefSet</code> Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>class TrainingHistory:\n    '''\n    Class to represent the history of a solver for a POMDP solver.\n    It has mainly the purpose to have visualizations for the solution, belief set and the whole solving history.\n    The visualizations available are:\n        - Belief set plot\n        - Solution plot\n        - Video of value function and belief set evolution over training.\n\n    ...\n\n    Parameters\n    ----------\n    tracking_level : int\n        The tracking level of the solver.\n    model : Model\n        The model the solver has solved.\n    gamma : float\n        The gamma parameter used by the solver (learning rate).\n    eps : float\n        The epsilon parameter used by the solver (covergence bound).\n    expand_append : bool\n        Whether the expand function appends new belief points to the belief set of reloads it all.\n    initial_value_function : ValueFunction\n        The initial value function the solver will use to start the solving process.\n    initial_belief_set : BeliefSet\n        The initial belief set the solver will use to start the solving process.\n\n    Attributes\n    ----------\n    tracking_level : int\n    model : Model\n    gamma : float\n    eps : float\n    expand_append : bool\n    run_ts : datetime\n        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.\n    expansion_times : list[float]\n        A list of recorded times of the expand function.\n    backup_times : list[float]\n        A list of recorded times of the backup function.\n    alpha_vector_counts : list[int]\n        A list of recorded alpha vector count making up the value function over the solving process.\n    beliefs_counts : list[int]\n        A list of recorded belief count making up the belief set over the solving process.\n    value_function_changes : list[float]\n        A list of recorded value function changes (the maximum changed value between 2 value functions).\n    value_functions : list[ValueFunction]\n        A list of recorded value functions.\n    belief_sets : list[BeliefSet]\n        A list of recorded belief sets.\n    solution : ValueFunction\n    explored_beliefs : BeliefSet\n    '''\n    def __init__(self,\n                 tracking_level:int,\n                 model:Model,\n                 gamma:float,\n                 eps:float,\n                 expand_append:bool,\n                 initial_value_function:ValueFunction,\n                 initial_belief_set:BeliefSet\n                 ):\n\n        self.tracking_level = tracking_level\n        self.model = model\n        self.gamma = gamma\n        self.eps = eps\n        self.run_ts = datetime.now()\n\n        self.expand_append = expand_append\n\n        # Time tracking\n        self.expansion_times = []\n        self.backup_times = []\n        self.pruning_times = []\n\n        # Value function and belief set sizes tracking\n        self.alpha_vector_counts = []\n        self.beliefs_counts = []\n        self.prune_counts = []\n\n        if self.tracking_level &gt;= 1:\n            self.alpha_vector_counts.append(len(initial_value_function))\n            self.beliefs_counts.append(len(initial_belief_set))\n\n        # Value function and belief set tracking\n        self.belief_sets = []\n        self.value_functions = []\n        self.value_function_changes = []\n\n        if self.tracking_level &gt;= 2:\n            self.belief_sets.append(initial_belief_set)\n            self.value_functions.append(initial_value_function)\n\n\n    @property\n    def solution(self) -&gt; ValueFunction:\n        '''\n        The last value function of the solving process.\n        '''\n        assert self.tracking_level &gt;= 2, \"Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.\"\n        return self.value_functions[-1]\n\n\n    @property\n    def explored_beliefs(self) -&gt; BeliefSet:\n        '''\n        The final set of beliefs explored during the solving.\n        '''\n        assert self.tracking_level &gt;= 2, \"Tracking level is set too low, increase it to 2 if you want to have belief sets tracking as well.\"\n        return self.belief_sets[-1]\n\n\n    def add_expand_step(self,\n                        expansion_time:float,\n                        belief_set:BeliefSet\n                        ) -&gt; None:\n        '''\n        Function to add an expansion step in the simulation history by the explored belief set the expand function generated.\n\n        Parameters\n        ----------\n        expansion_time : float\n            The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)\n        belief_set : BeliefSet\n            The belief set used for the Update step of the solving process.\n        '''\n        if self.tracking_level &gt;= 1:\n            self.expansion_times.append(float(expansion_time))\n            self.beliefs_counts.append(len(belief_set))\n\n        if self.tracking_level &gt;= 2:\n            self.belief_sets.append(belief_set if not belief_set.is_on_gpu else belief_set.to_cpu())\n\n\n    def add_backup_step(self,\n                        backup_time:float,\n                        value_function_change:float,\n                        value_function:ValueFunction\n                        ) -&gt; None:\n        '''\n        Function to add a backup step in the simulation history by recording the value function the backup function generated.\n\n        Parameters\n        ----------\n        backup_time : float\n            The time it took to run a step of backup of the value function. (Also known as the value function update.)\n        value_function_change : float\n            The change between the value function of this iteration and of the previous iteration.\n        value_function : ValueFunction\n            The value function resulting after a step of the solving process.\n        '''\n        if self.tracking_level &gt;= 1:\n            self.backup_times.append(float(backup_time))\n            self.alpha_vector_counts.append(len(value_function))\n            self.value_function_changes.append(float(value_function_change))\n\n        if self.tracking_level &gt;= 2:\n            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())\n\n\n    def add_prune_step(self,\n                       prune_time:float,\n                       alpha_vectors_pruned:int\n                       ) -&gt; None:\n        '''\n        Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.\n\n        Parameters\n        ----------\n        prune_time : float\n            The time it took to run the pruning step.\n        alpha_vectors_pruned : int\n            How many alpha vectors were pruned.\n        '''\n        if self.tracking_level &gt;= 1:\n            self.pruning_times.append(prune_time)\n            self.prune_counts.append(alpha_vectors_pruned)\n\n\n    @property\n    def summary(self) -&gt; str:\n        '''\n        A summary as a string of the information recorded.\n        '''\n        summary_str =  f'Summary of Point Based Value Iteration run'\n        summary_str += f'\\n  - Model: {self.model.state_count} state, {self.model.action_count} action, {self.model.observation_count} observations'\n        summary_str += f'\\n  - Converged or stopped after {len(self.expansion_times)} expansion steps and {len(self.backup_times)} backup steps.'\n\n        if self.tracking_level &gt;= 1:\n            summary_str += f'\\n  - Resulting value function has {self.alpha_vector_counts[-1]} alpha vectors.'\n            summary_str += f'\\n  - Converged in {(sum(self.expansion_times) + sum(self.backup_times)):.4f}s'\n            summary_str += f'\\n'\n\n            summary_str += f'\\n  - Expand function took on average {sum(self.expansion_times) / len(self.expansion_times):.4f}s '\n            if self.expand_append:\n                summary_str += f'and yielded on average {sum(np.diff(self.beliefs_counts)) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.'\n            else:\n                summary_str += f'and yielded on average {sum(self.beliefs_counts[1:]) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.'\n            summary_str += f' ({np.sum(np.divide(self.expansion_times, self.beliefs_counts[1:])) / len(self.expansion_times):.4f}s/it/belief)'\n\n            summary_str += f'\\n  - Backup function took on average {sum(self.backup_times) /len(self.backup_times):.4f}s '\n            summary_str += f'and yielded on average {np.average(np.diff(self.alpha_vector_counts)):.2f} alpha vectors per iteration.'\n            summary_str += f' ({np.sum(np.divide(self.backup_times, self.alpha_vector_counts[1:])) / len(self.backup_times):.4f}s/it/alpha)'\n\n            summary_str += f'\\n  - Pruning function took on average {sum(self.pruning_times) /len(self.pruning_times):.4f}s '\n            summary_str += f'and yielded on average prunings of {sum(self.prune_counts) / len(self.prune_counts):.2f} alpha vectors per iteration.'\n\n        return summary_str\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.explored_beliefs","title":"<code>explored_beliefs: BeliefSet</code>  <code>property</code>","text":"<p>The final set of beliefs explored during the solving.</p>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.solution","title":"<code>solution: ValueFunction</code>  <code>property</code>","text":"<p>The last value function of the solving process.</p>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.summary","title":"<code>summary: str</code>  <code>property</code>","text":"<p>A summary as a string of the information recorded.</p>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.add_backup_step","title":"<code>add_backup_step(backup_time, value_function_change, value_function)</code>","text":"<p>Function to add a backup step in the simulation history by recording the value function the backup function generated.</p> <p>Parameters:</p> Name Type Description Default <code>backup_time</code> <code>float</code> <p>The time it took to run a step of backup of the value function. (Also known as the value function update.)</p> required <code>value_function_change</code> <code>float</code> <p>The change between the value function of this iteration and of the previous iteration.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The value function resulting after a step of the solving process.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def add_backup_step(self,\n                    backup_time:float,\n                    value_function_change:float,\n                    value_function:ValueFunction\n                    ) -&gt; None:\n    '''\n    Function to add a backup step in the simulation history by recording the value function the backup function generated.\n\n    Parameters\n    ----------\n    backup_time : float\n        The time it took to run a step of backup of the value function. (Also known as the value function update.)\n    value_function_change : float\n        The change between the value function of this iteration and of the previous iteration.\n    value_function : ValueFunction\n        The value function resulting after a step of the solving process.\n    '''\n    if self.tracking_level &gt;= 1:\n        self.backup_times.append(float(backup_time))\n        self.alpha_vector_counts.append(len(value_function))\n        self.value_function_changes.append(float(value_function_change))\n\n    if self.tracking_level &gt;= 2:\n        self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.add_expand_step","title":"<code>add_expand_step(expansion_time, belief_set)</code>","text":"<p>Function to add an expansion step in the simulation history by the explored belief set the expand function generated.</p> <p>Parameters:</p> Name Type Description Default <code>expansion_time</code> <code>float</code> <p>The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)</p> required <code>belief_set</code> <code>BeliefSet</code> <p>The belief set used for the Update step of the solving process.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def add_expand_step(self,\n                    expansion_time:float,\n                    belief_set:BeliefSet\n                    ) -&gt; None:\n    '''\n    Function to add an expansion step in the simulation history by the explored belief set the expand function generated.\n\n    Parameters\n    ----------\n    expansion_time : float\n        The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)\n    belief_set : BeliefSet\n        The belief set used for the Update step of the solving process.\n    '''\n    if self.tracking_level &gt;= 1:\n        self.expansion_times.append(float(expansion_time))\n        self.beliefs_counts.append(len(belief_set))\n\n    if self.tracking_level &gt;= 2:\n        self.belief_sets.append(belief_set if not belief_set.is_on_gpu else belief_set.to_cpu())\n</code></pre>"},{"location":"reference/agents/pbvi_agent/#olfactory_navigation.agents.pbvi_agent.TrainingHistory.add_prune_step","title":"<code>add_prune_step(prune_time, alpha_vectors_pruned)</code>","text":"<p>Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.</p> <p>Parameters:</p> Name Type Description Default <code>prune_time</code> <code>float</code> <p>The time it took to run the pruning step.</p> required <code>alpha_vectors_pruned</code> <code>int</code> <p>How many alpha vectors were pruned.</p> required Source code in <code>olfactory_navigation\\agents\\pbvi_agent.py</code> <pre><code>def add_prune_step(self,\n                   prune_time:float,\n                   alpha_vectors_pruned:int\n                   ) -&gt; None:\n    '''\n    Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.\n\n    Parameters\n    ----------\n    prune_time : float\n        The time it took to run the pruning step.\n    alpha_vectors_pruned : int\n        How many alpha vectors were pruned.\n    '''\n    if self.tracking_level &gt;= 1:\n        self.pruning_times.append(prune_time)\n        self.prune_counts.append(alpha_vectors_pruned)\n</code></pre>"},{"location":"reference/agents/pbvi_ger_agent/","title":"pbvi_ger_agent","text":""},{"location":"reference/agents/pbvi_ger_agent/#olfactory_navigation.agents.pbvi_ger_agent.PBVI_GER_Agent","title":"<code>PBVI_GER_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing belief points that will most decrease the error in the value function (so increasing most the value).</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>class PBVI_GER_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing belief points that will most decrease the error in the value function (so increasing most the value).\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Greedy Error Reduction.\n        It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.\n        The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. Used to compute the value at belief points.\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))\n        new_belief_array[:old_shape[0]] = belief_set.belief_array\n\n        # Finding the min and max rewards for computation of the epsilon\n        r_min = model._min_reward / (1 - self.gamma)\n        r_max = model._max_reward / (1 - self.gamma)\n\n        # Generation of all potential successor beliefs\n        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n        # Finding the alphas associated with each previous beliefs\n        best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)\n        b_alphas = value_function.alpha_vector_array[best_alpha]\n\n        # Difference between beliefs and their successors\n        b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]\n\n        # Computing a 'next' alpha vector made of the max and min\n        alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)\n\n        # Difference between alpha vectors and their successors alpha vector\n        alphas_diffs = alphas_p - b_alphas[:,None,None,:]\n\n        # Computing epsilon for all successor beliefs\n        eps = xp.einsum('baos,baos-&gt;bao', alphas_diffs, b_diffs)\n\n        # Computing the probability of the b and doing action a and receiving observation o\n        bao_probs = xp.einsum('bs,saor-&gt;bao', belief_set.belief_array, model.reachable_transitional_observation_table)\n\n        # Taking the sumproduct of the probs with the epsilons\n        res = xp.einsum('bao,bao-&gt;ba', bao_probs, eps)\n\n        # Picking the correct amount of initial beliefs and ideal actions\n        b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)\n\n        # And picking the ideal observations\n        o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)\n\n        # Selecting the successor beliefs\n        new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Greedy Error Reduction Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ger_agent/#olfactory_navigation.agents.pbvi_ger_agent.PBVI_GER_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Greedy Error Reduction. It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error. The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. Used to compute the value at belief points.</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Greedy Error Reduction.\n    It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.\n    The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. Used to compute the value at belief points.\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))\n    new_belief_array[:old_shape[0]] = belief_set.belief_array\n\n    # Finding the min and max rewards for computation of the epsilon\n    r_min = model._min_reward / (1 - self.gamma)\n    r_max = model._max_reward / (1 - self.gamma)\n\n    # Generation of all potential successor beliefs\n    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n    # Finding the alphas associated with each previous beliefs\n    best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)\n    b_alphas = value_function.alpha_vector_array[best_alpha]\n\n    # Difference between beliefs and their successors\n    b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]\n\n    # Computing a 'next' alpha vector made of the max and min\n    alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)\n\n    # Difference between alpha vectors and their successors alpha vector\n    alphas_diffs = alphas_p - b_alphas[:,None,None,:]\n\n    # Computing epsilon for all successor beliefs\n    eps = xp.einsum('baos,baos-&gt;bao', alphas_diffs, b_diffs)\n\n    # Computing the probability of the b and doing action a and receiving observation o\n    bao_probs = xp.einsum('bs,saor-&gt;bao', belief_set.belief_array, model.reachable_transitional_observation_table)\n\n    # Taking the sumproduct of the probs with the epsilons\n    res = xp.einsum('bao,bao-&gt;ba', bao_probs, eps)\n\n    # Picking the correct amount of initial beliefs and ideal actions\n    b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)\n\n    # And picking the ideal observations\n    o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)\n\n    # Selecting the successor beliefs\n    new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/pbvi_ger_agent/#olfactory_navigation.agents.pbvi_ger_agent.PBVI_GER_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Greedy Error Reduction Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ger_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Greedy Error Reduction Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ra_agent/","title":"pbvi_ra_agent","text":""},{"location":"reference/agents/pbvi_ra_agent/#olfactory_navigation.agents.pbvi_ra_agent.PBVI_RA_Agent","title":"<code>PBVI_RA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing random belief points.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>class PBVI_RA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing random belief points.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # How many new beliefs to add\n        generation_count = min(belief_set.belief_array.shape[0], max_generation)\n\n        # Generation of the new beliefs at random\n        new_beliefs = xp.random.random((generation_count, model.state_count))\n        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]\n\n        return BeliefSet(model, new_beliefs)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Random Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ra_agent/#olfactory_navigation.agents.pbvi_ra_agent.PBVI_RA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # How many new beliefs to add\n    generation_count = min(belief_set.belief_array.shape[0], max_generation)\n\n    # Generation of the new beliefs at random\n    new_beliefs = xp.random.random((generation_count, model.state_count))\n    new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]\n\n    return BeliefSet(model, new_beliefs)\n</code></pre>"},{"location":"reference/agents/pbvi_ra_agent/#olfactory_navigation.agents.pbvi_ra_agent.PBVI_RA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Random Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ra_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Random Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ssea_agent/","title":"pbvi_ssea_agent","text":""},{"location":"reference/agents/pbvi_ssea_agent/#olfactory_navigation.agents.pbvi_ssea_agent.PBVI_SSEA_Agent","title":"<code>PBVI_SSEA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing belief points furtest away (L2 distance) from any other belief point already in the belief set based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>class PBVI_SSEA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing belief points furtest away (L2 distance) from any other belief point already in the belief set based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Exploratory Action.\n        Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.\n        These lead to a new state s_p and a observation o for each action.\n        From all these and observation o we can generate updated beliefs. \n        Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        # Generation of successors\n        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n        # Compute the distances between each pair and of successor are source beliefs\n        diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)\n        dist = xp.sqrt(xp.einsum('bnaos,bnaos-&gt;bnao', diff, diff))\n\n        # Taking the min distance for each belief\n        belief_min_dists = xp.min(dist,axis=0)\n\n        # Taking the max distanced successors\n        b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])\n\n        # Selecting successor beliefs\n        new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Exploratory Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ssea_agent/#olfactory_navigation.agents.pbvi_ssea_agent.PBVI_SSEA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Stochastic Simulation with Exploratory Action. Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability. These lead to a new state s_p and a observation o for each action. From all these and observation o we can generate updated beliefs.  Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Exploratory Action.\n    Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.\n    These lead to a new state s_p and a observation o for each action.\n    From all these and observation o we can generate updated beliefs. \n    Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    # Generation of successors\n    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])\n\n    # Compute the distances between each pair and of successor are source beliefs\n    diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)\n    dist = xp.sqrt(xp.einsum('bnaos,bnaos-&gt;bnao', diff, diff))\n\n    # Taking the min distance for each belief\n    belief_min_dists = xp.min(dist,axis=0)\n\n    # Taking the max distanced successors\n    b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])\n\n    # Selecting successor beliefs\n    new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/pbvi_ssea_agent/#olfactory_navigation.agents.pbvi_ssea_agent.PBVI_SSEA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Exploratory Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssea_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Exploratory Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ssga_agent/","title":"pbvi_ssga_agent","text":""},{"location":"reference/agents/pbvi_ssga_agent/#olfactory_navigation.agents.pbvi_ssga_agent.PBVI_SSGA_Agent","title":"<code>PBVI_SSGA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing actions in an epsilon greedy fashion and generating random observations and generating belief points based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>class PBVI_SSGA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing actions in an epsilon greedy fashion and generating random observations and generating belief points based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False,\n               epsilon:float=0.99\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Greedy Action.\n        Simulates running a single-step forward from the beliefs in the \"belief_set\".\n        The step forward is taking assuming we are in a random state s (weighted by the belief),\n        then taking the best action a based on the belief with probability 'epsilon'.\n        These lead to a new state s_p and a observation o.\n        From this action a and observation o we can update our belief. \n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n        epsilon : float, default=0.99\n            The epsilon parameter that determines whether to choose an action greedily or randomly.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n        # Random previous beliefs\n        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n            b = Belief(model, belief_vector)\n            s = b.random_state()\n\n            if random.random() &lt; epsilon:\n                a = random.choice(model.actions)\n            else:\n                best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))\n                a = value_function.actions[best_alpha_index]\n\n            s_p = model.transition(s, a)\n            o = model.observe(s_p, a)\n            b_new = b.update(a, o)\n\n            new_belief_array[i] = b_new.values\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True,\n              epsilon:float=0.99\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Greedy Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n        epsilon : float, default=0.99\n            Expand function parameter. threshold to how often to choose the action greedily to how often randomly.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats,\n                             epsilon = epsilon)\n</code></pre>"},{"location":"reference/agents/pbvi_ssga_agent/#olfactory_navigation.agents.pbvi_ssga_agent.PBVI_SSGA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False, epsilon=0.99)</code>","text":"<p>Stochastic Simulation with Greedy Action. Simulates running a single-step forward from the beliefs in the \"belief_set\". The step forward is taking assuming we are in a random state s (weighted by the belief), then taking the best action a based on the belief with probability 'epsilon'. These lead to a new state s_p and a observation o. From this action a and observation o we can update our belief. </p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>The epsilon parameter that determines whether to choose an action greedily or randomly.</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False,\n           epsilon:float=0.99\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Greedy Action.\n    Simulates running a single-step forward from the beliefs in the \"belief_set\".\n    The step forward is taking assuming we are in a random state s (weighted by the belief),\n    then taking the best action a based on the belief with probability 'epsilon'.\n    These lead to a new state s_p and a observation o.\n    From this action a and observation o we can update our belief. \n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n    epsilon : float, default=0.99\n        The epsilon parameter that determines whether to choose an action greedily or randomly.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n    # Random previous beliefs\n    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n        b = Belief(model, belief_vector)\n        s = b.random_state()\n\n        if random.random() &lt; epsilon:\n            a = random.choice(model.actions)\n        else:\n            best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))\n            a = value_function.actions[best_alpha_index]\n\n        s_p = model.transition(s, a)\n        o = model.observe(s_p, a)\n        b_new = b.update(a, o)\n\n        new_belief_array[i] = b_new.values\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/pbvi_ssga_agent/#olfactory_navigation.agents.pbvi_ssga_agent.PBVI_SSGA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True, epsilon=0.99)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Greedy Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <code>epsilon</code> <code>float</code> <p>Expand function parameter. threshold to how often to choose the action greedily to how often randomly.</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssga_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True,\n          epsilon:float=0.99\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Greedy Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n    epsilon : float, default=0.99\n        Expand function parameter. threshold to how often to choose the action greedily to how often randomly.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats,\n                         epsilon = epsilon)\n</code></pre>"},{"location":"reference/agents/pbvi_ssra_agent/","title":"pbvi_ssra_agent","text":""},{"location":"reference/agents/pbvi_ssra_agent/#olfactory_navigation.agents.pbvi_ssra_agent.PBVI_SSRA_Agent","title":"<code>PBVI_SSRA_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. The expand function consists in choosing random actions and observations and generating belief points based on that.</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>class PBVI_SSRA_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. The expand function consists in choosing random actions and observations and generating belief points based on that.\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        Stochastic Simulation with Random Action.\n        Simulates running a single-step forward from the beliefs in the \"belief_set\".\n        The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.\n        From this action a and observation o we can update our belief.\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set_new : BeliefSet\n            Union of the belief_set and the expansions of the beliefs in the belief_set.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        old_shape = belief_set.belief_array.shape\n        to_generate = min(max_generation, old_shape[0])\n\n        new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n        # Random previous beliefs\n        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n            b = Belief(model, belief_vector)\n            s = b.random_state()\n            a = random.choice(model.actions)\n            s_p = model.transition(s, a)\n            o = model.observe(s_p, a)\n            b_new = b.update(a, o)\n\n            new_belief_array[i] = b_new.values\n\n        return BeliefSet(model, new_belief_array)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Stochastic Search with Random Action Point-Based Value Iteration:\n        - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = True,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/pbvi_ssra_agent/#olfactory_navigation.agents.pbvi_ssra_agent.PBVI_SSRA_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":"<p>Stochastic Simulation with Random Action. Simulates running a single-step forward from the beliefs in the \"belief_set\". The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o. From this action a and observation o we can update our belief.</p> <p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set_new</code> <code>BeliefSet</code> <p>Union of the belief_set and the expansions of the beliefs in the belief_set.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    Stochastic Simulation with Random Action.\n    Simulates running a single-step forward from the beliefs in the \"belief_set\".\n    The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.\n    From this action a and observation o we can update our belief.\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set_new : BeliefSet\n        Union of the belief_set and the expansions of the beliefs in the belief_set.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    old_shape = belief_set.belief_array.shape\n    to_generate = min(max_generation, old_shape[0])\n\n    new_belief_array = xp.empty((to_generate, old_shape[1]))\n\n    # Random previous beliefs\n    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)\n\n    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):\n        b = Belief(model, belief_vector)\n        s = b.random_state()\n        a = random.choice(model.actions)\n        s_p = model.transition(s, a)\n        o = model.observe(s_p, a)\n        b_new = b.update(a, o)\n\n        new_belief_array[i] = b_new.values\n\n    return BeliefSet(model, new_belief_array)\n</code></pre>"},{"location":"reference/agents/pbvi_ssra_agent/#olfactory_navigation.agents.pbvi_ssra_agent.PBVI_SSRA_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Stochastic Search with Random Action Point-Based Value Iteration: - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\pbvi_ssra_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Stochastic Search with Random Action Point-Based Value Iteration:\n    - By default it performs the backup on the whole set of beliefs generated since the start. (so it full_backup=True)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = True,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/perseus_agent/","title":"perseus_agent","text":""},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent","title":"<code>Perseus_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>A flavor of the PBVI Agent. </p>"},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent--todo-do-document-of-perseus-agent","title":"TODO: Do document of Perseus agent","text":""},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent--todo-fix-perseus-expand","title":"TODO: FIX Perseus expand","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>class Perseus_Agent(PBVI_Agent):\n    '''\n    A flavor of the PBVI Agent. \n\n    # TODO: Do document of Perseus agent\n    # TODO: FIX Perseus expand\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def expand(self,\n               belief_set:BeliefSet,\n               value_function:ValueFunction,\n               max_generation:int,\n               use_gpu:bool=False\n               ) -&gt; BeliefSet:\n        '''\n        # TODO\n\n        Parameters\n        ----------\n        belief_set : BeliefSet\n            List of beliefs to expand on.\n        value_function : ValueFunction\n            The current value function. (NOT USED)\n        max_generation : int, default=10\n            The max amount of beliefs that can be added to the belief set at once.\n        use_gpu : bool, default=False\n            Whether to run this operation on the GPU or not.\n\n        Returns\n        -------\n        belief_set : BeliefSet\n            A new sequence of beliefs.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        xp = np if not use_gpu else cp\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        b = belief_set.belief_list[0]\n        belief_sequence = []\n\n        for i in range(max_generation):\n            # Choose random action\n            a = int(xp.random.choice(model.actions, size=1)[0])\n\n            # Choose random observation based on prob: P(o|b,a)\n            obs_prob = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n            o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])\n\n            # Update belief\n            bao = b.update(a,o)\n\n            # Finalization\n            belief_sequence.append(bao)\n            b = bao\n\n        return BeliefSet(model, belief_sequence)\n\n\n    def train(self,\n              expansions:int,\n              update_passes:int=1,\n              max_belief_growth:int=10,\n              initial_belief:BeliefSet|Belief|None=None,\n              initial_value_function:ValueFunction|None=None,\n              prune_level:int=1,\n              prune_interval:int=10,\n              limit_value_function_size:int=-1,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Main loop of the Point-Based Value Iteration algorithm.\n        It consists in 2 steps, Backup and Expand.\n        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n        2. Backup: Updates the alpha vectors based on the current belief set\n\n        Heuristic Search Value Iteration:\n        - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n        Parameters\n        ----------\n        expansions : int\n            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n        update_passes : int, default=1\n            How many times the backup function has to be run every time the belief set is expanded.\n        max_belief_growth : int, default=10\n            How many beliefs can be added at every expansion step to the belief set.\n        initial_belief : BeliefSet or Belief, optional\n            An initial list of beliefs to start with.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        prune_level : int, default=1\n            Parameter to prune the value function further before the expand function.\n        prune_interval : int, default=10\n            How often to prune the value function. It is counted in number of backup iterations.\n        limit_value_function_size : int, default=-1\n            When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n            If set to -1, the value function can grow without bounds.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        return super().train(expansions = expansions,\n                             full_backup = False,\n                             update_passes = update_passes,\n                             max_belief_growth = max_belief_growth,\n                             initial_belief = initial_belief,\n                             initial_value_function = initial_value_function,\n                             prune_level = prune_level,\n                             prune_interval = prune_interval,\n                             limit_value_function_size = limit_value_function_size,\n                             gamma = gamma,\n                             eps = eps,\n                             use_gpu = use_gpu,\n                             history_tracking_level = history_tracking_level,\n                             force = force,\n                             print_progress = print_progress,\n                             print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent.expand","title":"<code>expand(belief_set, value_function, max_generation, use_gpu=False)</code>","text":""},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent.expand--todo","title":"TODO","text":"<p>Parameters:</p> Name Type Description Default <code>belief_set</code> <code>BeliefSet</code> <p>List of beliefs to expand on.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The current value function. (NOT USED)</p> required <code>max_generation</code> <code>int</code> <p>The max amount of beliefs that can be added to the belief set at once.</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to run this operation on the GPU or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>belief_set</code> <code>BeliefSet</code> <p>A new sequence of beliefs.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>def expand(self,\n           belief_set:BeliefSet,\n           value_function:ValueFunction,\n           max_generation:int,\n           use_gpu:bool=False\n           ) -&gt; BeliefSet:\n    '''\n    # TODO\n\n    Parameters\n    ----------\n    belief_set : BeliefSet\n        List of beliefs to expand on.\n    value_function : ValueFunction\n        The current value function. (NOT USED)\n    max_generation : int, default=10\n        The max amount of beliefs that can be added to the belief set at once.\n    use_gpu : bool, default=False\n        Whether to run this operation on the GPU or not.\n\n    Returns\n    -------\n    belief_set : BeliefSet\n        A new sequence of beliefs.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    b = belief_set.belief_list[0]\n    belief_sequence = []\n\n    for i in range(max_generation):\n        # Choose random action\n        a = int(xp.random.choice(model.actions, size=1)[0])\n\n        # Choose random observation based on prob: P(o|b,a)\n        obs_prob = xp.einsum('sor,s-&gt;o', model.reachable_transitional_observation_table[:,a,:,:], b.values)\n        o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])\n\n        # Update belief\n        bao = b.update(a,o)\n\n        # Finalization\n        belief_sequence.append(bao)\n        b = bao\n\n    return BeliefSet(model, belief_sequence)\n</code></pre>"},{"location":"reference/agents/perseus_agent/#olfactory_navigation.agents.perseus_agent.Perseus_Agent.train","title":"<code>train(expansions, update_passes=1, max_belief_growth=10, initial_belief=None, initial_value_function=None, prune_level=1, prune_interval=10, limit_value_function_size=-1, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Main loop of the Point-Based Value Iteration algorithm. It consists in 2 steps, Backup and Expand. 1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function 2. Backup: Updates the alpha vectors based on the current belief set</p> <p>Heuristic Search Value Iteration: - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</p> required <code>update_passes</code> <code>int</code> <p>How many times the backup function has to be run every time the belief set is expanded.</p> <code>1</code> <code>max_belief_growth</code> <code>int</code> <p>How many beliefs can be added at every expansion step to the belief set.</p> <code>10</code> <code>initial_belief</code> <code>BeliefSet or Belief</code> <p>An initial list of beliefs to start with.</p> <code>None</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>prune_level</code> <code>int</code> <p>Parameter to prune the value function further before the expand function.</p> <code>1</code> <code>prune_interval</code> <code>int</code> <p>How often to prune the value function. It is counted in number of backup iterations.</p> <code>10</code> <code>limit_value_function_size</code> <code>int</code> <p>When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function If set to -1, the value function can grow without bounds.</p> <code>-1</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\perseus_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          update_passes:int=1,\n          max_belief_growth:int=10,\n          initial_belief:BeliefSet|Belief|None=None,\n          initial_value_function:ValueFunction|None=None,\n          prune_level:int=1,\n          prune_interval:int=10,\n          limit_value_function_size:int=-1,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Main loop of the Point-Based Value Iteration algorithm.\n    It consists in 2 steps, Backup and Expand.\n    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function\n    2. Backup: Updates the alpha vectors based on the current belief set\n\n    Heuristic Search Value Iteration:\n    - By default it performs the backup only on set of beliefs generated by the expand function. (so it full_backup=False)\n\n    Parameters\n    ----------\n    expansions : int\n        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)\n    update_passes : int, default=1\n        How many times the backup function has to be run every time the belief set is expanded.\n    max_belief_growth : int, default=10\n        How many beliefs can be added at every expansion step to the belief set.\n    initial_belief : BeliefSet or Belief, optional\n        An initial list of beliefs to start with.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    prune_level : int, default=1\n        Parameter to prune the value function further before the expand function.\n    prune_interval : int, default=10\n        How often to prune the value function. It is counted in number of backup iterations.\n    limit_value_function_size : int, default=-1\n        When the value function size crosses this threshold, a random selection of 'max_belief_growth' alpha vectors will be removed from the value function\n        If set to -1, the value function can grow without bounds.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    return super().train(expansions = expansions,\n                         full_backup = False,\n                         update_passes = update_passes,\n                         max_belief_growth = max_belief_growth,\n                         initial_belief = initial_belief,\n                         initial_value_function = initial_value_function,\n                         prune_level = prune_level,\n                         prune_interval = prune_interval,\n                         limit_value_function_size = limit_value_function_size,\n                         gamma = gamma,\n                         eps = eps,\n                         use_gpu = use_gpu,\n                         history_tracking_level = history_tracking_level,\n                         force = force,\n                         print_progress = print_progress,\n                         print_stats = print_stats)\n</code></pre>"},{"location":"reference/agents/q_agent/","title":"q_agent","text":""},{"location":"reference/agents/qmdp_agent/","title":"qmdp_agent","text":""},{"location":"reference/agents/qmdp_agent/#olfactory_navigation.agents.qmdp_agent.QMDP_Agent","title":"<code>QMDP_Agent</code>","text":"<p>             Bases: <code>PBVI_Agent</code></p> <p>An agent that relies on Model-Based Reinforcement Learning. It is a simplified version of the PBVI_Agent. It runs the a Value Iteration solver, assuming full observability. The value function that comes out from this is therefore used to make choices.</p> <p>As stated, during simulations, the agent will choose actions based on an argmax of what action has the highest matrix product of the value function with the belief vector.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment to train the agent with.</p> required <code>threshold</code> <code>float</code> <p>The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.</p> <code>3e-6</code> <code>name</code> <code>str</code> <p>A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <code>threshold</code> <code>float</code> <code>name</code> <code>str</code> <code>model</code> <code>Model</code> <p>The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.</p> <code>saved_at</code> <code>str</code> <p>The place on disk where the agent has been saved (None if not saved yet).</p> <code>on_gpu</code> <code>bool</code> <p>Whether the agent has been sent to the gpu or not.</p> <code>trained_at</code> <code>str</code> <p>A string timestamp of when the agent has been trained (None if not trained yet).</p> <code>value_function</code> <code>ValueFunction</code> <p>The value function used for the agent to make decisions.</p> <code>belief</code> <code>BeliefSet</code> <p>Used only during simulations. Part of the Agent's status. Where the agent believes he is over the state space. It is a list of n belief points based on how many simulations are running at once.</p> <code>action_played</code> <code>list[int]</code> <p>Used only during simulations. Part of the Agent's status. Records what action was last played by the agent. A list of n actions played based on how many simulations are running at once.</p> Source code in <code>olfactory_navigation\\agents\\qmdp_agent.py</code> <pre><code>class QMDP_Agent(PBVI_Agent):\n    '''\n    An agent that relies on Model-Based Reinforcement Learning. It is a simplified version of the PBVI_Agent.\n    It runs the a Value Iteration solver, assuming full observability. The value function that comes out from this is therefore used to make choices.\n\n    As stated, during simulations, the agent will choose actions based on an argmax of what action has the highest matrix product of the value function with the belief vector.\n\n    ...\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment to train the agent with.\n    threshold : float, optional, default=3e-6\n        The olfactory sensitivity of the agent. Odor cues under this threshold will not be detected by the agent.\n    name : str, optional\n        A custom name to give the agent. If not provided is will be a combination of the class-name and the threshold.\n\n    Attributes\n    ---------\n    environment : Environment\n    threshold : float\n    name : str\n    model : pomdp.Model\n        The environment converted to a POMDP model using the \"from_environment\" constructor of the pomdp.Model class.\n    saved_at : str\n        The place on disk where the agent has been saved (None if not saved yet).\n    on_gpu : bool\n        Whether the agent has been sent to the gpu or not.\n    trained_at : str\n        A string timestamp of when the agent has been trained (None if not trained yet).\n    value_function : ValueFunction\n        The value function used for the agent to make decisions.\n    belief : BeliefSet\n        Used only during simulations.\n        Part of the Agent's status. Where the agent believes he is over the state space.\n        It is a list of n belief points based on how many simulations are running at once.\n    action_played : list[int]\n        Used only during simulations.\n        Part of the Agent's status. Records what action was last played by the agent.\n        A list of n actions played based on how many simulations are running at once.\n    '''\n    def train(self,\n              expansions:int,\n              initial_value_function:ValueFunction|None=None,\n              gamma:float=0.99,\n              eps:float=1e-6,\n              use_gpu:bool=False,\n              history_tracking_level:int=1,\n              force:bool=False,\n              print_progress:bool=True,\n              print_stats:bool=True\n              ) -&gt; TrainingHistory:\n        '''\n        Simplified version of the training. It consists in running the Value Iteration process.\n\n        Parameters\n        ----------\n        expansions : int\n            How many iterations to run the Value Iteration process for.\n        initial_value_function : ValueFunction, optional\n            An initial value function to start the solving process with.\n        use_gpu : bool, default=False\n            Whether to use the GPU with cupy array to accelerate solving.\n        gamma : float, default=0.99\n            The discount factor to value immediate rewards more than long term rewards.\n            The learning rate is 1/gamma.\n        eps : float, default=1e-6\n            The smallest allowed changed for the value function.\n            Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n        history_tracking_level : int, default=1\n            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n        force : bool, default=False\n            Whether to force retraining if a value function already exists for this agent.\n        print_progress : bool, default=True\n            Whether or not to print out the progress of the value iteration process.\n        print_stats : bool, default=True\n            Whether or not to print out statistics at the end of the training run.\n\n        Returns\n        -------\n        solver_history : SolverHistory\n            The history of the solving process with some plotting options.\n        '''\n        # GPU support\n        if use_gpu:\n            assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n        # Handeling the case where the agent is already trained\n        if (self.value_function is not None) and (not force):\n            raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n        else:\n            self.trained_at = None\n            self.name = '-'.join(self.name.split('-')[:-1])\n            self.value_function = None\n\n        model = self.model if not use_gpu else self.model.gpu_model\n\n        # Value Iteration solving\n        value_function, hist = vi_solver.solve(model = model,\n                                               horizon = expansions,\n                                               initial_value_function = initial_value_function,\n                                               gamma = gamma,\n                                               eps = eps,\n                                               use_gpu = use_gpu,\n                                               history_tracking_level = history_tracking_level,\n                                               print_progress = print_progress)\n\n        # Record when it was trained\n        self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n        self.name += f'-trained_{self.trained_at}'\n\n        self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n        # Print stats if requested\n        if print_stats:\n            print(hist.summary)\n\n        return hist\n</code></pre>"},{"location":"reference/agents/qmdp_agent/#olfactory_navigation.agents.qmdp_agent.QMDP_Agent.train","title":"<code>train(expansions, initial_value_function=None, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, force=False, print_progress=True, print_stats=True)</code>","text":"<p>Simplified version of the training. It consists in running the Value Iteration process.</p> <p>Parameters:</p> Name Type Description Default <code>expansions</code> <code>int</code> <p>How many iterations to run the Value Iteration process for.</p> required <code>initial_value_function</code> <code>ValueFunction</code> <p>An initial value function to start the solving process with.</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force retraining if a value function already exists for this agent.</p> <code>False</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <code>print_stats</code> <code>bool</code> <p>Whether or not to print out statistics at the end of the training run.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>solver_history</code> <code>SolverHistory</code> <p>The history of the solving process with some plotting options.</p> Source code in <code>olfactory_navigation\\agents\\qmdp_agent.py</code> <pre><code>def train(self,\n          expansions:int,\n          initial_value_function:ValueFunction|None=None,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          force:bool=False,\n          print_progress:bool=True,\n          print_stats:bool=True\n          ) -&gt; TrainingHistory:\n    '''\n    Simplified version of the training. It consists in running the Value Iteration process.\n\n    Parameters\n    ----------\n    expansions : int\n        How many iterations to run the Value Iteration process for.\n    initial_value_function : ValueFunction, optional\n        An initial value function to start the solving process with.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    force : bool, default=False\n        Whether to force retraining if a value function already exists for this agent.\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n    print_stats : bool, default=True\n        Whether or not to print out statistics at the end of the training run.\n\n    Returns\n    -------\n    solver_history : SolverHistory\n        The history of the solving process with some plotting options.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    # Handeling the case where the agent is already trained\n    if (self.value_function is not None) and (not force):\n        raise Exception('Agent has already been trained. The force parameter needs to be set to \"True\" if training should still happen')\n    else:\n        self.trained_at = None\n        self.name = '-'.join(self.name.split('-')[:-1])\n        self.value_function = None\n\n    model = self.model if not use_gpu else self.model.gpu_model\n\n    # Value Iteration solving\n    value_function, hist = vi_solver.solve(model = model,\n                                           horizon = expansions,\n                                           initial_value_function = initial_value_function,\n                                           gamma = gamma,\n                                           eps = eps,\n                                           use_gpu = use_gpu,\n                                           history_tracking_level = history_tracking_level,\n                                           print_progress = print_progress)\n\n    # Record when it was trained\n    self.trained_at = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n    self.name += f'-trained_{self.trained_at}'\n\n    self.value_function = value_function.to_cpu() if not self.on_gpu else value_function.to_gpu()\n\n    # Print stats if requested\n    if print_stats:\n        print(hist.summary)\n\n    return hist\n</code></pre>"},{"location":"reference/agents/model_based_util/","title":"model_based_util","text":""},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief","title":"<code>Belief</code>","text":"<p>A class representing a belief in the space of a given model. It is the belief to be in any combination of states: eg:     - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.</p> <p>The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which the belief applies on.</p> required <code>values</code> <code>ndarray</code> <p>A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1. If not specified, it will be set as the start probabilities of the model.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <code>values</code> <code>ndarray</code> <code>bytes_repr</code> <code>bytes</code> <p>A representation in bytes of the value of the belief</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>class Belief:\n    '''\n    A class representing a belief in the space of a given model. It is the belief to be in any combination of states:\n    eg:\n        - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.\n\n    The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'\n\n    ...\n\n    Parameters\n    ----------\n    model : pomdp.Model\n        The model on which the belief applies on.\n    values : np.ndarray, optional\n        A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1.\n        If not specified, it will be set as the start probabilities of the model.\n\n    Attributes\n    ----------\n    model : pomdp.Model\n    values : np.ndarray\n    bytes_repr : bytes\n        A representation in bytes of the value of the belief\n    '''\n    def __init__(self, model:Model, values:Union[np.ndarray,None]=None):\n        assert model is not None\n        self.model = model\n\n        if values is not None:\n            assert values.shape[0] == model.state_count, \"Belief must contain be of dimension |S|\"\n\n            xp = np if not gpu_support else cp.get_array_module(values)\n\n            prob_sum = xp.sum(values)\n            rounded_sum = xp.round(prob_sum, decimals=3)\n            assert rounded_sum == 1.0, f\"States probabilities in belief must sum to 1 (found: {prob_sum}; rounded {rounded_sum})\"\n\n            self._values = values\n        else:\n            self._values = model.start_probabilities\n\n\n    def __new__(cls, *args, **kwargs):\n        instance = super().__new__(cls)\n\n        instance._bytes_repr = None\n        instance._successors = {}\n\n        return instance\n\n\n    @property\n    def bytes_repr(self) -&gt; bytes:\n        '''\n        A representation as bytes of a belief.\n        '''\n        if self._bytes_repr is None:\n            self._bytes_repr = self.values.tobytes()\n        return self._bytes_repr\n\n\n    def __eq__(self, other: object) -&gt; bool:\n        '''\n        A way to check the equality between two belief points.\n        The byte representation of each belief point is compared.\n        '''\n        return self.bytes_repr == other.bytes_repr\n\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        '''\n        An array of the probability distribution to be in each state.\n        '''\n        return self._values\n\n\n    def update(self,\n               a:int,\n               o:int,\n               throw_error:bool=True\n               ) -&gt; 'Belief':\n        '''\n        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n        Parameters\n        ----------\n        a : int\n            The most recent action.\n        o : int\n            The most recent observation.\n        throw_error : bool, default=True\n            Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.\n\n        Returns\n        -------\n        new_belief : Belief\n            An updated belief\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        # Check if successor exists\n        succ_id = f'{a}_{o}'\n        succ = self._successors.get(succ_id)\n        if succ is not None:\n            return succ\n\n        # Computing new probabilities\n        reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]\n        new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)\n\n        # Normalization\n        probability_sum = xp.sum(new_state_probabilities)\n        if probability_sum == 0:\n            if throw_error:\n                raise ValueError(\"Impossible belief: the sum of probabilities is 0...\")\n        else:\n            new_state_probabilities /= probability_sum\n\n        # Generation of new belief from new state probabilities\n        new_belief = self.__new__(self.__class__)\n        new_belief.model = self.model\n        new_belief._values = new_state_probabilities\n\n        # Remember generated successor\n        self._successors[succ_id] = new_belief\n\n        return new_belief\n\n\n    def generate_successors(self) -&gt; list['Belief']:\n        '''\n        Function to generate a set of belief that can be reached for each actions and observations available in the model.\n\n        Returns\n        -------\n        successor_beliefs : list[Belief]\n            The successor beliefs.\n        '''\n        successor_beliefs = []\n        for a in self.model.actions:\n            for o in self.model.observations:\n                b_ao = self.update(a,o)\n                successor_beliefs.append(b_ao)\n\n        return successor_beliefs\n\n\n    def random_state(self) -&gt; int:\n        '''\n        Returns a random state of the model weighted by the belief probabily.\n\n        Returns\n        -------\n        rand_s : int\n            A random state.\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])\n        return rand_s\n\n\n    @property\n    def entropy(self) -&gt; float:\n        '''\n        The entropy of the belief point\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        return float(entropy(self._values) if xp == np else cupy_entropy(self._values))\n\n\n    def plot(self, size:int=5) -&gt; None:\n        '''\n        Function to plot a heatmap of the belief distribution if the belief is of a grid model.\n\n        Parameters\n        ----------\n        size : int, default=5\n            The scale of the plot.\n        '''\n        # Plot setup\n        plt.figure(figsize=(size*1.2,size))\n\n        model = self.model.cpu_model\n\n        # Ticks\n        dimensions = model.state_grid.shape\n        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n        plt.xticks(x_ticks)\n        plt.yticks(y_ticks)\n\n        # Title\n        plt.title(f'Belief (probability distribution over states)')\n\n        # Actual plot\n        belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)\n        grid_values = belief_values[model.state_grid]\n        plt.imshow(grid_values,cmap='Blues')\n        plt.colorbar()\n        plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.bytes_repr","title":"<code>bytes_repr: bytes</code>  <code>property</code>","text":"<p>A representation as bytes of a belief.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.entropy","title":"<code>entropy: float</code>  <code>property</code>","text":"<p>The entropy of the belief point</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.values","title":"<code>values: np.ndarray</code>  <code>property</code>","text":"<p>An array of the probability distribution to be in each state.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.__eq__","title":"<code>__eq__(other)</code>","text":"<p>A way to check the equality between two belief points. The byte representation of each belief point is compared.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    '''\n    A way to check the equality between two belief points.\n    The byte representation of each belief point is compared.\n    '''\n    return self.bytes_repr == other.bytes_repr\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.generate_successors","title":"<code>generate_successors()</code>","text":"<p>Function to generate a set of belief that can be reached for each actions and observations available in the model.</p> <p>Returns:</p> Name Type Description <code>successor_beliefs</code> <code>list[Belief]</code> <p>The successor beliefs.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def generate_successors(self) -&gt; list['Belief']:\n    '''\n    Function to generate a set of belief that can be reached for each actions and observations available in the model.\n\n    Returns\n    -------\n    successor_beliefs : list[Belief]\n        The successor beliefs.\n    '''\n    successor_beliefs = []\n    for a in self.model.actions:\n        for o in self.model.observations:\n            b_ao = self.update(a,o)\n            successor_beliefs.append(b_ao)\n\n    return successor_beliefs\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.plot","title":"<code>plot(size=5)</code>","text":"<p>Function to plot a heatmap of the belief distribution if the belief is of a grid model.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The scale of the plot.</p> <code>5</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def plot(self, size:int=5) -&gt; None:\n    '''\n    Function to plot a heatmap of the belief distribution if the belief is of a grid model.\n\n    Parameters\n    ----------\n    size : int, default=5\n        The scale of the plot.\n    '''\n    # Plot setup\n    plt.figure(figsize=(size*1.2,size))\n\n    model = self.model.cpu_model\n\n    # Ticks\n    dimensions = model.state_grid.shape\n    x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n    y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n    plt.xticks(x_ticks)\n    plt.yticks(y_ticks)\n\n    # Title\n    plt.title(f'Belief (probability distribution over states)')\n\n    # Actual plot\n    belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)\n    grid_values = belief_values[model.state_grid]\n    plt.imshow(grid_values,cmap='Blues')\n    plt.colorbar()\n    plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.random_state","title":"<code>random_state()</code>","text":"<p>Returns a random state of the model weighted by the belief probabily.</p> <p>Returns:</p> Name Type Description <code>rand_s</code> <code>int</code> <p>A random state.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def random_state(self) -&gt; int:\n    '''\n    Returns a random state of the model weighted by the belief probabily.\n\n    Returns\n    -------\n    rand_s : int\n        A random state.\n    '''\n    xp = np if not gpu_support else cp.get_array_module(self._values)\n\n    rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])\n    return rand_s\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.Belief.update","title":"<code>update(a, o, throw_error=True)</code>","text":"<p>Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>int</code> <p>The most recent action.</p> required <code>o</code> <code>int</code> <p>The most recent observation.</p> required <code>throw_error</code> <code>bool</code> <p>Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>new_belief</code> <code>Belief</code> <p>An updated belief</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def update(self,\n           a:int,\n           o:int,\n           throw_error:bool=True\n           ) -&gt; 'Belief':\n    '''\n    Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n    Parameters\n    ----------\n    a : int\n        The most recent action.\n    o : int\n        The most recent observation.\n    throw_error : bool, default=True\n        Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.\n\n    Returns\n    -------\n    new_belief : Belief\n        An updated belief\n    '''\n    xp = np if not gpu_support else cp.get_array_module(self._values)\n\n    # Check if successor exists\n    succ_id = f'{a}_{o}'\n    succ = self._successors.get(succ_id)\n    if succ is not None:\n        return succ\n\n    # Computing new probabilities\n    reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]\n    new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)\n\n    # Normalization\n    probability_sum = xp.sum(new_state_probabilities)\n    if probability_sum == 0:\n        if throw_error:\n            raise ValueError(\"Impossible belief: the sum of probabilities is 0...\")\n    else:\n        new_state_probabilities /= probability_sum\n\n    # Generation of new belief from new state probabilities\n    new_belief = self.__new__(self.__class__)\n    new_belief.model = self.model\n    new_belief._values = new_state_probabilities\n\n    # Remember generated successor\n    self._successors[succ_id] = new_belief\n\n    return new_belief\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet","title":"<code>BeliefSet</code>","text":"<p>Class to represent a set of beliefs with regard to a POMDP model. It has the purpose to store the beliefs in a numpy array format and be able to conver it to a list of Belief class objects.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which the beliefs apply.</p> required <code>beliefs</code> <code>list[Belief] | ndarray</code> <p>The actual set of beliefs.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <code>belief_array</code> <code>ndarray</code> <p>A 2D array of shape N x S of N belief vectors.</p> <code>belief_list</code> <code>list[Belief]</code> <p>A list of N Belief object.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>class BeliefSet:\n    '''\n    Class to represent a set of beliefs with regard to a POMDP model.\n    It has the purpose to store the beliefs in a numpy array format and be able to conver it to a list of Belief class objects.\n\n    ...\n\n    Parameters\n    ----------\n    model : pomdp.Model\n        The model on which the beliefs apply.\n    beliefs : list[Belief] | np.ndarray\n        The actual set of beliefs.\n\n    Attributes\n    ----------\n    model : pomdp.Model\n    belief_array : np.ndarray\n        A 2D array of shape N x S of N belief vectors.\n    belief_list : list[Belief]\n        A list of N Belief object.\n    '''\n    def __init__(self, model:Model, beliefs:Union[list[Belief],np.ndarray]) -&gt; None:\n        self.model = model\n\n        self._belief_list = None\n        self._belief_array = None\n        self._uniqueness_dict = None\n\n        self.is_on_gpu = False\n\n        if isinstance(beliefs, list):\n            assert all(len(b.values) == model.state_count for b in beliefs), f\"Beliefs in belief list provided dont all have shape ({model.state_count},)\"\n            self._belief_list = beliefs\n\n            # Check if on gpu and make sure all beliefs are also on the gpu\n            if (len(beliefs) &gt; 0) and gpu_support and cp.get_array_module(beliefs[0].values) == cp:\n                assert all(cp.get_array_module(b.values) == cp for b in beliefs), \"Either all or none of the alpha vectors should be on the GPU, not just some.\"\n                self.is_on_gpu = True\n        else:\n            assert beliefs.shape[1] == model.state_count, f\"Belief array provided doesnt have the right shape (expected (-,{model.state_count}), received {beliefs.shape})\"\n\n            self._belief_array = beliefs\n\n            # Check if array is on gpu\n            if gpu_support and cp.get_array_module(beliefs) == cp:\n                self.is_on_gpu = True\n\n\n    @property\n    def belief_array(self) -&gt; np.ndarray:\n        '''\n        A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._belief_array is None:\n            self._belief_array = xp.array([b.values for b in self._belief_list])\n        return self._belief_array\n\n\n    @property\n    def belief_list(self) -&gt; list[Belief]:\n        '''\n        A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.\n        '''\n        if self._belief_list is None:\n            self._belief_list = [Belief(self.model, belief_vector) for belief_vector in self._belief_array]\n        return self._belief_list\n\n\n    def generate_all_successors(self) -&gt; 'BeliefSet':\n        '''\n        Function to generate the successors beliefs of all the beliefs in the belief set.\n\n        Returns\n        -------\n        all_successors : BeliefSet\n            All successors of all beliefs in the belief set.\n        '''\n        all_successors = []\n        for belief in self.belief_list:\n            all_successors.extend(belief.generate_successors())\n        return BeliefSet(self.model, all_successors)\n\n\n    def update(self,\n               actions:list|np.ndarray,\n               observations:list|np.ndarray,\n               throw_error:bool=True\n               ) -&gt; 'BeliefSet':\n        '''\n        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n        Parameters\n        ----------\n        actions : list or np.ndarray\n            The most recent played actions.\n        observations : list or np.ndarray\n            The most recent received observations.\n        throw_error : bool, default=True\n            Whether the throw an error when attempting to generate impossible beliefs.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            An set of updated beliefs.\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        # Ensuring we are dealing we are dealing with ndarrays\n        observations = xp.array(observations)\n        actions = xp.array(actions)\n\n        # Computing reachable probabilities and states\n        reachable_probabilities = (self.model.reachable_transitional_observation_table[:, actions, observations, :] * self.belief_array.T[:,:,None])\n        reachable_state_per_actions = self.model.reachable_states[:, actions, :]\n\n        # Computing new probabilities\n        flatten_offset = xp.arange(len(observations))[:,None] * self.model.state_count\n        flat_shape = (len(observations), (self.model.state_count * self.model.reachable_state_count))\n\n        a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape)\n        w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape)\n\n        a_offs = a + flatten_offset\n        new_probabilities = xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*self.model.state_count).reshape((-1,self.model.state_count))\n\n        # Normalization\n        probability_sum = xp.sum(new_probabilities, axis=1)\n        if xp.any(probability_sum == 0.0) and throw_error:\n            raise ValueError('One or more belief is impossible, (ie the sum of the probability distribution is 0)')\n        non_zero_mask = probability_sum != 0\n        new_probabilities[non_zero_mask] /= probability_sum[non_zero_mask,None]\n\n        return BeliefSet(self.model, new_probabilities)\n\n\n    @property\n    def unique_belief_dict(self) -&gt; dict:\n        '''\n        A dictionary of unique belief points with the keys being the byte representation of these belief points.\n        '''\n        if self._uniqueness_dict is None:\n            self._uniqueness_dict = {belief.bytes_repr: belief for belief in self.belief_list}\n        return self._uniqueness_dict\n\n\n    def union(self, other_belief_set:'BeliefSet') -&gt; 'BeliefSet':\n        '''\n        Function to make the union between two belief set objects.\n\n        Parameters\n        ----------\n        other_belief_set : BeliefSet\n            The other belief set to make the union with\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            A new, combined, belief set\n        '''\n        # Deduplication\n        combined_uniqueness_dict = self.unique_belief_dict | other_belief_set.unique_belief_dict\n\n        # Generation of new set\n        new_belief_set = BeliefSet(self.model, list(combined_uniqueness_dict.values()))\n        new_belief_set._uniqueness_dict = combined_uniqueness_dict\n\n        return new_belief_set\n\n\n    def __len__(self) -&gt; int:\n        return len(self._belief_list) if self._belief_list is not None else self._belief_array.shape[0]\n\n\n    @property\n    def entropies(self) -&gt; np.ndarray:\n        '''\n        An array of the entropies of the belief points.\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self.belief_array)\n\n        return entropy(self.belief_array, axis=1) if xp == np else cupy_entropy(self.belief_array, axis=1)\n\n\n    def to_gpu(self) -&gt; 'BeliefSet':\n        '''\n        Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.\n\n        Returns\n        -------\n        gpu_belief_set : BeliefSet\n            A new belief set with array on GPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        gpu_model = self.model.gpu_model\n\n        gpu_belief_set = None\n        if self._belief_array is not None:\n            gpu_belief_array = cp.array(self._belief_array)\n            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)\n        else:\n            gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]\n            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)\n\n        return gpu_belief_set\n\n\n    def to_cpu(self) -&gt; 'BeliefSet':\n        '''\n        Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.\n\n        Returns\n        -------\n        cpu_belief_set : BeliefSet\n            A new belief set with array on CPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        cpu_model = self.model.cpu_model\n\n        cpu_belief_set = None\n        if self._belief_array is not None:\n            cpu_belief_array = cp.asnumpy(self._belief_array)\n            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)\n\n        else:\n            cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]\n            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)\n\n        return cpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.belief_array","title":"<code>belief_array: np.ndarray</code>  <code>property</code>","text":"<p>A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.belief_list","title":"<code>belief_list: list[Belief]</code>  <code>property</code>","text":"<p>A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.entropies","title":"<code>entropies: np.ndarray</code>  <code>property</code>","text":"<p>An array of the entropies of the belief points.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.unique_belief_dict","title":"<code>unique_belief_dict: dict</code>  <code>property</code>","text":"<p>A dictionary of unique belief points with the keys being the byte representation of these belief points.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.generate_all_successors","title":"<code>generate_all_successors()</code>","text":"<p>Function to generate the successors beliefs of all the beliefs in the belief set.</p> <p>Returns:</p> Name Type Description <code>all_successors</code> <code>BeliefSet</code> <p>All successors of all beliefs in the belief set.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def generate_all_successors(self) -&gt; 'BeliefSet':\n    '''\n    Function to generate the successors beliefs of all the beliefs in the belief set.\n\n    Returns\n    -------\n    all_successors : BeliefSet\n        All successors of all beliefs in the belief set.\n    '''\n    all_successors = []\n    for belief in self.belief_list:\n        all_successors.extend(belief.generate_successors())\n    return BeliefSet(self.model, all_successors)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.</p> <p>Returns:</p> Name Type Description <code>cpu_belief_set</code> <code>BeliefSet</code> <p>A new belief set with array on CPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def to_cpu(self) -&gt; 'BeliefSet':\n    '''\n    Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.\n\n    Returns\n    -------\n    cpu_belief_set : BeliefSet\n        A new belief set with array on CPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    cpu_model = self.model.cpu_model\n\n    cpu_belief_set = None\n    if self._belief_array is not None:\n        cpu_belief_array = cp.asnumpy(self._belief_array)\n        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)\n\n    else:\n        cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]\n        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)\n\n    return cpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.</p> <p>Returns:</p> Name Type Description <code>gpu_belief_set</code> <code>BeliefSet</code> <p>A new belief set with array on GPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def to_gpu(self) -&gt; 'BeliefSet':\n    '''\n    Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.\n\n    Returns\n    -------\n    gpu_belief_set : BeliefSet\n        A new belief set with array on GPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    gpu_model = self.model.gpu_model\n\n    gpu_belief_set = None\n    if self._belief_array is not None:\n        gpu_belief_array = cp.array(self._belief_array)\n        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)\n    else:\n        gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]\n        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)\n\n    return gpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.union","title":"<code>union(other_belief_set)</code>","text":"<p>Function to make the union between two belief set objects.</p> <p>Parameters:</p> Name Type Description Default <code>other_belief_set</code> <code>BeliefSet</code> <p>The other belief set to make the union with</p> required <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>A new, combined, belief set</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def union(self, other_belief_set:'BeliefSet') -&gt; 'BeliefSet':\n    '''\n    Function to make the union between two belief set objects.\n\n    Parameters\n    ----------\n    other_belief_set : BeliefSet\n        The other belief set to make the union with\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        A new, combined, belief set\n    '''\n    # Deduplication\n    combined_uniqueness_dict = self.unique_belief_dict | other_belief_set.unique_belief_dict\n\n    # Generation of new set\n    new_belief_set = BeliefSet(self.model, list(combined_uniqueness_dict.values()))\n    new_belief_set._uniqueness_dict = combined_uniqueness_dict\n\n    return new_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.BeliefSet.update","title":"<code>update(actions, observations, throw_error=True)</code>","text":"<p>Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>list or ndarray</code> <p>The most recent played actions.</p> required <code>observations</code> <code>list or ndarray</code> <p>The most recent received observations.</p> required <code>throw_error</code> <code>bool</code> <p>Whether the throw an error when attempting to generate impossible beliefs.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>An set of updated beliefs.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def update(self,\n           actions:list|np.ndarray,\n           observations:list|np.ndarray,\n           throw_error:bool=True\n           ) -&gt; 'BeliefSet':\n    '''\n    Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n    Parameters\n    ----------\n    actions : list or np.ndarray\n        The most recent played actions.\n    observations : list or np.ndarray\n        The most recent received observations.\n    throw_error : bool, default=True\n        Whether the throw an error when attempting to generate impossible beliefs.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        An set of updated beliefs.\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    # Ensuring we are dealing we are dealing with ndarrays\n    observations = xp.array(observations)\n    actions = xp.array(actions)\n\n    # Computing reachable probabilities and states\n    reachable_probabilities = (self.model.reachable_transitional_observation_table[:, actions, observations, :] * self.belief_array.T[:,:,None])\n    reachable_state_per_actions = self.model.reachable_states[:, actions, :]\n\n    # Computing new probabilities\n    flatten_offset = xp.arange(len(observations))[:,None] * self.model.state_count\n    flat_shape = (len(observations), (self.model.state_count * self.model.reachable_state_count))\n\n    a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape)\n    w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape)\n\n    a_offs = a + flatten_offset\n    new_probabilities = xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*self.model.state_count).reshape((-1,self.model.state_count))\n\n    # Normalization\n    probability_sum = xp.sum(new_probabilities, axis=1)\n    if xp.any(probability_sum == 0.0) and throw_error:\n        raise ValueError('One or more belief is impossible, (ie the sum of the probability distribution is 0)')\n    non_zero_mask = probability_sum != 0\n    new_probabilities[non_zero_mask] /= probability_sum[non_zero_mask,None]\n\n    return BeliefSet(self.model, new_probabilities)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction","title":"<code>ValueFunction</code>","text":"<p>Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model the value function is associated with.</p> required <code>alpha_vectors</code> <code>list[AlphaVector] or ndarray</code> <p>The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.</p> <code>[]</code> <code>action_list</code> <code>list[int] or ndarray</code> <p>The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The model the value function is associated with.</p> <code>alpha_vector_list</code> <code>list[AlphaVector]</code> <code>alpha_vector_array</code> <code>ndarray</code> <code>actions</code> <code>ndarray</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>class ValueFunction:\n    '''\n    Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.\n\n    ...\n\n    Parameters\n    ----------\n    model : mdp.Model\n        The model the value function is associated with.\n    alpha_vectors : list[AlphaVector] or np.ndarray, optional\n        The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.\n    action_list : list[int] or np.ndarray, optional\n        The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.\n\n    Attributes\n    ----------\n    model : mdp.Model\n        The model the value function is associated with.\n    alpha_vector_list : list[AlphaVector]\n    alpha_vector_array : np.ndarray\n    actions : np.ndarray\n    '''\n    def __init__(self, model:Model, alpha_vectors:Union[list[AlphaVector], np.ndarray]=[], action_list:Union[list[int], np.ndarray]=[]):\n        self.model = model\n\n        self._vector_list = None\n        self._vector_array = None\n        self._actions = None\n\n        self.is_on_gpu = False\n\n        # List of alpha vectors\n        if isinstance(alpha_vectors, list):\n            assert all(v.values.shape[0] == model.state_count for v in alpha_vectors), f\"Some or all alpha vectors in the list provided dont have the right size, they should be of shape: {model.state_count}\"\n            self._vector_list = alpha_vectors\n\n            # Check if on gpu and make sure all vectors are also on the gpu\n            if (len(alpha_vectors) &gt; 0) and gpu_support and cp.get_array_module(alpha_vectors[0].values) == cp:\n                assert all(cp.get_array_module(v.values) == cp for v in alpha_vectors), \"Either all or none of the alpha vectors should be on the GPU, not just some.\"\n                self.is_on_gpu = True\n\n        # As numpy array\n        else:\n            av_shape = alpha_vectors.shape\n            exp_shape = (len(action_list), model.state_count)\n            assert av_shape == exp_shape, f\"Alpha vector array does not have the right shape (received: {av_shape}; expected: {exp_shape})\"\n\n            self._vector_list = []\n            for alpha_vect, action in zip(alpha_vectors, action_list):\n                self._vector_list.append(AlphaVector(alpha_vect, action))\n\n            # Check if array is on gpu\n            if gpu_support and cp.get_array_module(alpha_vectors) == cp:\n                self.is_on_gpu = True\n\n        # Deduplication\n        self._uniqueness_dict = {alpha_vector.values.tobytes(): alpha_vector for alpha_vector in self._vector_list}\n        self._vector_list = list(self._uniqueness_dict.values())\n\n        self._pruning_level = 1\n\n\n    @property\n    def alpha_vector_list(self) -&gt; list[AlphaVector]:\n        '''\n        A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.\n        '''\n        if self._vector_list is None:\n            self._vector_list = []\n            for alpha_vect, action in zip(self._vector_array, self._actions):\n                self._vector_list.append(AlphaVector(alpha_vect, action))\n        return self._vector_list\n\n\n    @property\n    def alpha_vector_array(self) -&gt; np.ndarray:\n        '''\n        A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)\n        If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._vector_array is None:\n            self._vector_array = xp.array([v.values for v in self._vector_list])\n            self._actions = xp.array([v.action for v in self._vector_list])\n        return self._vector_array\n\n\n    @property\n    def actions(self) -&gt; np.ndarray:\n        '''\n        A list of N actions corresponding to the N alpha vectors making up the value function.\n        If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._actions is None:\n            self._vector_array = xp.array(self._vector_list)\n            self._actions = xp.array([v.action for v in self._vector_list])\n        return self._actions\n\n\n    def __len__(self) -&gt; int:\n        return len(self._vector_list) if self._vector_list is not None else self._vector_array.shape[0]\n\n\n    def __add__(self, other_value_function:'Model') -&gt; 'Model':\n        # combined_dict = {**self._uniqueness_dict, **other_value_function._uniqueness_dict}\n        combined_dict = {}\n        combined_dict.update(self._uniqueness_dict)\n        combined_dict.update(other_value_function._uniqueness_dict)\n\n        # Instantiation of the new value function\n        new_value_function = super().__new__(self.__class__)\n        new_value_function.model = self.model\n        new_value_function.is_on_gpu = self.is_on_gpu\n\n        new_value_function._vector_list = list(combined_dict.values())\n        new_value_function._uniqueness_dict = combined_dict\n        new_value_function._pruning_level = 1\n\n        new_value_function._vector_array = None\n        new_value_function._actions = None\n\n        return new_value_function\n\n\n    def append(self, alpha_vector:AlphaVector) -&gt; None:\n        '''\n        Function to add an alpha vector to the value function.\n\n        Parameters\n        ----------\n        alpha_vector : AlphaVector\n            The alpha vector to be added to the value function.\n        '''\n        # Make sure size is correct\n        assert alpha_vector.values.shape[0] == self.model.state_count, f\"Vector to add to value function doesn't have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})\"\n\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n        assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f\"Vector is{' not' if self.is_on_gpu else ''} on GPU while value function is{'' if self.is_on_gpu else ' not'}.\"\n\n        if self._vector_array is not None:\n            self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)\n            self._actions = xp.append(self._actions, alpha_vector.action)\n\n        if self._vector_list is not None:\n            self._vector_list.append(alpha_vector)\n\n\n    def extend(self, other_value_function:'Model') -&gt; None:\n        '''\n        Function to add another value function is place.\n        Effectively, it performs the union of the two sets of alpha vectors.\n\n        Parameters\n        ----------\n        other_value_function : ValueFunction\n            The other side of the union.\n        '''\n        self._uniqueness_dict.update(other_value_function._uniqueness_dict)\n        self._vector_list = list(self._uniqueness_dict.values())\n\n        self._vector_array = None\n        self._actions = None\n\n        self._pruning_level = 1\n\n\n    def to_gpu(self) -&gt; 'ValueFunction':\n        '''\n        Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.\n\n        Returns\n        -------\n        gpu_value_function : ValueFunction\n            A new value function with arrays on GPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        gpu_model = self.model.gpu_model\n\n        gpu_value_function = None\n        if self._vector_list is not None:\n            gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]\n            gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)\n\n        else:\n            gpu_vector_array = cp.array(self._vector_array)\n            gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)\n            gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)\n\n        return gpu_value_function\n\n\n    def to_cpu(self) -&gt; 'ValueFunction':\n        '''\n        Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.\n\n        Returns\n        -------\n        cpu_value_function : ValueFunction\n            A new value function with arrays on CPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        cpu_model = self.model.cpu_model\n\n        cpu_value_function = None\n        if self._vector_list is not None:\n            cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]\n            cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)\n\n        else:\n            cpu_vector_array = cp.asnumpy(self._vector_array)\n            cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)\n            cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)\n\n        return cpu_value_function\n\n\n    def prune(self, level:int=1) -&gt; None:\n        '''\n        Function pruning the set of alpha vectors composing the value function.\n        The pruning is as thorough as the level:\n            - 2: 1+ Check of absolute domination (check if dominated at each state).\n            - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.\n\n        Note that the higher the level, the heavier the time impact will be.\n\n        Parameters\n        ----------\n        level : int, default=1\n            Between 0 and 3, how thorough the alpha vector pruning should be.\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        # Level 1 or under\n        if level &lt; self._pruning_level or level &gt; 3:\n            log('Attempting to prune a value function to a level already reached. Returning \\'self\\'')\n            return\n\n        # Level 2 pruning: Check for absolute domination\n        if level &gt;= 2 and self._pruning_level &lt; 2:\n            non_dominated_vector_indices = []\n\n            for i, v in enumerate(self.alpha_vector_array):\n                is_dom_by = xp.all(self.alpha_vector_array &gt;= v, axis=1)\n                if len(xp.where(is_dom_by)[0]) == 1:\n                    non_dominated_vector_indices.append(i)\n\n            self._vector_array = self._vector_array[non_dominated_vector_indices]\n            self._actions = self._actions[non_dominated_vector_indices]\n\n        # Level 3 pruning: LP to check for more complex domination\n        if level &gt;= 3:\n            assert ilp_support, \"ILP support not enabled...\"\n\n            pruned_alpha_set = pruned_alpha_set.to_cpu()\n\n            alpha_set = pruned_alpha_set.alpha_vector_array\n            non_dominated_vector_indices = []\n\n            for i, alpha_vect in enumerate(alpha_set):\n                other_alphas = alpha_set[:i] + alpha_set[(i+1):]\n\n                # Objective function\n                c = np.concatenate([np.array([1]), -1*alpha_vect])\n\n                # Alpha vector contraints\n                other_count = len(other_alphas)\n                A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]\n                alpha_constraints = LinearConstraint(A, 0, np.inf)\n\n                # Constraints that sum of beliefs is 1\n                belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)\n\n                # Solve problem\n                res = milp(c=c, constraints=[alpha_constraints, belief_constraint])\n\n                # Check if dominated\n                is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0\n                if is_dominated:\n                    print(alpha_vect)\n                    print(' -&gt; Dominated\\n')\n                else:\n                    non_dominated_vector_indices.append(i)\n\n            self._vector_array = self._vector_array[non_dominated_vector_indices]\n            self._actions = self._actions[non_dominated_vector_indices]\n\n        # Update the tracked pruned level so far\n        self._pruning_level = level\n\n\n    def evaluate_at(self, belief:Belief|BeliefSet) -&gt; tuple[float|np.ndarray, int|np.ndarray]:\n        '''\n        Function to evaluate the value function at a belief point or at a set of belief points.\n        It returns a value and the associated action.\n\n        Parameters\n        ----------\n        belief : Belief or BeliefSet\n\n        Returns\n        -------\n        value : float or np.ndarray\n            The largest value associated with the belief point(s)\n        action : int or np.ndarray\n            The action(s) associated with the vector having the highest values at the belief point(s).\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        best_value = None\n        best_action = None\n\n        if isinstance(belief, Belief):\n            # Computing values\n            belief_values = xp.dot(self.alpha_vector_array, belief.values)\n\n            # Selecting best vectors\n            best_vector = xp.argmax(belief_values)\n\n            # From best vector, compute the best value and action\n            best_value = float(belief_values[best_vector])\n            best_action = int(self.actions[best_vector])\n        else:\n            # Computing values\n            belief_values = xp.matmul(belief.values if isinstance(belief, Belief) else belief.belief_array, self.alpha_vector_array.T)\n\n            # Retrieving the top vectors according to the value function\n            best_vectors = xp.argmax(belief_values, axis=1)\n\n            # Retrieving the values and actions associated with the vectors chosen\n            best_value = belief_values[xp.arange(belief_values.shape[0]), best_vectors]\n            best_action = self.actions[best_vectors]\n\n        return (best_value, best_action)\n\n\n    def save(self,\n             folder:str='./ValueFunctions',\n             file_name:Union[str,None]=None\n             ) -&gt; None:\n        '''\n        Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.\n        If no file_name is provided, it be saved as '&lt;current_timestamp&gt;_value_function.csv'.\n\n        Parameters\n        ----------\n        folder : str, default='./ValueFunctions'\n            The path at which the npy file will be saved.\n        file_name : str, default='&lt;current_timestamp&gt;_value_function.npy'\n            The file name used to save in.\n        '''\n        if self.is_on_gpu:\n            self.to_cpu().save(path=folder, file_name=file_name)\n            return\n\n        # Handle file_name\n        if file_name is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_name = timestamp + '_value_function.npy'\n\n        # Make sure that .csv is in the file name\n        if '.npy' not in file_name:\n            file_name += '.npy'\n\n        # Getting array\n        av_array = np.hstack([self.actions[:,None], self.alpha_vector_array])\n\n        np.save(folder + '/' + file_name, av_array)\n\n\n    @classmethod\n    def load(cls,\n            file:str,\n            model:Model\n            ) -&gt; 'ValueFunction':\n        '''\n        Function to load the value function from a csv file.\n\n        Parameters\n        ----------\n        file : str\n            The path and file_name of the value function to be loaded.\n        model : mdp.Model\n            The model the value function is linked to.\n\n        Returns\n        -------\n        loaded_value_function : ValueFunction\n            The loaded value function.\n        '''\n        av_array = np.load(file)\n\n        return ValueFunction(model, alpha_vectors=av_array[:,1:], action_list=av_array[:,0].astype(int))\n\n\n    def plot(self,\n             as_grid:bool=False,\n             size:int=5,\n             belief_set:np.ndarray=None\n             ) -&gt; None:\n        '''\n        Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.\n        If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.\n\n        Parameters\n        ----------\n        as_grid : bool, default=False\n            Forces the plot to be plot as a grid.\n        size : int, default=5\n            The actual plot scale.\n        belief_set : np.ndarray, optional\n            A set of belief to plot the belief points that were explored.\n        '''\n        assert len(self) &gt; 0, \"Value function is empty, plotting is impossible...\"\n\n        # If on GPU, convert to CPU and plot that one\n        if self.is_on_gpu:\n            print('[Warning] Value function on GPU, converting to numpy before plotting...')\n            cpu_value_function = self.to_cpu()\n            cpu_value_function.plot(as_grid, size, belief_set)\n            return\n\n        func = None\n        if as_grid:\n            func = self._plot_grid\n        elif self.model.state_count == 2:\n            func = self._plot_2D\n        elif self.model.state_count == 3:\n            func = self._plot_3D\n        else:\n            print('[Warning] \\'as_grid\\' parameter set to False but state count is &gt;3 so it will be plotted as a grid')\n            func = self._plot_grid\n\n        func(size, belief_set)\n\n\n    def _plot_2D(self, size, belief_set=None):\n        x = np.linspace(0, 1, 100)\n\n        plt.figure(figsize=(int(size*1.5),size))\n        grid_spec = {'height_ratios': ([1] if belief_set is None else [19,1])}\n        _, ax = plt.subplots((2 if belief_set is not None else 1),1,sharex=True,gridspec_kw=grid_spec)\n\n        # Vector plotting\n        alpha_vects = self.alpha_vector_array\n\n        m = alpha_vects[:,1] - alpha_vects[:,0] # type: ignore\n        m = m.reshape(m.shape[0],1)\n\n        x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)\n        y = (m*x) + alpha_vects[:,0].reshape(m.shape[0],1)\n\n        ax0 = ax[0] if belief_set is not None else ax\n        for i, alpha in enumerate(self.alpha_vector_list):\n            ax0.plot(x[i,:], y[i,:], color=COLOR_LIST[alpha.action]['id']) # type: ignore\n\n        # Set title\n        title = 'Value function' + ('' if belief_set is None else ' and explored belief points')\n        ax0.set_title(title)\n\n        # X-axis setting\n        ticks = [0,0.25,0.5,0.75,1]\n        x_ticks = [str(t) for t in ticks]\n        x_ticks[0] = self.model.state_labels[0]\n        x_ticks[-1] = self.model.state_labels[1]\n\n        ax0.set_xticks(ticks, x_ticks) # type: ignore\n\n        # Action legend\n        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[a]['id']) for a in self.model.actions]\n        ax0.legend(proxy, self.model.action_labels, title='Actions') # type: ignore\n\n        # Belief plotting\n        if belief_set is not None:\n            beliefs_x = belief_set.belief_array[:,1]\n            ax[1].scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c='red')\n            ax[1].get_yaxis().set_visible(False)\n            ax[1].axhline(0, color='black')\n            ax[1].set_xlabel('Belief space')\n        else:\n            ax0.set_xlabel('Belief space')\n\n        # Axis labels\n        ax0.set_ylabel('V(b)')\n\n\n    def _plot_3D(self, size, belief_set=None):\n\n        def get_alpha_vect_z(xx, yy, alpha_vect):\n            x0, y0, z0 = [0, 0, alpha_vect[0]]\n            x1, y1, z1 = [1, 0, alpha_vect[1]]\n            x2, y2, z2 = [0, 1, alpha_vect[2]]\n\n            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]\n            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]\n\n            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]\n\n            point  = np.array([0, 0, alpha_vect[0]])\n            normal = np.array(u_cross_v)\n\n            d = -point.dot(normal)\n\n            z = (-normal[0] * xx - normal[1] * yy - d) * 1. / normal[2]\n\n            return z\n\n        def get_plane_gradient(alpha_vect):\n\n            x0, y0, z0 = [0, 0, alpha_vect[0]]\n            x1, y1, z1 = [1, 0, alpha_vect[1]]\n            x2, y2, z2 = [0, 1, alpha_vect[2]]\n\n            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]\n            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]\n\n            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]\n\n            normal_vector = np.array(u_cross_v)\n            normal_vector_norm = float(np.linalg.norm(normal_vector))\n            normal_vector = np.divide(normal_vector, normal_vector_norm)\n            normal_vector[2] = 0\n\n            return np.linalg.norm(normal_vector)\n\n        # Actual plotting\n        x = np.linspace(0, 1, 1000)\n        y = np.linspace(0, 1, 1000)\n\n        xx, yy = np.meshgrid(x, y)\n\n        max_z = np.zeros((xx.shape[0], yy.shape[0]))\n        best_a = (np.zeros((xx.shape[0], yy.shape[0])))\n        plane = (np.zeros((xx.shape[0], yy.shape[0])))\n        gradients = (np.zeros((xx.shape[0], yy.shape[0])))\n\n        for alpha in self.alpha_vector_list:\n\n            z = get_alpha_vect_z(xx, yy, alpha.values)\n\n            # Action array update\n            new_a_mask = np.argmax(np.array([max_z, z]), axis=0)\n\n            best_a[new_a_mask == 1] = alpha.action\n\n            plane[new_a_mask == 1] = random.randrange(100)\n\n            alpha_gradient = get_plane_gradient(alpha.values)\n            gradients[new_a_mask == 1] = alpha_gradient\n\n            # Max z update\n            max_z = np.max(np.array([max_z, z]), axis=0)\n\n        for x_i, x_val in enumerate(x):\n            for y_i, y_val in enumerate(y):\n                if (x_val+y_val) &gt; 1:\n                    max_z[x_i, y_i] = np.nan\n                    plane[x_i, y_i] = np.nan\n                    gradients[x_i, y_i] = np.nan\n                    best_a[x_i, y_i] = np.nan\n\n        belief_points = None\n        if belief_set is not None:\n            belief_points = belief_set.belief_array[:,1:]\n\n        fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(size*4,size*3.5), sharex=True, sharey=True)\n\n        # Set ticks\n        ticks = [0,0.25,0.5,0.75,1]\n        x_ticks = [str(t) for t in ticks]\n        x_ticks[0] = self.model.state_labels[0]\n        x_ticks[-1] = self.model.state_labels[1]\n\n        y_ticks = [str(t) for t in ticks]\n        y_ticks[0] = self.model.state_labels[0]\n        y_ticks[-1] = self.model.state_labels[2]\n\n        plt.setp([ax1,ax2,ax3,ax4], xticks=ticks, xticklabels=x_ticks, yticks=ticks, yticklabels=y_ticks)\n\n        # Value function ax\n        ax1.set_title(\"Value function\")\n        ax1_plot = ax1.contourf(x, y, max_z, 100, cmap=\"viridis\")\n        plt.colorbar(ax1_plot, ax=ax1)\n\n        # Alpha planes ax\n        ax2.set_title(\"Alpha planes\")\n        ax2_plot = ax2.contourf(x, y, plane, 100, cmap=\"viridis\")\n        plt.colorbar(ax2_plot, ax=ax2)\n\n        # Gradient of planes ax\n        ax3.set_title(\"Gradients of planes\")\n        ax3_plot = ax3.contourf(x, y, gradients, 100, cmap=\"Blues\")\n        plt.colorbar(ax3_plot, ax=ax3)\n\n        # Action policy ax\n        ax4.set_title(\"Action policy\")\n        ax4.contourf(x, y, best_a, 1, colors=[c['id'] for c in COLOR_LIST])\n        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[int(a)]['id']) for a in self.model.actions]\n        ax4.legend(proxy, self.model.action_labels, title='Actions')\n\n        if belief_points is not None:\n            for ax in [ax1,ax2,ax3,ax4]:\n                ax.scatter(belief_points[:,0], belief_points[:,1], s=1, c='black')\n\n\n    def _plot_grid(self, size=5, belief_set=None):\n        value_table = np.max(self.alpha_vector_array, axis=0)[self.model.state_grid]\n        best_action_table = np.array(self.actions)[np.argmax(self.alpha_vector_array, axis=0)][self.model.state_grid]\n        best_action_colors = COLOR_ARRAY[best_action_table]\n\n        dimensions = self.model.state_grid.shape\n\n        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(size*2, size), width_ratios=(0.55,0.45))\n\n        # Ticks\n        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n        ax1.set_title('Value function')\n        ax1_plot = ax1.imshow(value_table)\n\n        if dimensions[0] &gt;= dimensions[1]: # If higher than wide \n            plt.colorbar(ax1_plot, ax=ax1)\n        else:\n            plt.colorbar(ax1_plot, ax=ax1, location='bottom', orientation='horizontal')\n\n        ax1.set_xticks(x_ticks)\n        ax1.set_yticks(y_ticks)\n\n        ax2.set_title('Action policy')\n        ax2.imshow(best_action_colors)\n        p = [ patches.Patch(color=COLOR_LIST[int(i)]['id'], label=str(self.model.action_labels[int(i)])) for i in self.model.actions]\n        ax2.legend(handles=p, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Actions')\n        ax2.set_xticks(x_ticks)\n        ax2.set_yticks(y_ticks)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.actions","title":"<code>actions: np.ndarray</code>  <code>property</code>","text":"<p>A list of N actions corresponding to the N alpha vectors making up the value function. If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.alpha_vector_array","title":"<code>alpha_vector_array: np.ndarray</code>  <code>property</code>","text":"<p>A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model) If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.alpha_vector_list","title":"<code>alpha_vector_list: list[AlphaVector]</code>  <code>property</code>","text":"<p>A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.</p>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.append","title":"<code>append(alpha_vector)</code>","text":"<p>Function to add an alpha vector to the value function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha_vector</code> <code>AlphaVector</code> <p>The alpha vector to be added to the value function.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def append(self, alpha_vector:AlphaVector) -&gt; None:\n    '''\n    Function to add an alpha vector to the value function.\n\n    Parameters\n    ----------\n    alpha_vector : AlphaVector\n        The alpha vector to be added to the value function.\n    '''\n    # Make sure size is correct\n    assert alpha_vector.values.shape[0] == self.model.state_count, f\"Vector to add to value function doesn't have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})\"\n\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n    assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f\"Vector is{' not' if self.is_on_gpu else ''} on GPU while value function is{'' if self.is_on_gpu else ' not'}.\"\n\n    if self._vector_array is not None:\n        self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)\n        self._actions = xp.append(self._actions, alpha_vector.action)\n\n    if self._vector_list is not None:\n        self._vector_list.append(alpha_vector)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.evaluate_at","title":"<code>evaluate_at(belief)</code>","text":"<p>Function to evaluate the value function at a belief point or at a set of belief points. It returns a value and the associated action.</p> <p>Parameters:</p> Name Type Description Default <code>belief</code> <code>Belief or BeliefSet</code> required <p>Returns:</p> Name Type Description <code>value</code> <code>float or ndarray</code> <p>The largest value associated with the belief point(s)</p> <code>action</code> <code>int or ndarray</code> <p>The action(s) associated with the vector having the highest values at the belief point(s).</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def evaluate_at(self, belief:Belief|BeliefSet) -&gt; tuple[float|np.ndarray, int|np.ndarray]:\n    '''\n    Function to evaluate the value function at a belief point or at a set of belief points.\n    It returns a value and the associated action.\n\n    Parameters\n    ----------\n    belief : Belief or BeliefSet\n\n    Returns\n    -------\n    value : float or np.ndarray\n        The largest value associated with the belief point(s)\n    action : int or np.ndarray\n        The action(s) associated with the vector having the highest values at the belief point(s).\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    best_value = None\n    best_action = None\n\n    if isinstance(belief, Belief):\n        # Computing values\n        belief_values = xp.dot(self.alpha_vector_array, belief.values)\n\n        # Selecting best vectors\n        best_vector = xp.argmax(belief_values)\n\n        # From best vector, compute the best value and action\n        best_value = float(belief_values[best_vector])\n        best_action = int(self.actions[best_vector])\n    else:\n        # Computing values\n        belief_values = xp.matmul(belief.values if isinstance(belief, Belief) else belief.belief_array, self.alpha_vector_array.T)\n\n        # Retrieving the top vectors according to the value function\n        best_vectors = xp.argmax(belief_values, axis=1)\n\n        # Retrieving the values and actions associated with the vectors chosen\n        best_value = belief_values[xp.arange(belief_values.shape[0]), best_vectors]\n        best_action = self.actions[best_vectors]\n\n    return (best_value, best_action)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.extend","title":"<code>extend(other_value_function)</code>","text":"<p>Function to add another value function is place. Effectively, it performs the union of the two sets of alpha vectors.</p> <p>Parameters:</p> Name Type Description Default <code>other_value_function</code> <code>ValueFunction</code> <p>The other side of the union.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def extend(self, other_value_function:'Model') -&gt; None:\n    '''\n    Function to add another value function is place.\n    Effectively, it performs the union of the two sets of alpha vectors.\n\n    Parameters\n    ----------\n    other_value_function : ValueFunction\n        The other side of the union.\n    '''\n    self._uniqueness_dict.update(other_value_function._uniqueness_dict)\n    self._vector_list = list(self._uniqueness_dict.values())\n\n    self._vector_array = None\n    self._actions = None\n\n    self._pruning_level = 1\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.load","title":"<code>load(file, model)</code>  <code>classmethod</code>","text":"<p>Function to load the value function from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path and file_name of the value function to be loaded.</p> required <code>model</code> <code>Model</code> <p>The model the value function is linked to.</p> required <p>Returns:</p> Name Type Description <code>loaded_value_function</code> <code>ValueFunction</code> <p>The loaded value function.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>@classmethod\ndef load(cls,\n        file:str,\n        model:Model\n        ) -&gt; 'ValueFunction':\n    '''\n    Function to load the value function from a csv file.\n\n    Parameters\n    ----------\n    file : str\n        The path and file_name of the value function to be loaded.\n    model : mdp.Model\n        The model the value function is linked to.\n\n    Returns\n    -------\n    loaded_value_function : ValueFunction\n        The loaded value function.\n    '''\n    av_array = np.load(file)\n\n    return ValueFunction(model, alpha_vectors=av_array[:,1:], action_list=av_array[:,0].astype(int))\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.plot","title":"<code>plot(as_grid=False, size=5, belief_set=None)</code>","text":"<p>Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid. If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.</p> <p>Parameters:</p> Name Type Description Default <code>as_grid</code> <code>bool</code> <p>Forces the plot to be plot as a grid.</p> <code>False</code> <code>size</code> <code>int</code> <p>The actual plot scale.</p> <code>5</code> <code>belief_set</code> <code>ndarray</code> <p>A set of belief to plot the belief points that were explored.</p> <code>None</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def plot(self,\n         as_grid:bool=False,\n         size:int=5,\n         belief_set:np.ndarray=None\n         ) -&gt; None:\n    '''\n    Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.\n    If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.\n\n    Parameters\n    ----------\n    as_grid : bool, default=False\n        Forces the plot to be plot as a grid.\n    size : int, default=5\n        The actual plot scale.\n    belief_set : np.ndarray, optional\n        A set of belief to plot the belief points that were explored.\n    '''\n    assert len(self) &gt; 0, \"Value function is empty, plotting is impossible...\"\n\n    # If on GPU, convert to CPU and plot that one\n    if self.is_on_gpu:\n        print('[Warning] Value function on GPU, converting to numpy before plotting...')\n        cpu_value_function = self.to_cpu()\n        cpu_value_function.plot(as_grid, size, belief_set)\n        return\n\n    func = None\n    if as_grid:\n        func = self._plot_grid\n    elif self.model.state_count == 2:\n        func = self._plot_2D\n    elif self.model.state_count == 3:\n        func = self._plot_3D\n    else:\n        print('[Warning] \\'as_grid\\' parameter set to False but state count is &gt;3 so it will be plotted as a grid')\n        func = self._plot_grid\n\n    func(size, belief_set)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.prune","title":"<code>prune(level=1)</code>","text":"<p>Function pruning the set of alpha vectors composing the value function. The pruning is as thorough as the level:     - 2: 1+ Check of absolute domination (check if dominated at each state).     - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.</p> <p>Note that the higher the level, the heavier the time impact will be.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Between 0 and 3, how thorough the alpha vector pruning should be.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def prune(self, level:int=1) -&gt; None:\n    '''\n    Function pruning the set of alpha vectors composing the value function.\n    The pruning is as thorough as the level:\n        - 2: 1+ Check of absolute domination (check if dominated at each state).\n        - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.\n\n    Note that the higher the level, the heavier the time impact will be.\n\n    Parameters\n    ----------\n    level : int, default=1\n        Between 0 and 3, how thorough the alpha vector pruning should be.\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    # Level 1 or under\n    if level &lt; self._pruning_level or level &gt; 3:\n        log('Attempting to prune a value function to a level already reached. Returning \\'self\\'')\n        return\n\n    # Level 2 pruning: Check for absolute domination\n    if level &gt;= 2 and self._pruning_level &lt; 2:\n        non_dominated_vector_indices = []\n\n        for i, v in enumerate(self.alpha_vector_array):\n            is_dom_by = xp.all(self.alpha_vector_array &gt;= v, axis=1)\n            if len(xp.where(is_dom_by)[0]) == 1:\n                non_dominated_vector_indices.append(i)\n\n        self._vector_array = self._vector_array[non_dominated_vector_indices]\n        self._actions = self._actions[non_dominated_vector_indices]\n\n    # Level 3 pruning: LP to check for more complex domination\n    if level &gt;= 3:\n        assert ilp_support, \"ILP support not enabled...\"\n\n        pruned_alpha_set = pruned_alpha_set.to_cpu()\n\n        alpha_set = pruned_alpha_set.alpha_vector_array\n        non_dominated_vector_indices = []\n\n        for i, alpha_vect in enumerate(alpha_set):\n            other_alphas = alpha_set[:i] + alpha_set[(i+1):]\n\n            # Objective function\n            c = np.concatenate([np.array([1]), -1*alpha_vect])\n\n            # Alpha vector contraints\n            other_count = len(other_alphas)\n            A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]\n            alpha_constraints = LinearConstraint(A, 0, np.inf)\n\n            # Constraints that sum of beliefs is 1\n            belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)\n\n            # Solve problem\n            res = milp(c=c, constraints=[alpha_constraints, belief_constraint])\n\n            # Check if dominated\n            is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0\n            if is_dominated:\n                print(alpha_vect)\n                print(' -&gt; Dominated\\n')\n            else:\n                non_dominated_vector_indices.append(i)\n\n        self._vector_array = self._vector_array[non_dominated_vector_indices]\n        self._actions = self._actions[non_dominated_vector_indices]\n\n    # Update the tracked pruned level so far\n    self._pruning_level = level\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.save","title":"<code>save(folder='./ValueFunctions', file_name=None)</code>","text":"<p>Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory. If no file_name is provided, it be saved as '_value_function.csv'. <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The path at which the npy file will be saved.</p> <code>'./ValueFunctions'</code> <code>file_name</code> <code>str</code> <p>The file name used to save in.</p> <code>'&lt;current_timestamp&gt;_value_function.npy'</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def save(self,\n         folder:str='./ValueFunctions',\n         file_name:Union[str,None]=None\n         ) -&gt; None:\n    '''\n    Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.\n    If no file_name is provided, it be saved as '&lt;current_timestamp&gt;_value_function.csv'.\n\n    Parameters\n    ----------\n    folder : str, default='./ValueFunctions'\n        The path at which the npy file will be saved.\n    file_name : str, default='&lt;current_timestamp&gt;_value_function.npy'\n        The file name used to save in.\n    '''\n    if self.is_on_gpu:\n        self.to_cpu().save(path=folder, file_name=file_name)\n        return\n\n    # Handle file_name\n    if file_name is None:\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        file_name = timestamp + '_value_function.npy'\n\n    # Make sure that .csv is in the file name\n    if '.npy' not in file_name:\n        file_name += '.npy'\n\n    # Getting array\n    av_array = np.hstack([self.actions[:,None], self.alpha_vector_array])\n\n    np.save(folder + '/' + file_name, av_array)\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.</p> <p>Returns:</p> Name Type Description <code>cpu_value_function</code> <code>ValueFunction</code> <p>A new value function with arrays on CPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def to_cpu(self) -&gt; 'ValueFunction':\n    '''\n    Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.\n\n    Returns\n    -------\n    cpu_value_function : ValueFunction\n        A new value function with arrays on CPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    cpu_model = self.model.cpu_model\n\n    cpu_value_function = None\n    if self._vector_list is not None:\n        cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]\n        cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)\n\n    else:\n        cpu_vector_array = cp.asnumpy(self._vector_array)\n        cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)\n        cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)\n\n    return cpu_value_function\n</code></pre>"},{"location":"reference/agents/model_based_util/#olfactory_navigation.agents.model_based_util.ValueFunction.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.</p> <p>Returns:</p> Name Type Description <code>gpu_value_function</code> <code>ValueFunction</code> <p>A new value function with arrays on GPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def to_gpu(self) -&gt; 'ValueFunction':\n    '''\n    Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.\n\n    Returns\n    -------\n    gpu_value_function : ValueFunction\n        A new value function with arrays on GPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    gpu_model = self.model.gpu_model\n\n    gpu_value_function = None\n    if self._vector_list is not None:\n        gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]\n        gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)\n\n    else:\n        gpu_vector_array = cp.array(self._vector_array)\n        gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)\n        gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)\n\n    return gpu_value_function\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/","title":"belief","text":""},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief","title":"<code>Belief</code>","text":"<p>A class representing a belief in the space of a given model. It is the belief to be in any combination of states: eg:     - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.</p> <p>The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which the belief applies on.</p> required <code>values</code> <code>ndarray</code> <p>A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1. If not specified, it will be set as the start probabilities of the model.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <code>values</code> <code>ndarray</code> <code>bytes_repr</code> <code>bytes</code> <p>A representation in bytes of the value of the belief</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>class Belief:\n    '''\n    A class representing a belief in the space of a given model. It is the belief to be in any combination of states:\n    eg:\n        - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.\n\n    The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'\n\n    ...\n\n    Parameters\n    ----------\n    model : pomdp.Model\n        The model on which the belief applies on.\n    values : np.ndarray, optional\n        A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1.\n        If not specified, it will be set as the start probabilities of the model.\n\n    Attributes\n    ----------\n    model : pomdp.Model\n    values : np.ndarray\n    bytes_repr : bytes\n        A representation in bytes of the value of the belief\n    '''\n    def __init__(self, model:Model, values:Union[np.ndarray,None]=None):\n        assert model is not None\n        self.model = model\n\n        if values is not None:\n            assert values.shape[0] == model.state_count, \"Belief must contain be of dimension |S|\"\n\n            xp = np if not gpu_support else cp.get_array_module(values)\n\n            prob_sum = xp.sum(values)\n            rounded_sum = xp.round(prob_sum, decimals=3)\n            assert rounded_sum == 1.0, f\"States probabilities in belief must sum to 1 (found: {prob_sum}; rounded {rounded_sum})\"\n\n            self._values = values\n        else:\n            self._values = model.start_probabilities\n\n\n    def __new__(cls, *args, **kwargs):\n        instance = super().__new__(cls)\n\n        instance._bytes_repr = None\n        instance._successors = {}\n\n        return instance\n\n\n    @property\n    def bytes_repr(self) -&gt; bytes:\n        '''\n        A representation as bytes of a belief.\n        '''\n        if self._bytes_repr is None:\n            self._bytes_repr = self.values.tobytes()\n        return self._bytes_repr\n\n\n    def __eq__(self, other: object) -&gt; bool:\n        '''\n        A way to check the equality between two belief points.\n        The byte representation of each belief point is compared.\n        '''\n        return self.bytes_repr == other.bytes_repr\n\n\n    @property\n    def values(self) -&gt; np.ndarray:\n        '''\n        An array of the probability distribution to be in each state.\n        '''\n        return self._values\n\n\n    def update(self,\n               a:int,\n               o:int,\n               throw_error:bool=True\n               ) -&gt; 'Belief':\n        '''\n        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n        Parameters\n        ----------\n        a : int\n            The most recent action.\n        o : int\n            The most recent observation.\n        throw_error : bool, default=True\n            Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.\n\n        Returns\n        -------\n        new_belief : Belief\n            An updated belief\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        # Check if successor exists\n        succ_id = f'{a}_{o}'\n        succ = self._successors.get(succ_id)\n        if succ is not None:\n            return succ\n\n        # Computing new probabilities\n        reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]\n        new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)\n\n        # Normalization\n        probability_sum = xp.sum(new_state_probabilities)\n        if probability_sum == 0:\n            if throw_error:\n                raise ValueError(\"Impossible belief: the sum of probabilities is 0...\")\n        else:\n            new_state_probabilities /= probability_sum\n\n        # Generation of new belief from new state probabilities\n        new_belief = self.__new__(self.__class__)\n        new_belief.model = self.model\n        new_belief._values = new_state_probabilities\n\n        # Remember generated successor\n        self._successors[succ_id] = new_belief\n\n        return new_belief\n\n\n    def generate_successors(self) -&gt; list['Belief']:\n        '''\n        Function to generate a set of belief that can be reached for each actions and observations available in the model.\n\n        Returns\n        -------\n        successor_beliefs : list[Belief]\n            The successor beliefs.\n        '''\n        successor_beliefs = []\n        for a in self.model.actions:\n            for o in self.model.observations:\n                b_ao = self.update(a,o)\n                successor_beliefs.append(b_ao)\n\n        return successor_beliefs\n\n\n    def random_state(self) -&gt; int:\n        '''\n        Returns a random state of the model weighted by the belief probabily.\n\n        Returns\n        -------\n        rand_s : int\n            A random state.\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])\n        return rand_s\n\n\n    @property\n    def entropy(self) -&gt; float:\n        '''\n        The entropy of the belief point\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self._values)\n\n        return float(entropy(self._values) if xp == np else cupy_entropy(self._values))\n\n\n    def plot(self, size:int=5) -&gt; None:\n        '''\n        Function to plot a heatmap of the belief distribution if the belief is of a grid model.\n\n        Parameters\n        ----------\n        size : int, default=5\n            The scale of the plot.\n        '''\n        # Plot setup\n        plt.figure(figsize=(size*1.2,size))\n\n        model = self.model.cpu_model\n\n        # Ticks\n        dimensions = model.state_grid.shape\n        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n        plt.xticks(x_ticks)\n        plt.yticks(y_ticks)\n\n        # Title\n        plt.title(f'Belief (probability distribution over states)')\n\n        # Actual plot\n        belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)\n        grid_values = belief_values[model.state_grid]\n        plt.imshow(grid_values,cmap='Blues')\n        plt.colorbar()\n        plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.bytes_repr","title":"<code>bytes_repr: bytes</code>  <code>property</code>","text":"<p>A representation as bytes of a belief.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.entropy","title":"<code>entropy: float</code>  <code>property</code>","text":"<p>The entropy of the belief point</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.values","title":"<code>values: np.ndarray</code>  <code>property</code>","text":"<p>An array of the probability distribution to be in each state.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.__eq__","title":"<code>__eq__(other)</code>","text":"<p>A way to check the equality between two belief points. The byte representation of each belief point is compared.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    '''\n    A way to check the equality between two belief points.\n    The byte representation of each belief point is compared.\n    '''\n    return self.bytes_repr == other.bytes_repr\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.generate_successors","title":"<code>generate_successors()</code>","text":"<p>Function to generate a set of belief that can be reached for each actions and observations available in the model.</p> <p>Returns:</p> Name Type Description <code>successor_beliefs</code> <code>list[Belief]</code> <p>The successor beliefs.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def generate_successors(self) -&gt; list['Belief']:\n    '''\n    Function to generate a set of belief that can be reached for each actions and observations available in the model.\n\n    Returns\n    -------\n    successor_beliefs : list[Belief]\n        The successor beliefs.\n    '''\n    successor_beliefs = []\n    for a in self.model.actions:\n        for o in self.model.observations:\n            b_ao = self.update(a,o)\n            successor_beliefs.append(b_ao)\n\n    return successor_beliefs\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.plot","title":"<code>plot(size=5)</code>","text":"<p>Function to plot a heatmap of the belief distribution if the belief is of a grid model.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The scale of the plot.</p> <code>5</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def plot(self, size:int=5) -&gt; None:\n    '''\n    Function to plot a heatmap of the belief distribution if the belief is of a grid model.\n\n    Parameters\n    ----------\n    size : int, default=5\n        The scale of the plot.\n    '''\n    # Plot setup\n    plt.figure(figsize=(size*1.2,size))\n\n    model = self.model.cpu_model\n\n    # Ticks\n    dimensions = model.state_grid.shape\n    x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n    y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n    plt.xticks(x_ticks)\n    plt.yticks(y_ticks)\n\n    # Title\n    plt.title(f'Belief (probability distribution over states)')\n\n    # Actual plot\n    belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)\n    grid_values = belief_values[model.state_grid]\n    plt.imshow(grid_values,cmap='Blues')\n    plt.colorbar()\n    plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.random_state","title":"<code>random_state()</code>","text":"<p>Returns a random state of the model weighted by the belief probabily.</p> <p>Returns:</p> Name Type Description <code>rand_s</code> <code>int</code> <p>A random state.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def random_state(self) -&gt; int:\n    '''\n    Returns a random state of the model weighted by the belief probabily.\n\n    Returns\n    -------\n    rand_s : int\n        A random state.\n    '''\n    xp = np if not gpu_support else cp.get_array_module(self._values)\n\n    rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])\n    return rand_s\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.Belief.update","title":"<code>update(a, o, throw_error=True)</code>","text":"<p>Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>int</code> <p>The most recent action.</p> required <code>o</code> <code>int</code> <p>The most recent observation.</p> required <code>throw_error</code> <code>bool</code> <p>Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>new_belief</code> <code>Belief</code> <p>An updated belief</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def update(self,\n           a:int,\n           o:int,\n           throw_error:bool=True\n           ) -&gt; 'Belief':\n    '''\n    Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n    Parameters\n    ----------\n    a : int\n        The most recent action.\n    o : int\n        The most recent observation.\n    throw_error : bool, default=True\n        Whether the creation of an impossible belief (sum of probabilities of 0.0) will throw an error or not.\n\n    Returns\n    -------\n    new_belief : Belief\n        An updated belief\n    '''\n    xp = np if not gpu_support else cp.get_array_module(self._values)\n\n    # Check if successor exists\n    succ_id = f'{a}_{o}'\n    succ = self._successors.get(succ_id)\n    if succ is not None:\n        return succ\n\n    # Computing new probabilities\n    reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]\n    new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)\n\n    # Normalization\n    probability_sum = xp.sum(new_state_probabilities)\n    if probability_sum == 0:\n        if throw_error:\n            raise ValueError(\"Impossible belief: the sum of probabilities is 0...\")\n    else:\n        new_state_probabilities /= probability_sum\n\n    # Generation of new belief from new state probabilities\n    new_belief = self.__new__(self.__class__)\n    new_belief.model = self.model\n    new_belief._values = new_state_probabilities\n\n    # Remember generated successor\n    self._successors[succ_id] = new_belief\n\n    return new_belief\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet","title":"<code>BeliefSet</code>","text":"<p>Class to represent a set of beliefs with regard to a POMDP model. It has the purpose to store the beliefs in a numpy array format and be able to conver it to a list of Belief class objects.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which the beliefs apply.</p> required <code>beliefs</code> <code>list[Belief] | ndarray</code> <p>The actual set of beliefs.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <code>belief_array</code> <code>ndarray</code> <p>A 2D array of shape N x S of N belief vectors.</p> <code>belief_list</code> <code>list[Belief]</code> <p>A list of N Belief object.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>class BeliefSet:\n    '''\n    Class to represent a set of beliefs with regard to a POMDP model.\n    It has the purpose to store the beliefs in a numpy array format and be able to conver it to a list of Belief class objects.\n\n    ...\n\n    Parameters\n    ----------\n    model : pomdp.Model\n        The model on which the beliefs apply.\n    beliefs : list[Belief] | np.ndarray\n        The actual set of beliefs.\n\n    Attributes\n    ----------\n    model : pomdp.Model\n    belief_array : np.ndarray\n        A 2D array of shape N x S of N belief vectors.\n    belief_list : list[Belief]\n        A list of N Belief object.\n    '''\n    def __init__(self, model:Model, beliefs:Union[list[Belief],np.ndarray]) -&gt; None:\n        self.model = model\n\n        self._belief_list = None\n        self._belief_array = None\n        self._uniqueness_dict = None\n\n        self.is_on_gpu = False\n\n        if isinstance(beliefs, list):\n            assert all(len(b.values) == model.state_count for b in beliefs), f\"Beliefs in belief list provided dont all have shape ({model.state_count},)\"\n            self._belief_list = beliefs\n\n            # Check if on gpu and make sure all beliefs are also on the gpu\n            if (len(beliefs) &gt; 0) and gpu_support and cp.get_array_module(beliefs[0].values) == cp:\n                assert all(cp.get_array_module(b.values) == cp for b in beliefs), \"Either all or none of the alpha vectors should be on the GPU, not just some.\"\n                self.is_on_gpu = True\n        else:\n            assert beliefs.shape[1] == model.state_count, f\"Belief array provided doesnt have the right shape (expected (-,{model.state_count}), received {beliefs.shape})\"\n\n            self._belief_array = beliefs\n\n            # Check if array is on gpu\n            if gpu_support and cp.get_array_module(beliefs) == cp:\n                self.is_on_gpu = True\n\n\n    @property\n    def belief_array(self) -&gt; np.ndarray:\n        '''\n        A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._belief_array is None:\n            self._belief_array = xp.array([b.values for b in self._belief_list])\n        return self._belief_array\n\n\n    @property\n    def belief_list(self) -&gt; list[Belief]:\n        '''\n        A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.\n        '''\n        if self._belief_list is None:\n            self._belief_list = [Belief(self.model, belief_vector) for belief_vector in self._belief_array]\n        return self._belief_list\n\n\n    def generate_all_successors(self) -&gt; 'BeliefSet':\n        '''\n        Function to generate the successors beliefs of all the beliefs in the belief set.\n\n        Returns\n        -------\n        all_successors : BeliefSet\n            All successors of all beliefs in the belief set.\n        '''\n        all_successors = []\n        for belief in self.belief_list:\n            all_successors.extend(belief.generate_successors())\n        return BeliefSet(self.model, all_successors)\n\n\n    def update(self,\n               actions:list|np.ndarray,\n               observations:list|np.ndarray,\n               throw_error:bool=True\n               ) -&gt; 'BeliefSet':\n        '''\n        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n        Parameters\n        ----------\n        actions : list or np.ndarray\n            The most recent played actions.\n        observations : list or np.ndarray\n            The most recent received observations.\n        throw_error : bool, default=True\n            Whether the throw an error when attempting to generate impossible beliefs.\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            An set of updated beliefs.\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        # Ensuring we are dealing we are dealing with ndarrays\n        observations = xp.array(observations)\n        actions = xp.array(actions)\n\n        # Computing reachable probabilities and states\n        reachable_probabilities = (self.model.reachable_transitional_observation_table[:, actions, observations, :] * self.belief_array.T[:,:,None])\n        reachable_state_per_actions = self.model.reachable_states[:, actions, :]\n\n        # Computing new probabilities\n        flatten_offset = xp.arange(len(observations))[:,None] * self.model.state_count\n        flat_shape = (len(observations), (self.model.state_count * self.model.reachable_state_count))\n\n        a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape)\n        w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape)\n\n        a_offs = a + flatten_offset\n        new_probabilities = xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*self.model.state_count).reshape((-1,self.model.state_count))\n\n        # Normalization\n        probability_sum = xp.sum(new_probabilities, axis=1)\n        if xp.any(probability_sum == 0.0) and throw_error:\n            raise ValueError('One or more belief is impossible, (ie the sum of the probability distribution is 0)')\n        non_zero_mask = probability_sum != 0\n        new_probabilities[non_zero_mask] /= probability_sum[non_zero_mask,None]\n\n        return BeliefSet(self.model, new_probabilities)\n\n\n    @property\n    def unique_belief_dict(self) -&gt; dict:\n        '''\n        A dictionary of unique belief points with the keys being the byte representation of these belief points.\n        '''\n        if self._uniqueness_dict is None:\n            self._uniqueness_dict = {belief.bytes_repr: belief for belief in self.belief_list}\n        return self._uniqueness_dict\n\n\n    def union(self, other_belief_set:'BeliefSet') -&gt; 'BeliefSet':\n        '''\n        Function to make the union between two belief set objects.\n\n        Parameters\n        ----------\n        other_belief_set : BeliefSet\n            The other belief set to make the union with\n\n        Returns\n        -------\n        new_belief_set : BeliefSet\n            A new, combined, belief set\n        '''\n        # Deduplication\n        combined_uniqueness_dict = self.unique_belief_dict | other_belief_set.unique_belief_dict\n\n        # Generation of new set\n        new_belief_set = BeliefSet(self.model, list(combined_uniqueness_dict.values()))\n        new_belief_set._uniqueness_dict = combined_uniqueness_dict\n\n        return new_belief_set\n\n\n    def __len__(self) -&gt; int:\n        return len(self._belief_list) if self._belief_list is not None else self._belief_array.shape[0]\n\n\n    @property\n    def entropies(self) -&gt; np.ndarray:\n        '''\n        An array of the entropies of the belief points.\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self.belief_array)\n\n        return entropy(self.belief_array, axis=1) if xp == np else cupy_entropy(self.belief_array, axis=1)\n\n\n    def to_gpu(self) -&gt; 'BeliefSet':\n        '''\n        Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.\n\n        Returns\n        -------\n        gpu_belief_set : BeliefSet\n            A new belief set with array on GPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        gpu_model = self.model.gpu_model\n\n        gpu_belief_set = None\n        if self._belief_array is not None:\n            gpu_belief_array = cp.array(self._belief_array)\n            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)\n        else:\n            gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]\n            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)\n\n        return gpu_belief_set\n\n\n    def to_cpu(self) -&gt; 'BeliefSet':\n        '''\n        Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.\n\n        Returns\n        -------\n        cpu_belief_set : BeliefSet\n            A new belief set with array on CPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        cpu_model = self.model.cpu_model\n\n        cpu_belief_set = None\n        if self._belief_array is not None:\n            cpu_belief_array = cp.asnumpy(self._belief_array)\n            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)\n\n        else:\n            cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]\n            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)\n\n        return cpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.belief_array","title":"<code>belief_array: np.ndarray</code>  <code>property</code>","text":"<p>A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.belief_list","title":"<code>belief_list: list[Belief]</code>  <code>property</code>","text":"<p>A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.entropies","title":"<code>entropies: np.ndarray</code>  <code>property</code>","text":"<p>An array of the entropies of the belief points.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.unique_belief_dict","title":"<code>unique_belief_dict: dict</code>  <code>property</code>","text":"<p>A dictionary of unique belief points with the keys being the byte representation of these belief points.</p>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.generate_all_successors","title":"<code>generate_all_successors()</code>","text":"<p>Function to generate the successors beliefs of all the beliefs in the belief set.</p> <p>Returns:</p> Name Type Description <code>all_successors</code> <code>BeliefSet</code> <p>All successors of all beliefs in the belief set.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def generate_all_successors(self) -&gt; 'BeliefSet':\n    '''\n    Function to generate the successors beliefs of all the beliefs in the belief set.\n\n    Returns\n    -------\n    all_successors : BeliefSet\n        All successors of all beliefs in the belief set.\n    '''\n    all_successors = []\n    for belief in self.belief_list:\n        all_successors.extend(belief.generate_successors())\n    return BeliefSet(self.model, all_successors)\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.</p> <p>Returns:</p> Name Type Description <code>cpu_belief_set</code> <code>BeliefSet</code> <p>A new belief set with array on CPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def to_cpu(self) -&gt; 'BeliefSet':\n    '''\n    Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.\n\n    Returns\n    -------\n    cpu_belief_set : BeliefSet\n        A new belief set with array on CPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    cpu_model = self.model.cpu_model\n\n    cpu_belief_set = None\n    if self._belief_array is not None:\n        cpu_belief_array = cp.asnumpy(self._belief_array)\n        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)\n\n    else:\n        cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]\n        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)\n\n    return cpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.</p> <p>Returns:</p> Name Type Description <code>gpu_belief_set</code> <code>BeliefSet</code> <p>A new belief set with array on GPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def to_gpu(self) -&gt; 'BeliefSet':\n    '''\n    Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.\n\n    Returns\n    -------\n    gpu_belief_set : BeliefSet\n        A new belief set with array on GPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    gpu_model = self.model.gpu_model\n\n    gpu_belief_set = None\n    if self._belief_array is not None:\n        gpu_belief_array = cp.array(self._belief_array)\n        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)\n    else:\n        gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]\n        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)\n\n    return gpu_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.union","title":"<code>union(other_belief_set)</code>","text":"<p>Function to make the union between two belief set objects.</p> <p>Parameters:</p> Name Type Description Default <code>other_belief_set</code> <code>BeliefSet</code> <p>The other belief set to make the union with</p> required <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>A new, combined, belief set</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def union(self, other_belief_set:'BeliefSet') -&gt; 'BeliefSet':\n    '''\n    Function to make the union between two belief set objects.\n\n    Parameters\n    ----------\n    other_belief_set : BeliefSet\n        The other belief set to make the union with\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        A new, combined, belief set\n    '''\n    # Deduplication\n    combined_uniqueness_dict = self.unique_belief_dict | other_belief_set.unique_belief_dict\n\n    # Generation of new set\n    new_belief_set = BeliefSet(self.model, list(combined_uniqueness_dict.values()))\n    new_belief_set._uniqueness_dict = combined_uniqueness_dict\n\n    return new_belief_set\n</code></pre>"},{"location":"reference/agents/model_based_util/belief/#olfactory_navigation.agents.model_based_util.belief.BeliefSet.update","title":"<code>update(actions, observations, throw_error=True)</code>","text":"<p>Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>list or ndarray</code> <p>The most recent played actions.</p> required <code>observations</code> <code>list or ndarray</code> <p>The most recent received observations.</p> required <code>throw_error</code> <code>bool</code> <p>Whether the throw an error when attempting to generate impossible beliefs.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>new_belief_set</code> <code>BeliefSet</code> <p>An set of updated beliefs.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief.py</code> <pre><code>def update(self,\n           actions:list|np.ndarray,\n           observations:list|np.ndarray,\n           throw_error:bool=True\n           ) -&gt; 'BeliefSet':\n    '''\n    Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).\n\n    Parameters\n    ----------\n    actions : list or np.ndarray\n        The most recent played actions.\n    observations : list or np.ndarray\n        The most recent received observations.\n    throw_error : bool, default=True\n        Whether the throw an error when attempting to generate impossible beliefs.\n\n    Returns\n    -------\n    new_belief_set : BeliefSet\n        An set of updated beliefs.\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    # Ensuring we are dealing we are dealing with ndarrays\n    observations = xp.array(observations)\n    actions = xp.array(actions)\n\n    # Computing reachable probabilities and states\n    reachable_probabilities = (self.model.reachable_transitional_observation_table[:, actions, observations, :] * self.belief_array.T[:,:,None])\n    reachable_state_per_actions = self.model.reachable_states[:, actions, :]\n\n    # Computing new probabilities\n    flatten_offset = xp.arange(len(observations))[:,None] * self.model.state_count\n    flat_shape = (len(observations), (self.model.state_count * self.model.reachable_state_count))\n\n    a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape)\n    w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape)\n\n    a_offs = a + flatten_offset\n    new_probabilities = xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*self.model.state_count).reshape((-1,self.model.state_count))\n\n    # Normalization\n    probability_sum = xp.sum(new_probabilities, axis=1)\n    if xp.any(probability_sum == 0.0) and throw_error:\n        raise ValueError('One or more belief is impossible, (ie the sum of the probability distribution is 0)')\n    non_zero_mask = probability_sum != 0\n    new_probabilities[non_zero_mask] /= probability_sum[non_zero_mask,None]\n\n    return BeliefSet(self.model, new_probabilities)\n</code></pre>"},{"location":"reference/agents/model_based_util/belief_value_mapping/","title":"belief_value_mapping","text":""},{"location":"reference/agents/model_based_util/belief_value_mapping/#olfactory_navigation.agents.model_based_util.belief_value_mapping.BeliefValueMapping","title":"<code>BeliefValueMapping</code>","text":"<p>Alternate representation of a value function, particularly for pomdp models. It works by adding adding belief and associated value to the object. To evaluate this version of the value function the sawtooth algorithm is used (described in Shani G. et al., \"A survey of point-based POMDP solvers\")</p> <p>We can also compute the Q value for a particular belief b and action using the qva function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which the value function applies on</p> required <code>corner_belief_values</code> <code>ValueFunction</code> <p>A general value function to define the value at corner points in belief space (ie: at certainty beliefs, or when beliefs have a probability of 1 for a given state). This is usually the solution of the MDP version of the problem.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <code>corner_belief_values</code> <code>ValueFunction</code> <code>corner_values</code> <code>ndarray</code> <p>Array of |S| shape, having the max value at each state based on the corner_belief_values.</p> <code>beliefs</code> <code>Belief</code> <p>Beliefs contained in the belief-value mapping.</p> <code>belief_value_mapping</code> <code>dict[bytes, float]</code> <p>Mapping of beliefs points with their associated value.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief_value_mapping.py</code> <pre><code>class BeliefValueMapping:\n    '''\n    Alternate representation of a value function, particularly for pomdp models.\n    It works by adding adding belief and associated value to the object.\n    To evaluate this version of the value function the sawtooth algorithm is used (described in Shani G. et al., \"A survey of point-based POMDP solvers\")\n\n    We can also compute the Q value for a particular belief b and action using the qva function.\n\n    Parameters\n    ----------\n    model : pomdp.Model\n        The model on which the value function applies on\n    corner_belief_values : ValueFunction\n        A general value function to define the value at corner points in belief space (ie: at certainty beliefs, or when beliefs have a probability of 1 for a given state).\n        This is usually the solution of the MDP version of the problem.\n\n    Attributes\n    ----------\n    model : pomdp.Model\n    corner_belief_values : ValueFunction\n    corner_values : np.ndarray\n        Array of |S| shape, having the max value at each state based on the corner_belief_values.\n    beliefs : Belief\n        Beliefs contained in the belief-value mapping.\n    belief_value_mapping : dict[bytes, float]\n        Mapping of beliefs points with their associated value.\n\n    '''\n    def __init__(self, model, corner_belief_values:ValueFunction) -&gt; None:\n        xp = np if not gpu_support else cp.get_array_module(corner_belief_values.alpha_vector_array)\n\n        self.model = model\n        self.corner_belief_values = corner_belief_values\n\n        self.corner_values = xp.max(corner_belief_values.alpha_vector_array, axis=0)\n\n        self.beliefs = []\n        self.belief_value_mapping = {}\n\n        self._belief_array = None\n        self._value_array = None\n\n\n    def add(self, b:Belief, v:float) -&gt; None:\n        '''\n        Function to a belief point and its associated value to the belief value mappings\n\n        Parameters\n        ----------\n        b: Belief\n        v: float\n        '''\n        if b not in self.beliefs:\n            self.beliefs.append(b)\n            self.belief_value_mapping[b.bytes_repr] = v\n\n\n    @property\n    def belief_array(self) -&gt; np.ndarray:\n        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)\n\n        if self._belief_array is None:\n            self._belief_array = xp.array([b.values for b in self.beliefs])\n\n        return self._belief_array\n\n\n    @property\n    def value_array(self) -&gt; np.ndarray:\n        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)\n\n        if self._value_array is None:\n            self._value_array = xp.array(list(self.belief_value_mapping.values()))\n\n        return self._value_array\n\n\n    def update(self) -&gt; None:\n        '''\n        Function to update the belief and value arrays to speed up computation.\n        '''\n        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)\n\n        self._belief_array = xp.array([b.values for b in self.beliefs])\n        self._value_array = xp.array(list(self.belief_value_mapping.values()))\n\n\n    def evaluate(self, belief:Belief) -&gt; float:\n        '''\n        Runs the sawtooth algorithm to find the value at a given belief point.\n\n        Parameters\n        ----------\n        belief: Belief\n        '''\n        xp = np if not gpu_support else cp.get_array_module(belief.values)\n\n        # Shortcut if belief already exists in the mapping\n        if belief in self.beliefs:\n            return self.belief_value_mapping[belief.bytes_repr]\n\n        v0 = xp.dot(belief.values, self.corner_values)\n\n        if len(self.beliefs) == 0:\n            return float(v0)\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            vb = v0 + ((self.value_array - xp.dot(self.belief_array, self.corner_values)) * xp.min(belief.values / self.belief_array, axis=1))\n\n        return float(xp.min(xp.append(vb, v0)))\n</code></pre>"},{"location":"reference/agents/model_based_util/belief_value_mapping/#olfactory_navigation.agents.model_based_util.belief_value_mapping.BeliefValueMapping.add","title":"<code>add(b, v)</code>","text":"<p>Function to a belief point and its associated value to the belief value mappings</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>Belief</code> required <code>v</code> <code>float</code> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief_value_mapping.py</code> <pre><code>def add(self, b:Belief, v:float) -&gt; None:\n    '''\n    Function to a belief point and its associated value to the belief value mappings\n\n    Parameters\n    ----------\n    b: Belief\n    v: float\n    '''\n    if b not in self.beliefs:\n        self.beliefs.append(b)\n        self.belief_value_mapping[b.bytes_repr] = v\n</code></pre>"},{"location":"reference/agents/model_based_util/belief_value_mapping/#olfactory_navigation.agents.model_based_util.belief_value_mapping.BeliefValueMapping.evaluate","title":"<code>evaluate(belief)</code>","text":"<p>Runs the sawtooth algorithm to find the value at a given belief point.</p> <p>Parameters:</p> Name Type Description Default <code>belief</code> <code>Belief</code> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief_value_mapping.py</code> <pre><code>def evaluate(self, belief:Belief) -&gt; float:\n    '''\n    Runs the sawtooth algorithm to find the value at a given belief point.\n\n    Parameters\n    ----------\n    belief: Belief\n    '''\n    xp = np if not gpu_support else cp.get_array_module(belief.values)\n\n    # Shortcut if belief already exists in the mapping\n    if belief in self.beliefs:\n        return self.belief_value_mapping[belief.bytes_repr]\n\n    v0 = xp.dot(belief.values, self.corner_values)\n\n    if len(self.beliefs) == 0:\n        return float(v0)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        vb = v0 + ((self.value_array - xp.dot(self.belief_array, self.corner_values)) * xp.min(belief.values / self.belief_array, axis=1))\n\n    return float(xp.min(xp.append(vb, v0)))\n</code></pre>"},{"location":"reference/agents/model_based_util/belief_value_mapping/#olfactory_navigation.agents.model_based_util.belief_value_mapping.BeliefValueMapping.update","title":"<code>update()</code>","text":"<p>Function to update the belief and value arrays to speed up computation.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\belief_value_mapping.py</code> <pre><code>def update(self) -&gt; None:\n    '''\n    Function to update the belief and value arrays to speed up computation.\n    '''\n    xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)\n\n    self._belief_array = xp.array([b.values for b in self.beliefs])\n    self._value_array = xp.array(list(self.belief_value_mapping.values()))\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/","title":"mdp","text":""},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model","title":"<code>Model</code>","text":"<p>MDP Model class.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>int or list[str] or list[list[str]]</code> <p>A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.</p> required <code>actions</code> <code>int or list</code> <p>A list of action labels or an amount of actions to be used.</p> required <code>transitions</code> <code>array - like or function</code> <p>The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided.  If a function is provided, it has be able to deal with np.array arguments. If none is provided, it will be randomly generated.</p> <code>None</code> <code>reachable_states</code> <code>array - like</code> <p>A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair. It is optional but useful for speedup purposes.</p> <code>None</code> <code>rewards</code> <code>array - like or function</code> <p>The reward matrix, has to be |S| x |A| x |S|. A function can also be provided here but it has to be able to deal with np.array arguments. If provided, it will be use in combination with the transition matrix to fill to expected rewards.</p> <code>None</code> <code>rewards_are_probabilistic</code> <code>bool</code> <p>Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.</p> <code>False</code> <code>state_grid</code> <code>array - like</code> <p>If provided, the model will be converted to a grid model.</p> <code>None</code> <code>start_probabilities</code> <code>list</code> <p>The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state.</p> <code>None</code> <code>end_states</code> <code>list</code> <p>Entering either state in the list during a simulation will end the simulation.</p> <code>[]</code> <code>end_actions</code> <code>list</code> <p>Playing action of the list during a simulation will end the simulation.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>states</code> <code>ndarray</code> <p>A 1D array of states indices. Used to loop over states.</p> <code>state_labels</code> <code>list[str]</code> <p>A list of state labels. (To be mainly used for plotting)</p> <code>state_count</code> <code>int</code> <p>How many states are in the Model.</p> <code>state_grid</code> <code>ndarray</code> <p>The state indices organized as a 2D grid. (Used for plotting purposes)</p> <code>actions</code> <code>ndarry</code> <p>A 1D array of action indices. Used to loop over actions.</p> <code>action_labels</code> <code>list[str]</code> <p>A list of action labels. (To be mainly used for plotting)</p> <code>action_count</code> <code>int</code> <p>How many action are in the Model.</p> <code>transition_table</code> <code>ndarray</code> <p>A 3D matrix of the transition probabilities. Can be None in the case a transition function is provided instead. Note: When possible, use reachable states and reachable probabilities instead.</p> <code>transition_function</code> <code>function</code> <p>A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0. Can be None in the case a transition table is provided instead. Note: When possible, use reachable states and reachable probabilities instead.</p> <code>reachable_states</code> <code>ndarray</code> <p>A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.</p> <code>reachable_probabilities</code> <code>ndarray</code> <p>A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.</p> <code>reachable_state_count</code> <code>int</code> <p>The maximum of states that can be reached from any state-action combination.</p> <code>immediate_reward_table</code> <code>ndarray</code> <p>A 3D matrix of shape S x A x S of the reward that will received when taking action a, in state s and landing in state s_p. Can be None in the case an immediate rewards function is provided instead.</p> <code>immediate_reward_function</code> <code>function</code> <p>A callable function taking 3 argments: s, a, s_p and returning the immediate reward the agent will receive. Can be None in the case an immediate rewards function is provided instead.</p> <code>expected_reward_table</code> <code>ndarray</code> <p>A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s. It is made by taking the weighted average of immediate rewards and the transitions.</p> <code>start_probabilities</code> <code>ndarray</code> <p>A 1D array of length |S| containing the probility distribution of the agent starting in each state.</p> <code>rewards_are_probabilisitic</code> <code>bool</code> <p>Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.</p> <code>end_states</code> <code>list[int]</code> <p>A list of states that, when reached, terminate a simulation.</p> <code>end_actions</code> <code>list[int]</code> <p>A list of actions that, when taken, terminate a simulation.</p> <code>is_on_gpu</code> <code>bool</code> <p>Whether the numpy array of the model are stored on the gpu or not.</p> <code>gpu_model</code> <code>Model</code> <p>An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)</p> <code>cpu_model</code> <code>Model</code> <p>An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>class Model:\n    '''\n    MDP Model class.\n\n    ...\n\n    Parameters\n    ----------\n    states : int or list[str] or list[list[str]]\n        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.\n    actions : int or list\n        A list of action labels or an amount of actions to be used.\n    transitions : array-like or function, optional\n        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. \n        If a function is provided, it has be able to deal with np.array arguments.\n        If none is provided, it will be randomly generated.\n    reachable_states : array-like, optional\n        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.\n        It is optional but useful for speedup purposes.\n    rewards : array-like or function, optional\n        The reward matrix, has to be |S| x |A| x |S|.\n        A function can also be provided here but it has to be able to deal with np.array arguments.\n        If provided, it will be use in combination with the transition matrix to fill to expected rewards.\n    rewards_are_probabilistic : bool, default=False\n        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.\n    state_grid : array-like, optional\n        If provided, the model will be converted to a grid model.\n    start_probabilities : list, optional\n        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state.\n    end_states : list, optional\n        Entering either state in the list during a simulation will end the simulation.\n    end_actions : list, optional\n        Playing action of the list during a simulation will end the simulation.\n\n    Attributes\n    ----------\n    states : np.ndarray\n        A 1D array of states indices. Used to loop over states.\n    state_labels : list[str]\n        A list of state labels. (To be mainly used for plotting)\n    state_count : int\n        How many states are in the Model.\n    state_grid : np.ndarray\n        The state indices organized as a 2D grid. (Used for plotting purposes)\n    actions : np.ndarry\n        A 1D array of action indices. Used to loop over actions.\n    action_labels : list[str]\n        A list of action labels. (To be mainly used for plotting)\n    action_count : int\n        How many action are in the Model.\n    transition_table : np.ndarray\n        A 3D matrix of the transition probabilities.\n        Can be None in the case a transition function is provided instead.\n        Note: When possible, use reachable states and reachable probabilities instead.\n    transition_function : function\n        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.\n        Can be None in the case a transition table is provided instead.\n        Note: When possible, use reachable states and reachable probabilities instead.\n    reachable_states : np.ndarray\n        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.\n    reachable_probabilities : np.ndarray\n        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.\n    reachable_state_count : int\n        The maximum of states that can be reached from any state-action combination.\n    immediate_reward_table : np.ndarray\n        A 3D matrix of shape S x A x S of the reward that will received when taking action a, in state s and landing in state s_p.\n        Can be None in the case an immediate rewards function is provided instead.\n    immediate_reward_function : function\n        A callable function taking 3 argments: s, a, s_p and returning the immediate reward the agent will receive.\n        Can be None in the case an immediate rewards function is provided instead.\n    expected_reward_table : np.ndarray\n        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.\n        It is made by taking the weighted average of immediate rewards and the transitions.\n    start_probabilities : np.ndarray\n        A 1D array of length |S| containing the probility distribution of the agent starting in each state.\n    rewards_are_probabilisitic : bool\n        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.\n    end_states : list[int]\n        A list of states that, when reached, terminate a simulation.\n    end_actions : list[int]\n        A list of actions that, when taken, terminate a simulation.\n    is_on_gpu : bool\n        Whether the numpy array of the model are stored on the gpu or not.\n    gpu_model : mdp.Model\n        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)\n    cpu_model : mdp.Model\n        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)\n    '''\n    def __init__(self,\n                 states:Union[int, list[str], list[list[str]]],\n                 actions:Union[int, list],\n                 transitions=None,\n                 reachable_states=None,\n                 rewards=None,\n                 rewards_are_probabilistic:bool=False,\n                 state_grid=None,\n                 start_probabilities:Union[list,None]=None,\n                 end_states:list[int]=[],\n                 end_actions:list[int]=[]\n                 ):\n\n        # Empty variable\n        self._alt_model = None\n        self.is_on_gpu = False\n\n        log('Instantiation of MDP Model:')\n\n        # ------------------------- States -------------------------\n        self.state_grid = None\n        if isinstance(states, int): # State count\n            self.state_labels = [f's_{i}' for i in range(states)]\n\n        elif isinstance(states, list) and all(isinstance(item, list) for item in states): # 2D list of states\n            dim1 = len(states)\n            dim2 = len(states[0])\n            assert all(len(state_dim) == dim2 for state_dim in states), \"All sublists of states must be of equal size\"\n\n            self.state_labels = []\n            for state_dim in states:\n                for state in state_dim:\n                    self.state_labels.append(state)\n\n            self.state_grid = np.arange(dim1 * dim2).reshape(dim1, dim2)\n\n        else: # Default: single of list of string items\n            self.state_labels = [item for item in states if isinstance(item, str)]\n\n        self.state_count = len(self.state_labels)\n        self.states = np.arange(self.state_count)\n\n        log(f'- {self.state_count} states')\n\n        # ------------------------- Actions -------------------------\n        if isinstance(actions, int):\n            self.action_labels = [f'a_{i}' for i in range(actions)]\n        else:\n            self.action_labels = actions\n        self.action_count = len(self.action_labels)\n        self.actions = np.arange(self.action_count)\n\n        log(f'- {self.action_count} actions')\n\n        # ------------------------- Reachable states provided -------------------------\n        self.reachable_states = None\n        if reachable_states is not None:\n            self.reachable_states = np.array(reachable_states)\n            assert self.reachable_states.shape[:2] == (self.state_count, self.action_count), f\"Reachable states provided is not of the expected shape (received {self.reachable_states.shape}, expected ({self.state_count}, {self.action_count}, :))\"\n            self.reachable_state_count = self.reachable_states.shape[2]\n\n            log(f'- At most {self.reachable_state_count} reachable states per state-action pair')\n\n        # ------------------------- Transitions -------------------------\n        log('- Starting generation of transitions table')\n        start_ts = datetime.now()\n\n        self.transition_table = None\n        self.transition_function = None\n        if transitions is None:\n            if reachable_states is None:\n                # If no transitiong matrix and no reachable states given, generate random one\n                log('    &gt; [Warning] No transition matrix and no reachable states have provided so a random transition matrix is generated...')\n                random_probs = np.random.rand(self.state_count, self.action_count, self.state_count)\n\n                # Normalization to have s_p probabilies summing to 1\n                self.transition_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)\n            else:\n                # Make uniform transition probabilities over reachable states\n                log(f'    &gt; [Warning] No transition matrix or function provided but reachable states are, so probability to reach any reachable states will \"1 / reachable state count\" so here: {1/self.reachable_state_count:.3f}.')\n\n        elif callable(transitions): # Transition function\n            self.transition_function = transitions\n            # Attempt to create transition table in memory\n            t_arr = None\n            try:\n                t_arr = np.fromfunction(self.transition_function, (self.state_count, self.action_count, self.state_count))\n            except MemoryError:\n                log('    &gt; [Warning] Not enough memory to store transition table, using transition function provided...')\n            else:\n                self.transition_table = t_arr\n\n        else: # Array like\n            self.transition_table = np.array(transitions)\n            t_shape = self.transition_table.shape\n            exp_shape = (self.state_count, self.action_count, self.state_count)\n            assert t_shape == exp_shape, f\"Transitions table provided doesnt have the right shape, it should be SxAxS (expected {exp_shape}, received {t_shape})\"\n\n        duration = (datetime.now() - start_ts).total_seconds()\n        log(f'    &gt; Done in {duration:.3f}s')\n        if duration &gt; 1:\n            log(f'    &gt; [Warning] Transition table generation took long, if not done already, try to use the reachable_states parameter to speedup the process.')\n\n        # ------------------------- Rewards are probabilistic toggle -------------------------\n        self.rewards_are_probabilistic = rewards_are_probabilistic\n\n        # ------------------------- State grid -------------------------\n        log('- Generation of state grid')\n        if state_grid is None and self.state_grid is None:\n            self.state_grid = np.arange(self.state_count).reshape((1,self.state_count))\n\n        elif state_grid is not None:\n            assert all(isinstance(l, list) for l in state_grid), \"The provided states grid must be a list of lists.\"\n\n            grid_shape = (len(state_grid), len(state_grid[0]))\n            assert all(len(l) == grid_shape[1] for l in state_grid), \"All rows must have the same length.\"\n\n            if all(all(isinstance(e, int) for e in l) for l in state_grid):\n                state_grid = np.array(state_grid)\n                try:\n                    self.states[state_grid]\n                except:\n                    raise Exception('An error occured with the list of state indices provided...')\n                else:\n                    self.state_grid = state_grid\n\n            else:\n                log('    &gt; [Warning] Looping through all grid states provided to find the corresponding states, can take a while...')\n\n                np_state_grid = np.zeros(grid_shape, dtype=int)\n                states_covered = 0\n                for i, row in enumerate(state_grid):\n                    for j, element in enumerate(state_grid):\n                        if isinstance(element, str) and (element in self.state_labels):\n                            states_covered += 1\n                            np_state_grid[i,j] = self.state_labels.index(element)\n                        elif isinstance(element, int) and (element &lt; self.state_count):\n                            np_state_grid[i,j] = element\n\n                        else:\n                            raise Exception(f'Countains a state (\\'{state}\\') not in the list of states...')\n\n                assert states_covered == self.state_count, \"Some states of the state list are missing...\"\n\n        # ------------------------- Start state probabilities -------------------------\n        log('- Generating start probabilities table')\n        if start_probabilities is not None:\n            assert len(start_probabilities) == self.state_count\n            self.start_probabilities = np.array(start_probabilities,dtype=float)\n        else:\n            self.start_probabilities = np.full((self.state_count), 1/self.state_count)\n\n        # ------------------------- End state conditions -------------------------\n        self.end_states = end_states\n        self.end_actions = end_actions\n\n        # ------------------------- Reachable states -------------------------\n        # If not set yet\n        if self.reachable_states is None:\n            log('- Starting computation of reachable states from transition data')\n\n            if self.state_count &gt; 1000:\n                log('-    &gt; [Warning] For models with large amounts of states, this operation can take time. Try generating it advance and use the parameter \\'reachable_states\\'...')\n\n            start_ts = datetime.now()\n\n            self.reachable_states = []\n            self.reachable_state_count = 0\n            for s in self.states:\n                reachable_states_for_action = []\n                for a in self.actions:\n                    reachable_list = []\n                    if self.transition_table is not None:\n                        reachable_list = np.argwhere(self.transition_table[s,a,:] &gt; 0)[:,0].tolist()\n                    else:\n                        for sn in self.states:\n                            if self.transition_function(s,a,sn) &gt; 0:\n                                reachable_list.append(sn)\n                    reachable_states_for_action.append(reachable_list)\n\n                    if len(reachable_list) &gt; self.reachable_state_count:\n                        self.reachable_state_count = len(reachable_list)\n\n                self.reachable_states.append(reachable_states_for_action)\n\n            # In case some state-action pairs lead to more states than other, we fill with the 1st non states not used\n            for s in self.states:\n                for a in self.actions:\n                    to_add = 0\n                    while len(self.reachable_states[s][a]) &lt; self.reachable_state_count:\n                        if to_add not in self.reachable_states[s][a]:\n                            self.reachable_states[s][a].append(to_add)\n                        to_add += 1\n\n            # Converting to ndarray\n            self.reachable_states = np.array(self.reachable_states, dtype=int)\n\n            duration = (datetime.now() - start_ts).total_seconds()\n            log(f'    &gt; Done in {duration:.3f}s')\n            log(f'- At most {self.reachable_state_count} reachable states per state-action pair')\n\n        # ------------------------- Reachable state probabilities -------------------------\n        log('- Starting computation of reachable state probabilities from transition data')\n        start_ts = datetime.now()\n\n        if self.transition_function is None and self.transition_table is None:\n            self.reachable_probabilities = np.full(self.reachable_states.shape, 1/self.reachable_state_count)\n        elif self.transition_table is not None:\n            self.reachable_probabilities = self.transition_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]\n        else:\n            self.reachable_probabilities = np.fromfunction((lambda s,a,ri: self.transition_function(s.astype(int), a.astype(int), self.reachable_states[s.astype(int), a.astype(int), ri.astype(int)])), self.reachable_states.shape)\n\n        duration = (datetime.now() - start_ts).total_seconds()\n        log(f'    &gt; Done in {duration:.3f}s')\n\n        # ------------------------- Rewards -------------------------\n        self.immediate_reward_table = None\n        self.immediate_reward_function = None\n        if rewards == -1: # If -1 is set, it means the rewards are defined in the superclass POMDP\n            pass\n        elif rewards is None:\n            if (len(self.end_states) &gt; 0) or (len(self.end_actions) &gt; 0):\n                log('- [Warning] Rewards are not define but end states/actions are, reaching an end state or doing an end action will give a reward of 1.')\n                self.immediate_reward_function = self._end_reward_function\n            else:\n                # If no reward matrix given, generate random one\n                self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count)\n        elif callable(rewards):\n            # Rewards is a function\n            log('- [Warning] The rewards are provided as a function, if the model is saved, the rewards will need to be defined before loading model.')\n            log('    &gt; Alternative: Setting end states/actions and leaving the rewards can be done to make the end states/action giving a reward of 1 by default.')\n            self.immediate_reward_function = rewards\n            assert len(signature(rewards).parameters) == 3, \"Reward function should accept 3 parameters: s, a, sn...\"\n        else:\n            # Array like\n            self.immediate_reward_table = np.array(rewards)\n            r_shape = self.immediate_reward_table.shape\n            exp_shape = (self.state_count, self.action_count, self.state_count)\n            assert r_shape == exp_shape, f\"Rewards table doesnt have the right shape, it should be SxAxS (expected: {exp_shape}, received {r_shape})\"\n\n        # ------------------------- Min and max rewards -------------------------\n        self._min_reward = None\n        self._max_reward = None\n\n        # ------------------------- Expected rewards -------------------------\n        self.expected_rewards_table = None\n        if rewards != -1:\n            log('- Starting generation of expected rewards table')\n            start_ts = datetime.now()\n\n            reachable_rewards = None\n            if self.immediate_reward_table is not None:\n                reachable_rewards = self.immediate_reward_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]\n            else:\n                def reach_reward_func(s,a,ri):\n                    s = s.astype(int)\n                    a = a.astype(int)\n                    ri = ri.astype(int)\n                    return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri])\n\n                reachable_rewards = np.fromfunction(reach_reward_func, self.reachable_states.shape)\n\n            self._min_reward = float(np.min(reachable_rewards))\n            self._max_reward = float(np.max(reachable_rewards))\n\n            self.expected_rewards_table = np.einsum('sar,sar-&gt;sa', self.reachable_probabilities, reachable_rewards)\n\n            duration = (datetime.now() - start_ts).total_seconds()\n            log(f'    &gt; Done in {duration:.3f}s')\n\n\n    def _end_reward_function(self, s, a, sn):\n        return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)\n\n\n    def transition(self, s:int, a:int) -&gt; int:\n        '''\n        Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.\n\n        Parameters\n        ----------\n        s : int \n            The current state\n        a : int\n            The action to take\n\n        Returns\n        -------\n        s_p : int\n            The posterior state\n        '''\n        xp = cp if self.is_on_gpu else np\n\n        # Shortcut for deterministic systems\n        if self.reachable_state_count == 1:\n            return int(self.reachable_states[s,a,0])\n\n        s_p = int(xp.random.choice(a=self.reachable_states[s,a], size=1, p=self.reachable_probabilities[s,a])[0])\n        return s_p\n\n\n    def reward(self, s:int, a:int, s_p:int) -&gt; Union[int,float]:\n        '''\n        Returns the rewards of playing action a when in state s and landing in state s_p.\n        If the rewards are probabilistic, it will return 0 or 1.\n\n        Parameters\n        ----------\n        s : int\n            The current state\n        a : int\n            The action taking in state s\n        s_p : int\n            The state landing in after taking action a in state s\n\n        Returns\n        -------\n        reward : int or float\n            The reward received.\n        '''\n        reward = float(self.immediate_reward_table[s,a,s_p] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p))\n        if self.rewards_are_probabilistic:\n            rnd = random.random()\n            return 1 if rnd &lt; reward else 0\n        else:\n            return reward\n\n\n    def get_coords(self, items:Union[int,list]) -&gt; list[list[int]] | list[int]:\n        '''\n        Function to get the coordinate (on the state_grid) for the provided state index or indices.\n\n        Parameters\n        ----------\n        items : int | list[int]\n            The states ids or id get convert to a 2D coordinate.\n\n        Returns\n        -------\n        item_coords : list[int] | list[list[int]]\n            The 2D positions of the provided item ids.\n        '''\n        item_list = [items] if isinstance(items, int) else items\n        item_coords = [np.argwhere(self.cpu_model.state_grid == s)[0] for s in item_list]\n\n        return item_coords[0] if isinstance(items, int) else item_coords\n\n\n    def save(self, file_name:str, path:str='./Models') -&gt; None:\n        '''\n        Function to save the current model in a pickle file.\n        By default, the model will be saved in 'Models' directory in the current working directory but this can be changed using the 'path' parameter.\n\n        Parameters\n        ----------\n        file_name : str\n            The name of the json file the model will be saved in.\n        path : str, default='./Models'\n            The path at which the model will be saved.\n        '''\n        if not os.path.exists(path):\n            print('Folder does not exist yet, creating it...')\n            os.makedirs(path)\n\n        if not file_name.endswith('.pck'):\n            file_name += '.pck'\n\n        # Writing the cpu version of the file to a pickle file\n        with open(path + '/' + file_name, 'wb') as f:\n            pickle.dump(self.cpu_model, f)\n\n\n    @classmethod\n    def load_from_file(cls, file:str) -&gt; 'Model':\n        '''\n        Function to load a MDP model from a pickle file. The json structure must contain the same items as in the constructor of this class.\n\n        Parameters\n        ----------\n        file : str\n            The file and path of the model to be loaded.\n\n        Returns\n        -------\n        loaded_model : mdp.Model\n            An instance of the loaded model.\n        '''\n        with open(file, 'rb') as openfile:\n            loaded_model = pickle.load(openfile)\n\n        return loaded_model\n\n\n    @property\n    def gpu_model(self) -&gt; 'Model':\n        '''\n        The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.\n        '''\n        if self.is_on_gpu:\n            return self\n\n        assert gpu_support, \"GPU Support is not available, try installing cupy...\"\n\n        if self._alt_model is None:\n            log('Sending Model to GPU...')\n            start = datetime.now()\n\n            # Setting all the arguments of the new class and convert to cupy if numpy array\n            new_model = super().__new__(self.__class__)\n            for arg, val in self.__dict__.items():\n                new_model.__setattr__(arg, cp.array(val) if isinstance(val, np.ndarray) else val)\n\n            # GPU/CPU variables\n            new_model.is_on_gpu = True\n            new_model._alt_model = self\n            self._alt_model = new_model\n\n            duration = (datetime.now() - start).total_seconds()\n            log(f'    &gt; Done in {duration:.3f}s')\n\n        return self._alt_model\n\n\n    @property\n    def cpu_model(self) -&gt; 'Model':\n        '''\n        The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.\n        '''\n        if not self.is_on_gpu:\n            return self\n\n        assert gpu_support, \"GPU Support is not available, try installing cupy...\"\n\n        if self._alt_model is None:\n            log('Sending Model to CPU...')\n            start = datetime.now()\n\n            # Setting all the arguments of the new class and convert to numpy if cupy array\n            new_model = super().__new__(self.__class__)\n            for arg, val in self.__dict__.items():\n                new_model.__setattr__(arg, cp.asnumpy(val) if isinstance(val, cp.ndarray) else val)\n\n            # GPU/CPU variables\n            new_model.is_on_gpu = False\n            new_model._alt_model = self\n            self._alt_model = new_model\n\n            duration = (datetime.now() - start).total_seconds()\n            log(f'    &gt; Done in {duration:.3f}s')\n\n        return self._alt_model\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.cpu_model","title":"<code>cpu_model: Model</code>  <code>property</code>","text":"<p>The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.</p>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.gpu_model","title":"<code>gpu_model: Model</code>  <code>property</code>","text":"<p>The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.</p>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.get_coords","title":"<code>get_coords(items)</code>","text":"<p>Function to get the coordinate (on the state_grid) for the provided state index or indices.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>int | list[int]</code> <p>The states ids or id get convert to a 2D coordinate.</p> required <p>Returns:</p> Name Type Description <code>item_coords</code> <code>list[int] | list[list[int]]</code> <p>The 2D positions of the provided item ids.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>def get_coords(self, items:Union[int,list]) -&gt; list[list[int]] | list[int]:\n    '''\n    Function to get the coordinate (on the state_grid) for the provided state index or indices.\n\n    Parameters\n    ----------\n    items : int | list[int]\n        The states ids or id get convert to a 2D coordinate.\n\n    Returns\n    -------\n    item_coords : list[int] | list[list[int]]\n        The 2D positions of the provided item ids.\n    '''\n    item_list = [items] if isinstance(items, int) else items\n    item_coords = [np.argwhere(self.cpu_model.state_grid == s)[0] for s in item_list]\n\n    return item_coords[0] if isinstance(items, int) else item_coords\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.load_from_file","title":"<code>load_from_file(file)</code>  <code>classmethod</code>","text":"<p>Function to load a MDP model from a pickle file. The json structure must contain the same items as in the constructor of this class.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file and path of the model to be loaded.</p> required <p>Returns:</p> Name Type Description <code>loaded_model</code> <code>Model</code> <p>An instance of the loaded model.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>@classmethod\ndef load_from_file(cls, file:str) -&gt; 'Model':\n    '''\n    Function to load a MDP model from a pickle file. The json structure must contain the same items as in the constructor of this class.\n\n    Parameters\n    ----------\n    file : str\n        The file and path of the model to be loaded.\n\n    Returns\n    -------\n    loaded_model : mdp.Model\n        An instance of the loaded model.\n    '''\n    with open(file, 'rb') as openfile:\n        loaded_model = pickle.load(openfile)\n\n    return loaded_model\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.reward","title":"<code>reward(s, a, s_p)</code>","text":"<p>Returns the rewards of playing action a when in state s and landing in state s_p. If the rewards are probabilistic, it will return 0 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>The current state</p> required <code>a</code> <code>int</code> <p>The action taking in state s</p> required <code>s_p</code> <code>int</code> <p>The state landing in after taking action a in state s</p> required <p>Returns:</p> Name Type Description <code>reward</code> <code>int or float</code> <p>The reward received.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>def reward(self, s:int, a:int, s_p:int) -&gt; Union[int,float]:\n    '''\n    Returns the rewards of playing action a when in state s and landing in state s_p.\n    If the rewards are probabilistic, it will return 0 or 1.\n\n    Parameters\n    ----------\n    s : int\n        The current state\n    a : int\n        The action taking in state s\n    s_p : int\n        The state landing in after taking action a in state s\n\n    Returns\n    -------\n    reward : int or float\n        The reward received.\n    '''\n    reward = float(self.immediate_reward_table[s,a,s_p] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p))\n    if self.rewards_are_probabilistic:\n        rnd = random.random()\n        return 1 if rnd &lt; reward else 0\n    else:\n        return reward\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.save","title":"<code>save(file_name, path='./Models')</code>","text":"<p>Function to save the current model in a pickle file. By default, the model will be saved in 'Models' directory in the current working directory but this can be changed using the 'path' parameter.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the json file the model will be saved in.</p> required <code>path</code> <code>str</code> <p>The path at which the model will be saved.</p> <code>'./Models'</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>def save(self, file_name:str, path:str='./Models') -&gt; None:\n    '''\n    Function to save the current model in a pickle file.\n    By default, the model will be saved in 'Models' directory in the current working directory but this can be changed using the 'path' parameter.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the json file the model will be saved in.\n    path : str, default='./Models'\n        The path at which the model will be saved.\n    '''\n    if not os.path.exists(path):\n        print('Folder does not exist yet, creating it...')\n        os.makedirs(path)\n\n    if not file_name.endswith('.pck'):\n        file_name += '.pck'\n\n    # Writing the cpu version of the file to a pickle file\n    with open(path + '/' + file_name, 'wb') as f:\n        pickle.dump(self.cpu_model, f)\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.Model.transition","title":"<code>transition(s, a)</code>","text":"<p>Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>The current state</p> required <code>a</code> <code>int</code> <p>The action to take</p> required <p>Returns:</p> Name Type Description <code>s_p</code> <code>int</code> <p>The posterior state</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>def transition(self, s:int, a:int) -&gt; int:\n    '''\n    Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.\n\n    Parameters\n    ----------\n    s : int \n        The current state\n    a : int\n        The action to take\n\n    Returns\n    -------\n    s_p : int\n        The posterior state\n    '''\n    xp = cp if self.is_on_gpu else np\n\n    # Shortcut for deterministic systems\n    if self.reachable_state_count == 1:\n        return int(self.reachable_states[s,a,0])\n\n    s_p = int(xp.random.choice(a=self.reachable_states[s,a], size=1, p=self.reachable_probabilities[s,a])[0])\n    return s_p\n</code></pre>"},{"location":"reference/agents/model_based_util/mdp/#olfactory_navigation.agents.model_based_util.mdp.log","title":"<code>log(content)</code>","text":"<p>Function to print a log line with a timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to be printed as a log.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\mdp.py</code> <pre><code>def log(content:str) -&gt; None:\n    '''\n    Function to print a log line with a timestamp.\n\n    Parameters\n    ----------\n    content : str\n        The content to be printed as a log.\n    '''\n    print(f'[{datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")}] ' + content)\n</code></pre>"},{"location":"reference/agents/model_based_util/pomdp/","title":"pomdp","text":""},{"location":"reference/agents/model_based_util/pomdp/#olfactory_navigation.agents.model_based_util.pomdp.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>Model</code></p> <p>POMDP Model class. Partially Observable Markov Decision Process Model.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>int or list[str] or list[list[str]]</code> <p>A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.</p> required <code>actions</code> <code>int or list</code> <p>A list of action labels or an amount of actions to be used.</p> required <code>observations</code> <code>int or list</code> <p>A list of observation labels or an amount of observations to be used</p> required <code>transitions</code> <code>array - like or function</code> <p>The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided.  If a function is provided, it has be able to deal with np.array arguments. If none is provided, it will be randomly generated.</p> <code>None</code> <code>reachable_states</code> <code>array - like</code> <p>A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair. It is optional but useful for speedup purposes.</p> <code>None</code> <code>rewards</code> <code>array - like or function</code> <p>The reward matrix, has to be |S| x |A| x |S|. A function can also be provided here but it has to be able to deal with np.array arguments. If provided, it will be use in combination with the transition matrix to fill to expected rewards.</p> <code>None</code> <code>observation_table</code> <code>array - like or function</code> <p>The observation matrix, has to be |S| x |A| x |O|. If none is provided, it will be randomly generated.</p> <code>None</code> <code>rewards_are_probabilistic</code> <code>bool</code> <p>Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.</p> <code>False</code> <code>state_grid</code> <code>array - like</code> <p>If provided, the model will be converted to a grid model.</p> <code>None</code> <code>start_probabilities</code> <code>list</code> <p>The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state. It is also used to represent a belief of complete uncertainty.</p> <code>None</code> <code>end_states</code> <code>list</code> <p>Entering either state in the list during a simulation will end the simulation.</p> <code>[]</code> <code>end_actions</code> <code>list</code> <p>Playing action of the list during a simulation will end the simulation.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>states</code> <code>ndarray</code> <p>A 1D array of states indices. Used to loop over states.</p> <code>state_labels</code> <code>list[str]</code> <p>A list of state labels. (To be mainly used for plotting)</p> <code>state_count</code> <code>int</code> <p>How many states are in the Model.</p> <code>state_grid</code> <code>ndarray</code> <p>The state indices organized as a 2D grid. (Used for plotting purposes)</p> <code>actions</code> <code>ndarry</code> <p>A 1D array of action indices. Used to loop over actions.</p> <code>action_labels</code> <code>list[str]</code> <p>A list of action labels. (To be mainly used for plotting)</p> <code>action_count</code> <code>int</code> <p>How many action are in the Model.</p> <code>observations</code> <code>ndarray</code> <p>A 1D array of observation indices. Used to loop over obervations.</p> <code>observation_labels</code> <code>list[str]</code> <p>A list of observation labels. (To be mainly used for plotting)</p> <code>observation_count</code> <code>int</code> <p>How many observations can be made in the Model.</p> <code>transition_table</code> <code>ndarray</code> <p>A 3D matrix of the transition probabilities. Can be None in the case a transition function is provided instead. Note: When possible, use reachable states and reachable probabilities instead.</p> <code>transition_function</code> <code>function</code> <p>A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0. Can be None in the case a transition table is provided instead. Note: When possible, use reachable states and reachable probabilities instead.</p> <code>observation_table</code> <code>ndarray</code> <p>A 3D matrix of shape S x A x O representing the probabilies of obsevating o when taking action a and leading to state s_p.</p> <code>reachable_states</code> <code>ndarray</code> <p>A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.</p> <code>reachable_probabilities</code> <code>ndarray</code> <p>A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.</p> <code>reachable_state_count</code> <code>int</code> <p>The maximum of states that can be reached from any state-action combination.</p> <code>reachable_transitional_observation_table</code> <code>ndarray</code> <p>A 4D array of shape S x A x O x R, representing the probabiliies of landing if each reachable state r, while observing o after having taken action a from state s. Mainly used to speedup repeated operations in solver.</p> <code>immediate_reward_table</code> <code>ndarray</code> <p>A 3D matrix of shape S x A x S x O of the reward that will received when taking action a, in state s, landing in state s_p, and observing o. Can be None in the case an immediate rewards function is provided instead.</p> <code>immediate_reward_function</code> <code>function</code> <p>A callable function taking 4 argments: s, a, s_p, o and returning the immediate reward the agent will receive. Can be None in the case an immediate rewards function is provided instead.</p> <code>expected_reward_table</code> <code>ndarray</code> <p>A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s. It is made by taking the weighted average of immediate rewards with the transitions and the observation probabilities.</p> <code>start_probabilities</code> <code>ndarray</code> <p>A 1D array of length |S| containing the probility distribution of the agent starting in each state.</p> <code>rewards_are_probabilisitic</code> <code>bool</code> <p>Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.</p> <code>end_states</code> <code>list[int]</code> <p>A list of states that, when reached, terminate a simulation.</p> <code>end_actions</code> <code>list[int]</code> <p>A list of actions that, when taken, terminate a simulation.</p> <code>is_on_gpu</code> <code>bool</code> <p>Whether the numpy array of the model are stored on the gpu or not.</p> <code>gpu_model</code> <code>Model</code> <p>An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)</p> <code>cpu_model</code> <code>Model</code> <p>An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\pomdp.py</code> <pre><code>class Model(MDP_Model):\n    '''\n    POMDP Model class. Partially Observable Markov Decision Process Model.\n\n    ...\n\n    Parameters\n    ----------\n    states : int or list[str] or list[list[str]]\n        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.\n    actions : int or list\n        A list of action labels or an amount of actions to be used.\n    observations : int or list\n        A list of observation labels or an amount of observations to be used\n    transitions : array-like or function, optional\n        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. \n        If a function is provided, it has be able to deal with np.array arguments.\n        If none is provided, it will be randomly generated.\n    reachable_states : array-like, optional\n        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.\n        It is optional but useful for speedup purposes.\n    rewards : array-like or function, optional\n        The reward matrix, has to be |S| x |A| x |S|.\n        A function can also be provided here but it has to be able to deal with np.array arguments.\n        If provided, it will be use in combination with the transition matrix to fill to expected rewards.\n    observation_table : array-like or function, optional\n        The observation matrix, has to be |S| x |A| x |O|. If none is provided, it will be randomly generated.\n    rewards_are_probabilistic: bool, default=False\n        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.\n    state_grid : array-like, optional\n        If provided, the model will be converted to a grid model.\n    start_probabilities : list, optional\n        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state. It is also used to represent a belief of complete uncertainty.\n    end_states : list, optional\n        Entering either state in the list during a simulation will end the simulation.\n    end_actions : list, optional\n        Playing action of the list during a simulation will end the simulation.\n\n    Attributes\n    ----------\n    states : np.ndarray\n        A 1D array of states indices. Used to loop over states.\n    state_labels : list[str]\n        A list of state labels. (To be mainly used for plotting)\n    state_count : int\n        How many states are in the Model.\n    state_grid : np.ndarray\n        The state indices organized as a 2D grid. (Used for plotting purposes)\n    actions : np.ndarry\n        A 1D array of action indices. Used to loop over actions.\n    action_labels : list[str]\n        A list of action labels. (To be mainly used for plotting)\n    action_count : int\n        How many action are in the Model.\n    observations : np.ndarray\n        A 1D array of observation indices. Used to loop over obervations.\n    observation_labels : list[str]\n        A list of observation labels. (To be mainly used for plotting)\n    observation_count : int\n        How many observations can be made in the Model.\n    transition_table : np.ndarray\n        A 3D matrix of the transition probabilities.\n        Can be None in the case a transition function is provided instead.\n        Note: When possible, use reachable states and reachable probabilities instead.\n    transition_function : function\n        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.\n        Can be None in the case a transition table is provided instead.\n        Note: When possible, use reachable states and reachable probabilities instead.\n    observation_table : np.ndarray\n        A 3D matrix of shape S x A x O representing the probabilies of obsevating o when taking action a and leading to state s_p.\n    reachable_states : np.ndarray\n        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.\n    reachable_probabilities : np.ndarray\n        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.\n    reachable_state_count : int\n        The maximum of states that can be reached from any state-action combination.\n    reachable_transitional_observation_table : np.ndarray\n        A 4D array of shape S x A x O x R, representing the probabiliies of landing if each reachable state r, while observing o after having taken action a from state s.\n        Mainly used to speedup repeated operations in solver.\n    immediate_reward_table : np.ndarray\n        A 3D matrix of shape S x A x S x O of the reward that will received when taking action a, in state s, landing in state s_p, and observing o.\n        Can be None in the case an immediate rewards function is provided instead.\n    immediate_reward_function : function\n        A callable function taking 4 argments: s, a, s_p, o and returning the immediate reward the agent will receive.\n        Can be None in the case an immediate rewards function is provided instead.\n    expected_reward_table : np.ndarray\n        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.\n        It is made by taking the weighted average of immediate rewards with the transitions and the observation probabilities.\n    start_probabilities : np.ndarray\n        A 1D array of length |S| containing the probility distribution of the agent starting in each state.\n    rewards_are_probabilisitic : bool\n        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.\n    end_states : list[int]\n        A list of states that, when reached, terminate a simulation.\n    end_actions : list[int]\n        A list of actions that, when taken, terminate a simulation.\n    is_on_gpu : bool\n        Whether the numpy array of the model are stored on the gpu or not.\n    gpu_model : mdp.Model\n        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)\n    cpu_model : mdp.Model\n        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)\n    '''\n\n    def __init__(self,\n                 states:Union[int, list[str], list[list[str]]],\n                 actions:Union[int, list],\n                 observations:Union[int, list],\n                 transitions=None,\n                 reachable_states=None,\n                 rewards=None,\n                 observation_table=None,\n                 rewards_are_probabilistic:bool=False,\n                 state_grid=None,\n                 start_probabilities:Union[list,None]=None,\n                 end_states:list[int]=[],\n                 end_actions:list[int]=[]\n                 ):\n\n        super().__init__(states=states,\n                         actions=actions,\n                         transitions=transitions,\n                         reachable_states=reachable_states,\n                         rewards=-1, # Defined here lower since immediate reward table has different shape for MDP is different than for POMDP\n                         rewards_are_probabilistic=rewards_are_probabilistic,\n                         state_grid=state_grid,\n                         start_probabilities=start_probabilities,\n                         end_states=end_states,\n                         end_actions=end_actions)\n\n        print()\n        log('POMDP particular parameters:')\n\n        self.movement_vector = np.array([\n            [-1,  0], # North\n            [ 0,  1], # East\n            [ 1,  0], # South\n            [ 0, -1]  # West\n        ])\n\n        # ------------------------- Observations -------------------------\n        if isinstance(observations, int):\n            self.observation_labels = [f'o_{i}' for i in range(observations)]\n        else:\n            self.observation_labels = observations\n        self.observation_count = len(self.observation_labels)\n        self.observations = np.arange(self.observation_count)\n\n        if observation_table is None:\n            # If no observation matrix given, generate random one\n            random_probs = np.random.rand(self.state_count, self.action_count, self.observation_count)\n            # Normalization to have s_p probabilies summing to 1\n            self.observation_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)\n        else:\n            self.observation_table = np.array(observation_table)\n            o_shape = self.observation_table.shape\n            exp_shape = (self.state_count, self.action_count, self.observation_count)\n            assert o_shape == exp_shape, f\"Observations table doesnt have the right shape, it should be SxAxO (expected: {exp_shape}, received: {o_shape}).\"\n\n        log(f'- {self.observation_count} observations')\n\n        # ------------------------- Reachable transitional observation probabilities -------------------------\n        log('- Starting of transitional observations for reachable states table')\n        start_ts = datetime.now()\n\n        reachable_observations = self.observation_table[self.reachable_states[:,:,None,:], self.actions[None,:,None,None], self.observations[None,None,:,None]] # SAOR\n        self.reachable_transitional_observation_table = np.einsum('sar,saor-&gt;saor', self.reachable_probabilities, reachable_observations)\n\n        duration = (datetime.now() - start_ts).total_seconds()\n        log(f'    &gt; Done in {duration:.3f}s')\n\n        # ------------------------- Rewards -------------------------\n        self.immediate_reward_table = None\n        self.immediate_reward_function = None\n\n        if rewards is None:\n            if (len(self.end_states) &gt; 0) or (len(self.end_actions) &gt; 0):\n                log('- [Warning] Rewards are not define but end states/actions are, reaching an end state or doing an end action will give a reward of 1.')\n                self.immediate_reward_function = self._end_reward_function\n            else:\n                # If no reward matrix given, generate random one\n                self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count, self.observation_count)\n        elif callable(rewards):\n            # Rewards is a function\n            log('- [Warning] The rewards are provided as a function, if the model is saved, the rewards will need to be defined before loading model.')\n            log('    &gt; Alternative: Setting end states/actions and leaving the rewards can be done to make the end states/action giving a reward of 1 by default.')\n            self.immediate_reward_function = rewards\n            assert len(signature(rewards).parameters) == 4, \"Reward function should accept 4 parameters: s, a, sn, o...\"\n        else:\n            # Array like\n            self.immediate_reward_table = np.array(rewards)\n            r_shape = self.immediate_reward_table.shape\n            exp_shape = (self.state_count, self.action_count, self.state_count, self.observation_count)\n            assert r_shape == exp_shape, f\"Rewards table doesnt have the right shape, it should be SxAxSxO (expected: {exp_shape}, received {r_shape})\"\n\n        # ------------------------- Expected rewards -------------------------\n        log('- Starting generation of expected rewards table')\n        start_ts = datetime.now()\n\n        reachable_rewards = None\n        if self.immediate_reward_table is not None:\n            reachable_rewards = rewards[self.states[:,None,None,None], self.actions[None,:,None,None], self.reachable_states[:,:,:,None], self.observations[None,None,None,:]]\n        else:\n            def reach_reward_func(s,a,ri,o):\n                s = s.astype(int)\n                a = a.astype(int)\n                ri = ri.astype(int)\n                o = o.astype(int)\n                return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri],o)\n\n            reachable_rewards = np.fromfunction(reach_reward_func, (*self.reachable_states.shape, self.observation_count))\n\n        self._min_reward = float(np.min(reachable_rewards))\n        self._max_reward = float(np.max(reachable_rewards))\n\n        self.expected_rewards_table = np.einsum('saor,saro-&gt;sa', self.reachable_transitional_observation_table, reachable_rewards)\n\n        duration = (datetime.now() - start_ts).total_seconds()\n        log(f'    &gt; Done in {duration:.3f}s')\n\n\n    @classmethod\n    def from_environment(cls,\n                         environment:Environment,\n                         threshold:float|list\n                         ) -&gt; 'Model':\n        '''\n        Method to create a POMDP model based on an olfactory environment object.\n        # TODO: Implement different action sets\n\n        Parameters\n        ----------\n        environment : Environment\n            The olfactory environment object to create the POMDP model from.\n        threshold : float or list\n            A threshold for the odor cues.\n            If a single is provided, the agent will smell something when an odor is above the threshold and nothing when it is bellow.\n            If a list is provided, the agent will able to distinguish different levels of smell.\n        '''\n        state_count = np.prod(environment.shape)\n\n        state_grid = [[f's_{x}_{y}' for x in range(environment.shape[1])] for y in range(environment.shape[0])]\n        end_states = np.argwhere(np.fromfunction(lambda x,y: ((x-environment.source_position[0])**2 + (y-environment.source_position[1])**2) &lt;= environment.source_radius**2,\n                                                 shape=environment.shape).ravel())[:,0].tolist()\n\n        # Compute observation matrix\n        if not isinstance(threshold, list):\n            threshold = [threshold]\n\n        # Ensure 0.0 and 1.0 begin and end the threshold list\n        if threshold[0] != -np.inf:\n            threshold = [-np.inf] + threshold\n\n        if threshold[-1] != np.inf:\n            threshold = threshold + [np.inf]\n\n        # Computing odor probabilities\n        grid = environment.grid[:,:,:,None]\n        threshs = np.array(threshold)\n        odor_fields = np.average(((grid &gt;= threshs[:-1][None,None,None,:]) &amp; (grid &lt; threshs[1:][None,None,None,:])), axis=0)\n\n        # Building observation matrix\n        observations = np.empty((state_count, 4, len(threshold)), dtype=float) # 4-actions, observations: |thresholds|-1 + goal \n\n        for i in range(len(threshold)-1):\n            observations[:,:,i] = odor_fields[:,:,i].ravel()[:,None]\n\n        # Goal observation\n        observations[:,:,-1] = 0.0\n        observations[end_states,:,:] = 0.0\n        observations[end_states,:,-1] = 1.0\n\n        # Observation labels\n        observation_labels = ['nothing']\n        if len(threshold) &gt; 3:\n            for i,_ in enumerate(threshold[1:-1]):\n                observation_labels.append(f'something_l{i}')\n        else:\n            observation_labels.append('something')\n        observation_labels.append('goal')\n\n        # Compute reachable states\n        row_w = environment.shape[1]\n\n        reachable_states = np.zeros((state_count, 4, 1), dtype=int)\n        for s in range(state_count):\n            reachable_states[s,0,0] = s - row_w if s - row_w &gt;= 0 else (state_count - row_w) + s # North\n            reachable_states[s,1,0] = s + 1 if (s + 1) % row_w &gt; 0 else s # East\n            reachable_states[s,2,0] = s + row_w if s + row_w &lt; state_count else s % row_w # South\n            reachable_states[s,3,0] = s - 1 if (s - 1) % row_w &lt; (row_w - 1) else s # West\n\n        reachable_states = np.array(reachable_states)\n\n        # Instantiate the model object\n        model = Model(\n            states=state_grid,\n            actions=['N','E','S','W'],\n            observations=observation_labels,\n            reachable_states=reachable_states,\n            observation_table=observations,\n            end_states=end_states,\n            start_probabilities=environment.start_probabilities.ravel()\n        )\n        return model\n\n\n    def _end_reward_function(self, s, a, sn, o):\n        '''\n        The default reward function.\n        Returns 1 if the next state sn is in the end states or if the action is in the end actions (terminating actions)\n        '''\n        return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)\n\n\n    def reward(self, s:int, a:int, s_p:int, o:int) -&gt; Union[int,float]:\n        '''\n        Returns the rewards of playing action a when in state s and landing in state s_p.\n        If the rewards are probabilistic, it will return 0 or 1.\n\n        Parameters\n        ----------\n        s : int\n            The current state.\n        a : int\n            The action taking in state s.\n        s_p : int\n            The state landing in after taking action a in state s\n        o : int\n            The observation that is done after having played action a in state s and landing in s_p\n\n        Returns\n        -------\n        reward : int or float\n            The reward received.\n        '''\n        reward = float(self.immediate_reward_table[s,a,s_p,o] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p,o))\n        if self.rewards_are_probabilistic:\n            rnd = random.random()\n            return 1 if rnd &lt; reward else 0\n        else:\n            return reward\n\n\n    def observe(self, s_p:int, a:int) -&gt; int:\n        '''\n        Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.\n\n        Parameters\n        ----------\n        s_p : int\n            The state landed on after having done action a.\n        a : int\n            The action to take.\n\n        Returns\n        -------\n        o : int\n            A random observation.\n        '''\n        xp = cp if self.is_on_gpu else np\n        o = int(xp.random.choice(a=self.observations, size=1, p=self.observation_table[s_p,a])[0])\n        return o\n</code></pre>"},{"location":"reference/agents/model_based_util/pomdp/#olfactory_navigation.agents.model_based_util.pomdp.Model.from_environment","title":"<code>from_environment(environment, threshold)</code>  <code>classmethod</code>","text":"<p>Method to create a POMDP model based on an olfactory environment object.</p>"},{"location":"reference/agents/model_based_util/pomdp/#olfactory_navigation.agents.model_based_util.pomdp.Model.from_environment--todo-implement-different-action-sets","title":"TODO: Implement different action sets","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>The olfactory environment object to create the POMDP model from.</p> required <code>threshold</code> <code>float or list</code> <p>A threshold for the odor cues. If a single is provided, the agent will smell something when an odor is above the threshold and nothing when it is bellow. If a list is provided, the agent will able to distinguish different levels of smell.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\pomdp.py</code> <pre><code>@classmethod\ndef from_environment(cls,\n                     environment:Environment,\n                     threshold:float|list\n                     ) -&gt; 'Model':\n    '''\n    Method to create a POMDP model based on an olfactory environment object.\n    # TODO: Implement different action sets\n\n    Parameters\n    ----------\n    environment : Environment\n        The olfactory environment object to create the POMDP model from.\n    threshold : float or list\n        A threshold for the odor cues.\n        If a single is provided, the agent will smell something when an odor is above the threshold and nothing when it is bellow.\n        If a list is provided, the agent will able to distinguish different levels of smell.\n    '''\n    state_count = np.prod(environment.shape)\n\n    state_grid = [[f's_{x}_{y}' for x in range(environment.shape[1])] for y in range(environment.shape[0])]\n    end_states = np.argwhere(np.fromfunction(lambda x,y: ((x-environment.source_position[0])**2 + (y-environment.source_position[1])**2) &lt;= environment.source_radius**2,\n                                             shape=environment.shape).ravel())[:,0].tolist()\n\n    # Compute observation matrix\n    if not isinstance(threshold, list):\n        threshold = [threshold]\n\n    # Ensure 0.0 and 1.0 begin and end the threshold list\n    if threshold[0] != -np.inf:\n        threshold = [-np.inf] + threshold\n\n    if threshold[-1] != np.inf:\n        threshold = threshold + [np.inf]\n\n    # Computing odor probabilities\n    grid = environment.grid[:,:,:,None]\n    threshs = np.array(threshold)\n    odor_fields = np.average(((grid &gt;= threshs[:-1][None,None,None,:]) &amp; (grid &lt; threshs[1:][None,None,None,:])), axis=0)\n\n    # Building observation matrix\n    observations = np.empty((state_count, 4, len(threshold)), dtype=float) # 4-actions, observations: |thresholds|-1 + goal \n\n    for i in range(len(threshold)-1):\n        observations[:,:,i] = odor_fields[:,:,i].ravel()[:,None]\n\n    # Goal observation\n    observations[:,:,-1] = 0.0\n    observations[end_states,:,:] = 0.0\n    observations[end_states,:,-1] = 1.0\n\n    # Observation labels\n    observation_labels = ['nothing']\n    if len(threshold) &gt; 3:\n        for i,_ in enumerate(threshold[1:-1]):\n            observation_labels.append(f'something_l{i}')\n    else:\n        observation_labels.append('something')\n    observation_labels.append('goal')\n\n    # Compute reachable states\n    row_w = environment.shape[1]\n\n    reachable_states = np.zeros((state_count, 4, 1), dtype=int)\n    for s in range(state_count):\n        reachable_states[s,0,0] = s - row_w if s - row_w &gt;= 0 else (state_count - row_w) + s # North\n        reachable_states[s,1,0] = s + 1 if (s + 1) % row_w &gt; 0 else s # East\n        reachable_states[s,2,0] = s + row_w if s + row_w &lt; state_count else s % row_w # South\n        reachable_states[s,3,0] = s - 1 if (s - 1) % row_w &lt; (row_w - 1) else s # West\n\n    reachable_states = np.array(reachable_states)\n\n    # Instantiate the model object\n    model = Model(\n        states=state_grid,\n        actions=['N','E','S','W'],\n        observations=observation_labels,\n        reachable_states=reachable_states,\n        observation_table=observations,\n        end_states=end_states,\n        start_probabilities=environment.start_probabilities.ravel()\n    )\n    return model\n</code></pre>"},{"location":"reference/agents/model_based_util/pomdp/#olfactory_navigation.agents.model_based_util.pomdp.Model.observe","title":"<code>observe(s_p, a)</code>","text":"<p>Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>s_p</code> <code>int</code> <p>The state landed on after having done action a.</p> required <code>a</code> <code>int</code> <p>The action to take.</p> required <p>Returns:</p> Name Type Description <code>o</code> <code>int</code> <p>A random observation.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\pomdp.py</code> <pre><code>def observe(self, s_p:int, a:int) -&gt; int:\n    '''\n    Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.\n\n    Parameters\n    ----------\n    s_p : int\n        The state landed on after having done action a.\n    a : int\n        The action to take.\n\n    Returns\n    -------\n    o : int\n        A random observation.\n    '''\n    xp = cp if self.is_on_gpu else np\n    o = int(xp.random.choice(a=self.observations, size=1, p=self.observation_table[s_p,a])[0])\n    return o\n</code></pre>"},{"location":"reference/agents/model_based_util/pomdp/#olfactory_navigation.agents.model_based_util.pomdp.Model.reward","title":"<code>reward(s, a, s_p, o)</code>","text":"<p>Returns the rewards of playing action a when in state s and landing in state s_p. If the rewards are probabilistic, it will return 0 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>int</code> <p>The current state.</p> required <code>a</code> <code>int</code> <p>The action taking in state s.</p> required <code>s_p</code> <code>int</code> <p>The state landing in after taking action a in state s</p> required <code>o</code> <code>int</code> <p>The observation that is done after having played action a in state s and landing in s_p</p> required <p>Returns:</p> Name Type Description <code>reward</code> <code>int or float</code> <p>The reward received.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\pomdp.py</code> <pre><code>def reward(self, s:int, a:int, s_p:int, o:int) -&gt; Union[int,float]:\n    '''\n    Returns the rewards of playing action a when in state s and landing in state s_p.\n    If the rewards are probabilistic, it will return 0 or 1.\n\n    Parameters\n    ----------\n    s : int\n        The current state.\n    a : int\n        The action taking in state s.\n    s_p : int\n        The state landing in after taking action a in state s\n    o : int\n        The observation that is done after having played action a in state s and landing in s_p\n\n    Returns\n    -------\n    reward : int or float\n        The reward received.\n    '''\n    reward = float(self.immediate_reward_table[s,a,s_p,o] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p,o))\n    if self.rewards_are_probabilistic:\n        rnd = random.random()\n        return 1 if rnd &lt; reward else 0\n    else:\n        return reward\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/","title":"value_function","text":""},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.AlphaVector","title":"<code>AlphaVector</code>","text":"<p>A class to represent an Alpha Vector, a vector representing a plane in |S| dimension for POMDP models.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>The actual vector with the value for each state.</p> required <code>action</code> <code>int</code> <p>The action associated with the vector.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>class AlphaVector:\n    '''\n    A class to represent an Alpha Vector, a vector representing a plane in |S| dimension for POMDP models.\n\n    ...\n\n    Parameters\n    ----------\n    values : np.ndarray\n        The actual vector with the value for each state.\n    action : int\n        The action associated with the vector.\n    '''\n    def __init__(self, values:np.ndarray, action:int) -&gt; None:\n        self.values = values\n        self.action = int(action)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction","title":"<code>ValueFunction</code>","text":"<p>Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model the value function is associated with.</p> required <code>alpha_vectors</code> <code>list[AlphaVector] or ndarray</code> <p>The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.</p> <code>[]</code> <code>action_list</code> <code>list[int] or ndarray</code> <p>The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The model the value function is associated with.</p> <code>alpha_vector_list</code> <code>list[AlphaVector]</code> <code>alpha_vector_array</code> <code>ndarray</code> <code>actions</code> <code>ndarray</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>class ValueFunction:\n    '''\n    Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.\n\n    ...\n\n    Parameters\n    ----------\n    model : mdp.Model\n        The model the value function is associated with.\n    alpha_vectors : list[AlphaVector] or np.ndarray, optional\n        The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.\n    action_list : list[int] or np.ndarray, optional\n        The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.\n\n    Attributes\n    ----------\n    model : mdp.Model\n        The model the value function is associated with.\n    alpha_vector_list : list[AlphaVector]\n    alpha_vector_array : np.ndarray\n    actions : np.ndarray\n    '''\n    def __init__(self, model:Model, alpha_vectors:Union[list[AlphaVector], np.ndarray]=[], action_list:Union[list[int], np.ndarray]=[]):\n        self.model = model\n\n        self._vector_list = None\n        self._vector_array = None\n        self._actions = None\n\n        self.is_on_gpu = False\n\n        # List of alpha vectors\n        if isinstance(alpha_vectors, list):\n            assert all(v.values.shape[0] == model.state_count for v in alpha_vectors), f\"Some or all alpha vectors in the list provided dont have the right size, they should be of shape: {model.state_count}\"\n            self._vector_list = alpha_vectors\n\n            # Check if on gpu and make sure all vectors are also on the gpu\n            if (len(alpha_vectors) &gt; 0) and gpu_support and cp.get_array_module(alpha_vectors[0].values) == cp:\n                assert all(cp.get_array_module(v.values) == cp for v in alpha_vectors), \"Either all or none of the alpha vectors should be on the GPU, not just some.\"\n                self.is_on_gpu = True\n\n        # As numpy array\n        else:\n            av_shape = alpha_vectors.shape\n            exp_shape = (len(action_list), model.state_count)\n            assert av_shape == exp_shape, f\"Alpha vector array does not have the right shape (received: {av_shape}; expected: {exp_shape})\"\n\n            self._vector_list = []\n            for alpha_vect, action in zip(alpha_vectors, action_list):\n                self._vector_list.append(AlphaVector(alpha_vect, action))\n\n            # Check if array is on gpu\n            if gpu_support and cp.get_array_module(alpha_vectors) == cp:\n                self.is_on_gpu = True\n\n        # Deduplication\n        self._uniqueness_dict = {alpha_vector.values.tobytes(): alpha_vector for alpha_vector in self._vector_list}\n        self._vector_list = list(self._uniqueness_dict.values())\n\n        self._pruning_level = 1\n\n\n    @property\n    def alpha_vector_list(self) -&gt; list[AlphaVector]:\n        '''\n        A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.\n        '''\n        if self._vector_list is None:\n            self._vector_list = []\n            for alpha_vect, action in zip(self._vector_array, self._actions):\n                self._vector_list.append(AlphaVector(alpha_vect, action))\n        return self._vector_list\n\n\n    @property\n    def alpha_vector_array(self) -&gt; np.ndarray:\n        '''\n        A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)\n        If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._vector_array is None:\n            self._vector_array = xp.array([v.values for v in self._vector_list])\n            self._actions = xp.array([v.action for v in self._vector_list])\n        return self._vector_array\n\n\n    @property\n    def actions(self) -&gt; np.ndarray:\n        '''\n        A list of N actions corresponding to the N alpha vectors making up the value function.\n        If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.\n        '''\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        if self._actions is None:\n            self._vector_array = xp.array(self._vector_list)\n            self._actions = xp.array([v.action for v in self._vector_list])\n        return self._actions\n\n\n    def __len__(self) -&gt; int:\n        return len(self._vector_list) if self._vector_list is not None else self._vector_array.shape[0]\n\n\n    def __add__(self, other_value_function:'Model') -&gt; 'Model':\n        # combined_dict = {**self._uniqueness_dict, **other_value_function._uniqueness_dict}\n        combined_dict = {}\n        combined_dict.update(self._uniqueness_dict)\n        combined_dict.update(other_value_function._uniqueness_dict)\n\n        # Instantiation of the new value function\n        new_value_function = super().__new__(self.__class__)\n        new_value_function.model = self.model\n        new_value_function.is_on_gpu = self.is_on_gpu\n\n        new_value_function._vector_list = list(combined_dict.values())\n        new_value_function._uniqueness_dict = combined_dict\n        new_value_function._pruning_level = 1\n\n        new_value_function._vector_array = None\n        new_value_function._actions = None\n\n        return new_value_function\n\n\n    def append(self, alpha_vector:AlphaVector) -&gt; None:\n        '''\n        Function to add an alpha vector to the value function.\n\n        Parameters\n        ----------\n        alpha_vector : AlphaVector\n            The alpha vector to be added to the value function.\n        '''\n        # Make sure size is correct\n        assert alpha_vector.values.shape[0] == self.model.state_count, f\"Vector to add to value function doesn't have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})\"\n\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n        assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f\"Vector is{' not' if self.is_on_gpu else ''} on GPU while value function is{'' if self.is_on_gpu else ' not'}.\"\n\n        if self._vector_array is not None:\n            self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)\n            self._actions = xp.append(self._actions, alpha_vector.action)\n\n        if self._vector_list is not None:\n            self._vector_list.append(alpha_vector)\n\n\n    def extend(self, other_value_function:'Model') -&gt; None:\n        '''\n        Function to add another value function is place.\n        Effectively, it performs the union of the two sets of alpha vectors.\n\n        Parameters\n        ----------\n        other_value_function : ValueFunction\n            The other side of the union.\n        '''\n        self._uniqueness_dict.update(other_value_function._uniqueness_dict)\n        self._vector_list = list(self._uniqueness_dict.values())\n\n        self._vector_array = None\n        self._actions = None\n\n        self._pruning_level = 1\n\n\n    def to_gpu(self) -&gt; 'ValueFunction':\n        '''\n        Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.\n\n        Returns\n        -------\n        gpu_value_function : ValueFunction\n            A new value function with arrays on GPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        gpu_model = self.model.gpu_model\n\n        gpu_value_function = None\n        if self._vector_list is not None:\n            gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]\n            gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)\n\n        else:\n            gpu_vector_array = cp.array(self._vector_array)\n            gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)\n            gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)\n\n        return gpu_value_function\n\n\n    def to_cpu(self) -&gt; 'ValueFunction':\n        '''\n        Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.\n\n        Returns\n        -------\n        cpu_value_function : ValueFunction\n            A new value function with arrays on CPU.\n        '''\n        assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n        cpu_model = self.model.cpu_model\n\n        cpu_value_function = None\n        if self._vector_list is not None:\n            cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]\n            cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)\n\n        else:\n            cpu_vector_array = cp.asnumpy(self._vector_array)\n            cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)\n            cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)\n\n        return cpu_value_function\n\n\n    def prune(self, level:int=1) -&gt; None:\n        '''\n        Function pruning the set of alpha vectors composing the value function.\n        The pruning is as thorough as the level:\n            - 2: 1+ Check of absolute domination (check if dominated at each state).\n            - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.\n\n        Note that the higher the level, the heavier the time impact will be.\n\n        Parameters\n        ----------\n        level : int, default=1\n            Between 0 and 3, how thorough the alpha vector pruning should be.\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        # Level 1 or under\n        if level &lt; self._pruning_level or level &gt; 3:\n            log('Attempting to prune a value function to a level already reached. Returning \\'self\\'')\n            return\n\n        # Level 2 pruning: Check for absolute domination\n        if level &gt;= 2 and self._pruning_level &lt; 2:\n            non_dominated_vector_indices = []\n\n            for i, v in enumerate(self.alpha_vector_array):\n                is_dom_by = xp.all(self.alpha_vector_array &gt;= v, axis=1)\n                if len(xp.where(is_dom_by)[0]) == 1:\n                    non_dominated_vector_indices.append(i)\n\n            self._vector_array = self._vector_array[non_dominated_vector_indices]\n            self._actions = self._actions[non_dominated_vector_indices]\n\n        # Level 3 pruning: LP to check for more complex domination\n        if level &gt;= 3:\n            assert ilp_support, \"ILP support not enabled...\"\n\n            pruned_alpha_set = pruned_alpha_set.to_cpu()\n\n            alpha_set = pruned_alpha_set.alpha_vector_array\n            non_dominated_vector_indices = []\n\n            for i, alpha_vect in enumerate(alpha_set):\n                other_alphas = alpha_set[:i] + alpha_set[(i+1):]\n\n                # Objective function\n                c = np.concatenate([np.array([1]), -1*alpha_vect])\n\n                # Alpha vector contraints\n                other_count = len(other_alphas)\n                A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]\n                alpha_constraints = LinearConstraint(A, 0, np.inf)\n\n                # Constraints that sum of beliefs is 1\n                belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)\n\n                # Solve problem\n                res = milp(c=c, constraints=[alpha_constraints, belief_constraint])\n\n                # Check if dominated\n                is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0\n                if is_dominated:\n                    print(alpha_vect)\n                    print(' -&gt; Dominated\\n')\n                else:\n                    non_dominated_vector_indices.append(i)\n\n            self._vector_array = self._vector_array[non_dominated_vector_indices]\n            self._actions = self._actions[non_dominated_vector_indices]\n\n        # Update the tracked pruned level so far\n        self._pruning_level = level\n\n\n    def evaluate_at(self, belief:Belief|BeliefSet) -&gt; tuple[float|np.ndarray, int|np.ndarray]:\n        '''\n        Function to evaluate the value function at a belief point or at a set of belief points.\n        It returns a value and the associated action.\n\n        Parameters\n        ----------\n        belief : Belief or BeliefSet\n\n        Returns\n        -------\n        value : float or np.ndarray\n            The largest value associated with the belief point(s)\n        action : int or np.ndarray\n            The action(s) associated with the vector having the highest values at the belief point(s).\n        '''\n        # GPU support check\n        xp = cp if (gpu_support and self.is_on_gpu) else np\n\n        best_value = None\n        best_action = None\n\n        if isinstance(belief, Belief):\n            # Computing values\n            belief_values = xp.dot(self.alpha_vector_array, belief.values)\n\n            # Selecting best vectors\n            best_vector = xp.argmax(belief_values)\n\n            # From best vector, compute the best value and action\n            best_value = float(belief_values[best_vector])\n            best_action = int(self.actions[best_vector])\n        else:\n            # Computing values\n            belief_values = xp.matmul(belief.values if isinstance(belief, Belief) else belief.belief_array, self.alpha_vector_array.T)\n\n            # Retrieving the top vectors according to the value function\n            best_vectors = xp.argmax(belief_values, axis=1)\n\n            # Retrieving the values and actions associated with the vectors chosen\n            best_value = belief_values[xp.arange(belief_values.shape[0]), best_vectors]\n            best_action = self.actions[best_vectors]\n\n        return (best_value, best_action)\n\n\n    def save(self,\n             folder:str='./ValueFunctions',\n             file_name:Union[str,None]=None\n             ) -&gt; None:\n        '''\n        Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.\n        If no file_name is provided, it be saved as '&lt;current_timestamp&gt;_value_function.csv'.\n\n        Parameters\n        ----------\n        folder : str, default='./ValueFunctions'\n            The path at which the npy file will be saved.\n        file_name : str, default='&lt;current_timestamp&gt;_value_function.npy'\n            The file name used to save in.\n        '''\n        if self.is_on_gpu:\n            self.to_cpu().save(path=folder, file_name=file_name)\n            return\n\n        # Handle file_name\n        if file_name is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            file_name = timestamp + '_value_function.npy'\n\n        # Make sure that .csv is in the file name\n        if '.npy' not in file_name:\n            file_name += '.npy'\n\n        # Getting array\n        av_array = np.hstack([self.actions[:,None], self.alpha_vector_array])\n\n        np.save(folder + '/' + file_name, av_array)\n\n\n    @classmethod\n    def load(cls,\n            file:str,\n            model:Model\n            ) -&gt; 'ValueFunction':\n        '''\n        Function to load the value function from a csv file.\n\n        Parameters\n        ----------\n        file : str\n            The path and file_name of the value function to be loaded.\n        model : mdp.Model\n            The model the value function is linked to.\n\n        Returns\n        -------\n        loaded_value_function : ValueFunction\n            The loaded value function.\n        '''\n        av_array = np.load(file)\n\n        return ValueFunction(model, alpha_vectors=av_array[:,1:], action_list=av_array[:,0].astype(int))\n\n\n    def plot(self,\n             as_grid:bool=False,\n             size:int=5,\n             belief_set:np.ndarray=None\n             ) -&gt; None:\n        '''\n        Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.\n        If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.\n\n        Parameters\n        ----------\n        as_grid : bool, default=False\n            Forces the plot to be plot as a grid.\n        size : int, default=5\n            The actual plot scale.\n        belief_set : np.ndarray, optional\n            A set of belief to plot the belief points that were explored.\n        '''\n        assert len(self) &gt; 0, \"Value function is empty, plotting is impossible...\"\n\n        # If on GPU, convert to CPU and plot that one\n        if self.is_on_gpu:\n            print('[Warning] Value function on GPU, converting to numpy before plotting...')\n            cpu_value_function = self.to_cpu()\n            cpu_value_function.plot(as_grid, size, belief_set)\n            return\n\n        func = None\n        if as_grid:\n            func = self._plot_grid\n        elif self.model.state_count == 2:\n            func = self._plot_2D\n        elif self.model.state_count == 3:\n            func = self._plot_3D\n        else:\n            print('[Warning] \\'as_grid\\' parameter set to False but state count is &gt;3 so it will be plotted as a grid')\n            func = self._plot_grid\n\n        func(size, belief_set)\n\n\n    def _plot_2D(self, size, belief_set=None):\n        x = np.linspace(0, 1, 100)\n\n        plt.figure(figsize=(int(size*1.5),size))\n        grid_spec = {'height_ratios': ([1] if belief_set is None else [19,1])}\n        _, ax = plt.subplots((2 if belief_set is not None else 1),1,sharex=True,gridspec_kw=grid_spec)\n\n        # Vector plotting\n        alpha_vects = self.alpha_vector_array\n\n        m = alpha_vects[:,1] - alpha_vects[:,0] # type: ignore\n        m = m.reshape(m.shape[0],1)\n\n        x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)\n        y = (m*x) + alpha_vects[:,0].reshape(m.shape[0],1)\n\n        ax0 = ax[0] if belief_set is not None else ax\n        for i, alpha in enumerate(self.alpha_vector_list):\n            ax0.plot(x[i,:], y[i,:], color=COLOR_LIST[alpha.action]['id']) # type: ignore\n\n        # Set title\n        title = 'Value function' + ('' if belief_set is None else ' and explored belief points')\n        ax0.set_title(title)\n\n        # X-axis setting\n        ticks = [0,0.25,0.5,0.75,1]\n        x_ticks = [str(t) for t in ticks]\n        x_ticks[0] = self.model.state_labels[0]\n        x_ticks[-1] = self.model.state_labels[1]\n\n        ax0.set_xticks(ticks, x_ticks) # type: ignore\n\n        # Action legend\n        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[a]['id']) for a in self.model.actions]\n        ax0.legend(proxy, self.model.action_labels, title='Actions') # type: ignore\n\n        # Belief plotting\n        if belief_set is not None:\n            beliefs_x = belief_set.belief_array[:,1]\n            ax[1].scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c='red')\n            ax[1].get_yaxis().set_visible(False)\n            ax[1].axhline(0, color='black')\n            ax[1].set_xlabel('Belief space')\n        else:\n            ax0.set_xlabel('Belief space')\n\n        # Axis labels\n        ax0.set_ylabel('V(b)')\n\n\n    def _plot_3D(self, size, belief_set=None):\n\n        def get_alpha_vect_z(xx, yy, alpha_vect):\n            x0, y0, z0 = [0, 0, alpha_vect[0]]\n            x1, y1, z1 = [1, 0, alpha_vect[1]]\n            x2, y2, z2 = [0, 1, alpha_vect[2]]\n\n            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]\n            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]\n\n            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]\n\n            point  = np.array([0, 0, alpha_vect[0]])\n            normal = np.array(u_cross_v)\n\n            d = -point.dot(normal)\n\n            z = (-normal[0] * xx - normal[1] * yy - d) * 1. / normal[2]\n\n            return z\n\n        def get_plane_gradient(alpha_vect):\n\n            x0, y0, z0 = [0, 0, alpha_vect[0]]\n            x1, y1, z1 = [1, 0, alpha_vect[1]]\n            x2, y2, z2 = [0, 1, alpha_vect[2]]\n\n            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]\n            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]\n\n            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]\n\n            normal_vector = np.array(u_cross_v)\n            normal_vector_norm = float(np.linalg.norm(normal_vector))\n            normal_vector = np.divide(normal_vector, normal_vector_norm)\n            normal_vector[2] = 0\n\n            return np.linalg.norm(normal_vector)\n\n        # Actual plotting\n        x = np.linspace(0, 1, 1000)\n        y = np.linspace(0, 1, 1000)\n\n        xx, yy = np.meshgrid(x, y)\n\n        max_z = np.zeros((xx.shape[0], yy.shape[0]))\n        best_a = (np.zeros((xx.shape[0], yy.shape[0])))\n        plane = (np.zeros((xx.shape[0], yy.shape[0])))\n        gradients = (np.zeros((xx.shape[0], yy.shape[0])))\n\n        for alpha in self.alpha_vector_list:\n\n            z = get_alpha_vect_z(xx, yy, alpha.values)\n\n            # Action array update\n            new_a_mask = np.argmax(np.array([max_z, z]), axis=0)\n\n            best_a[new_a_mask == 1] = alpha.action\n\n            plane[new_a_mask == 1] = random.randrange(100)\n\n            alpha_gradient = get_plane_gradient(alpha.values)\n            gradients[new_a_mask == 1] = alpha_gradient\n\n            # Max z update\n            max_z = np.max(np.array([max_z, z]), axis=0)\n\n        for x_i, x_val in enumerate(x):\n            for y_i, y_val in enumerate(y):\n                if (x_val+y_val) &gt; 1:\n                    max_z[x_i, y_i] = np.nan\n                    plane[x_i, y_i] = np.nan\n                    gradients[x_i, y_i] = np.nan\n                    best_a[x_i, y_i] = np.nan\n\n        belief_points = None\n        if belief_set is not None:\n            belief_points = belief_set.belief_array[:,1:]\n\n        fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(size*4,size*3.5), sharex=True, sharey=True)\n\n        # Set ticks\n        ticks = [0,0.25,0.5,0.75,1]\n        x_ticks = [str(t) for t in ticks]\n        x_ticks[0] = self.model.state_labels[0]\n        x_ticks[-1] = self.model.state_labels[1]\n\n        y_ticks = [str(t) for t in ticks]\n        y_ticks[0] = self.model.state_labels[0]\n        y_ticks[-1] = self.model.state_labels[2]\n\n        plt.setp([ax1,ax2,ax3,ax4], xticks=ticks, xticklabels=x_ticks, yticks=ticks, yticklabels=y_ticks)\n\n        # Value function ax\n        ax1.set_title(\"Value function\")\n        ax1_plot = ax1.contourf(x, y, max_z, 100, cmap=\"viridis\")\n        plt.colorbar(ax1_plot, ax=ax1)\n\n        # Alpha planes ax\n        ax2.set_title(\"Alpha planes\")\n        ax2_plot = ax2.contourf(x, y, plane, 100, cmap=\"viridis\")\n        plt.colorbar(ax2_plot, ax=ax2)\n\n        # Gradient of planes ax\n        ax3.set_title(\"Gradients of planes\")\n        ax3_plot = ax3.contourf(x, y, gradients, 100, cmap=\"Blues\")\n        plt.colorbar(ax3_plot, ax=ax3)\n\n        # Action policy ax\n        ax4.set_title(\"Action policy\")\n        ax4.contourf(x, y, best_a, 1, colors=[c['id'] for c in COLOR_LIST])\n        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[int(a)]['id']) for a in self.model.actions]\n        ax4.legend(proxy, self.model.action_labels, title='Actions')\n\n        if belief_points is not None:\n            for ax in [ax1,ax2,ax3,ax4]:\n                ax.scatter(belief_points[:,0], belief_points[:,1], s=1, c='black')\n\n\n    def _plot_grid(self, size=5, belief_set=None):\n        value_table = np.max(self.alpha_vector_array, axis=0)[self.model.state_grid]\n        best_action_table = np.array(self.actions)[np.argmax(self.alpha_vector_array, axis=0)][self.model.state_grid]\n        best_action_colors = COLOR_ARRAY[best_action_table]\n\n        dimensions = self.model.state_grid.shape\n\n        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(size*2, size), width_ratios=(0.55,0.45))\n\n        # Ticks\n        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))\n        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))\n\n        ax1.set_title('Value function')\n        ax1_plot = ax1.imshow(value_table)\n\n        if dimensions[0] &gt;= dimensions[1]: # If higher than wide \n            plt.colorbar(ax1_plot, ax=ax1)\n        else:\n            plt.colorbar(ax1_plot, ax=ax1, location='bottom', orientation='horizontal')\n\n        ax1.set_xticks(x_ticks)\n        ax1.set_yticks(y_ticks)\n\n        ax2.set_title('Action policy')\n        ax2.imshow(best_action_colors)\n        p = [ patches.Patch(color=COLOR_LIST[int(i)]['id'], label=str(self.model.action_labels[int(i)])) for i in self.model.actions]\n        ax2.legend(handles=p, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Actions')\n        ax2.set_xticks(x_ticks)\n        ax2.set_yticks(y_ticks)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.actions","title":"<code>actions: np.ndarray</code>  <code>property</code>","text":"<p>A list of N actions corresponding to the N alpha vectors making up the value function. If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.</p>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.alpha_vector_array","title":"<code>alpha_vector_array: np.ndarray</code>  <code>property</code>","text":"<p>A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model) If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.</p>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.alpha_vector_list","title":"<code>alpha_vector_list: list[AlphaVector]</code>  <code>property</code>","text":"<p>A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.</p>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.append","title":"<code>append(alpha_vector)</code>","text":"<p>Function to add an alpha vector to the value function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha_vector</code> <code>AlphaVector</code> <p>The alpha vector to be added to the value function.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def append(self, alpha_vector:AlphaVector) -&gt; None:\n    '''\n    Function to add an alpha vector to the value function.\n\n    Parameters\n    ----------\n    alpha_vector : AlphaVector\n        The alpha vector to be added to the value function.\n    '''\n    # Make sure size is correct\n    assert alpha_vector.values.shape[0] == self.model.state_count, f\"Vector to add to value function doesn't have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})\"\n\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n    assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f\"Vector is{' not' if self.is_on_gpu else ''} on GPU while value function is{'' if self.is_on_gpu else ' not'}.\"\n\n    if self._vector_array is not None:\n        self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)\n        self._actions = xp.append(self._actions, alpha_vector.action)\n\n    if self._vector_list is not None:\n        self._vector_list.append(alpha_vector)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.evaluate_at","title":"<code>evaluate_at(belief)</code>","text":"<p>Function to evaluate the value function at a belief point or at a set of belief points. It returns a value and the associated action.</p> <p>Parameters:</p> Name Type Description Default <code>belief</code> <code>Belief or BeliefSet</code> required <p>Returns:</p> Name Type Description <code>value</code> <code>float or ndarray</code> <p>The largest value associated with the belief point(s)</p> <code>action</code> <code>int or ndarray</code> <p>The action(s) associated with the vector having the highest values at the belief point(s).</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def evaluate_at(self, belief:Belief|BeliefSet) -&gt; tuple[float|np.ndarray, int|np.ndarray]:\n    '''\n    Function to evaluate the value function at a belief point or at a set of belief points.\n    It returns a value and the associated action.\n\n    Parameters\n    ----------\n    belief : Belief or BeliefSet\n\n    Returns\n    -------\n    value : float or np.ndarray\n        The largest value associated with the belief point(s)\n    action : int or np.ndarray\n        The action(s) associated with the vector having the highest values at the belief point(s).\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    best_value = None\n    best_action = None\n\n    if isinstance(belief, Belief):\n        # Computing values\n        belief_values = xp.dot(self.alpha_vector_array, belief.values)\n\n        # Selecting best vectors\n        best_vector = xp.argmax(belief_values)\n\n        # From best vector, compute the best value and action\n        best_value = float(belief_values[best_vector])\n        best_action = int(self.actions[best_vector])\n    else:\n        # Computing values\n        belief_values = xp.matmul(belief.values if isinstance(belief, Belief) else belief.belief_array, self.alpha_vector_array.T)\n\n        # Retrieving the top vectors according to the value function\n        best_vectors = xp.argmax(belief_values, axis=1)\n\n        # Retrieving the values and actions associated with the vectors chosen\n        best_value = belief_values[xp.arange(belief_values.shape[0]), best_vectors]\n        best_action = self.actions[best_vectors]\n\n    return (best_value, best_action)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.extend","title":"<code>extend(other_value_function)</code>","text":"<p>Function to add another value function is place. Effectively, it performs the union of the two sets of alpha vectors.</p> <p>Parameters:</p> Name Type Description Default <code>other_value_function</code> <code>ValueFunction</code> <p>The other side of the union.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def extend(self, other_value_function:'Model') -&gt; None:\n    '''\n    Function to add another value function is place.\n    Effectively, it performs the union of the two sets of alpha vectors.\n\n    Parameters\n    ----------\n    other_value_function : ValueFunction\n        The other side of the union.\n    '''\n    self._uniqueness_dict.update(other_value_function._uniqueness_dict)\n    self._vector_list = list(self._uniqueness_dict.values())\n\n    self._vector_array = None\n    self._actions = None\n\n    self._pruning_level = 1\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.load","title":"<code>load(file, model)</code>  <code>classmethod</code>","text":"<p>Function to load the value function from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path and file_name of the value function to be loaded.</p> required <code>model</code> <code>Model</code> <p>The model the value function is linked to.</p> required <p>Returns:</p> Name Type Description <code>loaded_value_function</code> <code>ValueFunction</code> <p>The loaded value function.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>@classmethod\ndef load(cls,\n        file:str,\n        model:Model\n        ) -&gt; 'ValueFunction':\n    '''\n    Function to load the value function from a csv file.\n\n    Parameters\n    ----------\n    file : str\n        The path and file_name of the value function to be loaded.\n    model : mdp.Model\n        The model the value function is linked to.\n\n    Returns\n    -------\n    loaded_value_function : ValueFunction\n        The loaded value function.\n    '''\n    av_array = np.load(file)\n\n    return ValueFunction(model, alpha_vectors=av_array[:,1:], action_list=av_array[:,0].astype(int))\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.plot","title":"<code>plot(as_grid=False, size=5, belief_set=None)</code>","text":"<p>Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid. If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.</p> <p>Parameters:</p> Name Type Description Default <code>as_grid</code> <code>bool</code> <p>Forces the plot to be plot as a grid.</p> <code>False</code> <code>size</code> <code>int</code> <p>The actual plot scale.</p> <code>5</code> <code>belief_set</code> <code>ndarray</code> <p>A set of belief to plot the belief points that were explored.</p> <code>None</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def plot(self,\n         as_grid:bool=False,\n         size:int=5,\n         belief_set:np.ndarray=None\n         ) -&gt; None:\n    '''\n    Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.\n    If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.\n\n    Parameters\n    ----------\n    as_grid : bool, default=False\n        Forces the plot to be plot as a grid.\n    size : int, default=5\n        The actual plot scale.\n    belief_set : np.ndarray, optional\n        A set of belief to plot the belief points that were explored.\n    '''\n    assert len(self) &gt; 0, \"Value function is empty, plotting is impossible...\"\n\n    # If on GPU, convert to CPU and plot that one\n    if self.is_on_gpu:\n        print('[Warning] Value function on GPU, converting to numpy before plotting...')\n        cpu_value_function = self.to_cpu()\n        cpu_value_function.plot(as_grid, size, belief_set)\n        return\n\n    func = None\n    if as_grid:\n        func = self._plot_grid\n    elif self.model.state_count == 2:\n        func = self._plot_2D\n    elif self.model.state_count == 3:\n        func = self._plot_3D\n    else:\n        print('[Warning] \\'as_grid\\' parameter set to False but state count is &gt;3 so it will be plotted as a grid')\n        func = self._plot_grid\n\n    func(size, belief_set)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.prune","title":"<code>prune(level=1)</code>","text":"<p>Function pruning the set of alpha vectors composing the value function. The pruning is as thorough as the level:     - 2: 1+ Check of absolute domination (check if dominated at each state).     - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.</p> <p>Note that the higher the level, the heavier the time impact will be.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Between 0 and 3, how thorough the alpha vector pruning should be.</p> <code>1</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def prune(self, level:int=1) -&gt; None:\n    '''\n    Function pruning the set of alpha vectors composing the value function.\n    The pruning is as thorough as the level:\n        - 2: 1+ Check of absolute domination (check if dominated at each state).\n        - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.\n\n    Note that the higher the level, the heavier the time impact will be.\n\n    Parameters\n    ----------\n    level : int, default=1\n        Between 0 and 3, how thorough the alpha vector pruning should be.\n    '''\n    # GPU support check\n    xp = cp if (gpu_support and self.is_on_gpu) else np\n\n    # Level 1 or under\n    if level &lt; self._pruning_level or level &gt; 3:\n        log('Attempting to prune a value function to a level already reached. Returning \\'self\\'')\n        return\n\n    # Level 2 pruning: Check for absolute domination\n    if level &gt;= 2 and self._pruning_level &lt; 2:\n        non_dominated_vector_indices = []\n\n        for i, v in enumerate(self.alpha_vector_array):\n            is_dom_by = xp.all(self.alpha_vector_array &gt;= v, axis=1)\n            if len(xp.where(is_dom_by)[0]) == 1:\n                non_dominated_vector_indices.append(i)\n\n        self._vector_array = self._vector_array[non_dominated_vector_indices]\n        self._actions = self._actions[non_dominated_vector_indices]\n\n    # Level 3 pruning: LP to check for more complex domination\n    if level &gt;= 3:\n        assert ilp_support, \"ILP support not enabled...\"\n\n        pruned_alpha_set = pruned_alpha_set.to_cpu()\n\n        alpha_set = pruned_alpha_set.alpha_vector_array\n        non_dominated_vector_indices = []\n\n        for i, alpha_vect in enumerate(alpha_set):\n            other_alphas = alpha_set[:i] + alpha_set[(i+1):]\n\n            # Objective function\n            c = np.concatenate([np.array([1]), -1*alpha_vect])\n\n            # Alpha vector contraints\n            other_count = len(other_alphas)\n            A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]\n            alpha_constraints = LinearConstraint(A, 0, np.inf)\n\n            # Constraints that sum of beliefs is 1\n            belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)\n\n            # Solve problem\n            res = milp(c=c, constraints=[alpha_constraints, belief_constraint])\n\n            # Check if dominated\n            is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0\n            if is_dominated:\n                print(alpha_vect)\n                print(' -&gt; Dominated\\n')\n            else:\n                non_dominated_vector_indices.append(i)\n\n        self._vector_array = self._vector_array[non_dominated_vector_indices]\n        self._actions = self._actions[non_dominated_vector_indices]\n\n    # Update the tracked pruned level so far\n    self._pruning_level = level\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.save","title":"<code>save(folder='./ValueFunctions', file_name=None)</code>","text":"<p>Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory. If no file_name is provided, it be saved as '_value_function.csv'. <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The path at which the npy file will be saved.</p> <code>'./ValueFunctions'</code> <code>file_name</code> <code>str</code> <p>The file name used to save in.</p> <code>'&lt;current_timestamp&gt;_value_function.npy'</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def save(self,\n         folder:str='./ValueFunctions',\n         file_name:Union[str,None]=None\n         ) -&gt; None:\n    '''\n    Function to save the value function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.\n    If no file_name is provided, it be saved as '&lt;current_timestamp&gt;_value_function.csv'.\n\n    Parameters\n    ----------\n    folder : str, default='./ValueFunctions'\n        The path at which the npy file will be saved.\n    file_name : str, default='&lt;current_timestamp&gt;_value_function.npy'\n        The file name used to save in.\n    '''\n    if self.is_on_gpu:\n        self.to_cpu().save(path=folder, file_name=file_name)\n        return\n\n    # Handle file_name\n    if file_name is None:\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        file_name = timestamp + '_value_function.npy'\n\n    # Make sure that .csv is in the file name\n    if '.npy' not in file_name:\n        file_name += '.npy'\n\n    # Getting array\n    av_array = np.hstack([self.actions[:,None], self.alpha_vector_array])\n\n    np.save(folder + '/' + file_name, av_array)\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.to_cpu","title":"<code>to_cpu()</code>","text":"<p>Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.</p> <p>Returns:</p> Name Type Description <code>cpu_value_function</code> <code>ValueFunction</code> <p>A new value function with arrays on CPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def to_cpu(self) -&gt; 'ValueFunction':\n    '''\n    Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.\n\n    Returns\n    -------\n    cpu_value_function : ValueFunction\n        A new value function with arrays on CPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    cpu_model = self.model.cpu_model\n\n    cpu_value_function = None\n    if self._vector_list is not None:\n        cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]\n        cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)\n\n    else:\n        cpu_vector_array = cp.asnumpy(self._vector_array)\n        cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)\n        cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)\n\n    return cpu_value_function\n</code></pre>"},{"location":"reference/agents/model_based_util/value_function/#olfactory_navigation.agents.model_based_util.value_function.ValueFunction.to_gpu","title":"<code>to_gpu()</code>","text":"<p>Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.</p> <p>Returns:</p> Name Type Description <code>gpu_value_function</code> <code>ValueFunction</code> <p>A new value function with arrays on GPU.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\value_function.py</code> <pre><code>def to_gpu(self) -&gt; 'ValueFunction':\n    '''\n    Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.\n\n    Returns\n    -------\n    gpu_value_function : ValueFunction\n        A new value function with arrays on GPU.\n    '''\n    assert gpu_support, \"GPU support is not enabled, unable to execute this function\"\n\n    gpu_model = self.model.gpu_model\n\n    gpu_value_function = None\n    if self._vector_list is not None:\n        gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]\n        gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)\n\n    else:\n        gpu_vector_array = cp.array(self._vector_array)\n        gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)\n        gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)\n\n    return gpu_value_function\n</code></pre>"},{"location":"reference/agents/model_based_util/vi_solver/","title":"vi_solver","text":""},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.SolverHistory","title":"<code>SolverHistory</code>","text":"<p>Class to represent the solving history of a solver. The purpose of this class is to allow plotting of the solution and plotting the evolution of the value function over the training process. This class is not meant to be instanciated manually, it meant to be used when returned by the solve() method of a Solver object.</p> <p>...</p> <p>Parameters:</p> Name Type Description Default <code>tracking_level</code> <code>int</code> <p>The tracking level of the solver.</p> required <code>model</code> <code>Model</code> <p>The model that has been solved by the Solver.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter used by the solver (learning rate).</p> required <code>eps</code> <code>float</code> <p>The epsilon parameter used by the solver (covergence bound).</p> required <code>initial_value_function</code> <code>ValueFunction</code> <p>The initial value function the solver will use to start the solving process.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>tracking_level</code> <code>int</code> <code>model</code> <code>Model</code> <code>gamma</code> <code>float</code> <code>eps</code> <code>float</code> <code>run_ts</code> <code>datetime</code> <p>The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.</p> <code>iteration_times</code> <code>list[float]</code> <p>A list of recorded iteration times.</p> <code>value_function_changes</code> <code>list[float]</code> <p>A list of recorded value function changes (the maximum changed value between 2 value functions).</p> <code>value_functions</code> <code>list[ValueFunction]</code> <p>A list of recorded value functions.</p> <code>solution</code> <code>ValueFunction</code> <code>summary</code> <code>str</code> Source code in <code>olfactory_navigation\\agents\\model_based_util\\vi_solver.py</code> <pre><code>class SolverHistory:\n    '''\n    Class to represent the solving history of a solver.\n    The purpose of this class is to allow plotting of the solution and plotting the evolution of the value function over the training process.\n    This class is not meant to be instanciated manually, it meant to be used when returned by the solve() method of a Solver object.\n\n    ...\n\n    Parameters\n    ----------\n    tracking_level : int\n        The tracking level of the solver.\n    model : mdp.Model\n        The model that has been solved by the Solver.\n    gamma : float\n        The gamma parameter used by the solver (learning rate).\n    eps : float\n        The epsilon parameter used by the solver (covergence bound).\n    initial_value_function : ValueFunction, optional\n        The initial value function the solver will use to start the solving process.\n\n    Attributes\n    ----------\n    tracking_level : int\n    model : mdp.Model\n    gamma : float\n    eps : float\n    run_ts : datetime\n        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.\n    iteration_times : list[float]\n        A list of recorded iteration times.\n    value_function_changes : list[float]\n        A list of recorded value function changes (the maximum changed value between 2 value functions).\n    value_functions : list[ValueFunction]\n        A list of recorded value functions.\n    solution : ValueFunction\n    summary : str\n    '''\n    def __init__(self,\n                 tracking_level:int,\n                 model:Model,\n                 gamma:float,\n                 eps:float,\n                 initial_value_function:Union[ValueFunction,None]=None\n                 ):\n        self.tracking_level = tracking_level\n        self.model = model\n        self.gamma = gamma\n        self.eps = eps\n        self.run_ts = datetime.now()\n\n        # Tracking metrics\n        self.iteration_times = []\n        self.value_function_changes = []\n\n        self.value_functions = []\n        if self.tracking_level &gt;= 2:\n            self.value_functions.append(initial_value_function)\n\n\n    @property\n    def solution(self) -&gt; ValueFunction:\n        '''\n        The last value function of the solving process.\n        '''\n        assert self.tracking_level &gt;= 2, \"Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.\"\n        return self.value_functions[-1]\n\n\n    def add(self,\n            iteration_time:float,\n            value_function_change:float,\n            value_function:ValueFunction\n            ) -&gt; None:\n        '''\n        Function to add a step in the simulation history.\n\n        Parameters\n        ----------\n        iteration_time : float\n            The time it took to run the iteration.\n        value_function_change : float\n            The change between the value function of this iteration and of the previous iteration.\n        value_function : ValueFunction\n            The value function resulting after a step of the solving process.\n        '''\n        if self.tracking_level &gt;= 1:\n            self.iteration_times.append(float(iteration_time))\n            self.value_function_changes.append(float(value_function_change))\n\n        if self.tracking_level &gt;= 2:\n            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())\n\n\n    @property\n    def summary(self) -&gt; str:\n        '''\n        A summary as a string of the information recorded.\n        '''\n        summary_str =  f'Summary of Value Iteration run'\n        summary_str += f'\\n  - Model: {self.model.state_count}-state, {self.model.action_count}-action'\n        summary_str += f'\\n  - Converged in {len(self.iteration_times)} iterations and {sum(self.iteration_times):.4f} seconds'\n\n        if self.tracking_level &gt;= 1:\n            summary_str += f'\\n  - Took on average {sum(self.iteration_times) / len(self.iteration_times):.4f}s per iteration'\n\n        return summary_str\n\n\n    def plot_changes(self) -&gt; None:\n        '''\n        Function to plot the value function changes over the solving process.\n        '''\n        assert self.tracking_level &gt;= 1, \"To plot the change of the value function over time, use tracking level 1 or higher.\"\n\n        plt.title('Value function change over time')\n        plt.plot(np.arange(len(self.value_function_changes)), self.value_function_changes)\n        plt.xlabel('Iteration')\n        plt.ylabel('Value function change')\n        plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.SolverHistory.solution","title":"<code>solution: ValueFunction</code>  <code>property</code>","text":"<p>The last value function of the solving process.</p>"},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.SolverHistory.summary","title":"<code>summary: str</code>  <code>property</code>","text":"<p>A summary as a string of the information recorded.</p>"},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.SolverHistory.add","title":"<code>add(iteration_time, value_function_change, value_function)</code>","text":"<p>Function to add a step in the simulation history.</p> <p>Parameters:</p> Name Type Description Default <code>iteration_time</code> <code>float</code> <p>The time it took to run the iteration.</p> required <code>value_function_change</code> <code>float</code> <p>The change between the value function of this iteration and of the previous iteration.</p> required <code>value_function</code> <code>ValueFunction</code> <p>The value function resulting after a step of the solving process.</p> required Source code in <code>olfactory_navigation\\agents\\model_based_util\\vi_solver.py</code> <pre><code>def add(self,\n        iteration_time:float,\n        value_function_change:float,\n        value_function:ValueFunction\n        ) -&gt; None:\n    '''\n    Function to add a step in the simulation history.\n\n    Parameters\n    ----------\n    iteration_time : float\n        The time it took to run the iteration.\n    value_function_change : float\n        The change between the value function of this iteration and of the previous iteration.\n    value_function : ValueFunction\n        The value function resulting after a step of the solving process.\n    '''\n    if self.tracking_level &gt;= 1:\n        self.iteration_times.append(float(iteration_time))\n        self.value_function_changes.append(float(value_function_change))\n\n    if self.tracking_level &gt;= 2:\n        self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())\n</code></pre>"},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.SolverHistory.plot_changes","title":"<code>plot_changes()</code>","text":"<p>Function to plot the value function changes over the solving process.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\vi_solver.py</code> <pre><code>def plot_changes(self) -&gt; None:\n    '''\n    Function to plot the value function changes over the solving process.\n    '''\n    assert self.tracking_level &gt;= 1, \"To plot the change of the value function over time, use tracking level 1 or higher.\"\n\n    plt.title('Value function change over time')\n    plt.plot(np.arange(len(self.value_function_changes)), self.value_function_changes)\n    plt.xlabel('Iteration')\n    plt.ylabel('Value function change')\n    plt.show()\n</code></pre>"},{"location":"reference/agents/model_based_util/vi_solver/#olfactory_navigation.agents.model_based_util.vi_solver.solve","title":"<code>solve(model, horizon=100, initial_value_function=None, gamma=0.99, eps=1e-06, use_gpu=False, history_tracking_level=1, print_progress=True)</code>","text":"<p>Function to solve an MDP model using Value Iteration. If an initial value function is not provided, the value function will be initiated with the expected rewards.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model on which to run value iteration.</p> required <code>horizon</code> <code>int</code> <p>How many iterations to run the value iteration solver for.</p> <code>100</code> <code>initial_value_function</code> <code>ValueFunction</code> <p>An optional initial value function to kick-start the value iteration process.</p> <code>None</code> <code>gamma</code> <code>float</code> <p>The discount factor to value immediate rewards more than long term rewards. The learning rate is 1/gamma.</p> <code>0.99</code> <code>eps</code> <code>float</code> <p>The smallest allowed changed for the value function. Bellow the amound of change, the value function is considered converged and the value iteration process will end early.</p> <code>1e-6</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use the GPU with cupy array to accelerate solving.</p> <code>False</code> <code>history_tracking_level</code> <code>int</code> <p>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</p> <code>1</code> <code>print_progress</code> <code>bool</code> <p>Whether or not to print out the progress of the value iteration process.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>value_function</code> <code>ValueFunction</code> <p>The resulting value function solution to the model.</p> <code>history</code> <code>SolverHistory</code> <p>The tracking of the solution over time.</p> Source code in <code>olfactory_navigation\\agents\\model_based_util\\vi_solver.py</code> <pre><code>def solve(model:Model,\n          horizon:int=100,\n          initial_value_function:Union[ValueFunction,None]=None,\n          gamma:float=0.99,\n          eps:float=1e-6,\n          use_gpu:bool=False,\n          history_tracking_level:int=1,\n          print_progress:bool=True\n          ) -&gt; tuple[ValueFunction, SolverHistory]:\n    '''\n    Function to solve an MDP model using Value Iteration.\n    If an initial value function is not provided, the value function will be initiated with the expected rewards.\n\n    Parameters\n    ----------\n    model : mdp.Model\n        The model on which to run value iteration.\n    horizon : int, default=100\n        How many iterations to run the value iteration solver for.\n    initial_value_function : ValueFunction, optional\n        An optional initial value function to kick-start the value iteration process.\n    gamma : float, default=0.99\n        The discount factor to value immediate rewards more than long term rewards.\n        The learning rate is 1/gamma.\n    eps : float, default=1e-6\n        The smallest allowed changed for the value function.\n        Bellow the amound of change, the value function is considered converged and the value iteration process will end early.\n    use_gpu : bool, default=False\n        Whether to use the GPU with cupy array to accelerate solving.\n    history_tracking_level : int, default=1\n        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)\n    print_progress : bool, default=True\n        Whether or not to print out the progress of the value iteration process.\n\n    Returns\n    -------\n    value_function: ValueFunction\n        The resulting value function solution to the model.\n    history : SolverHistory\n        The tracking of the solution over time.\n    '''\n    # GPU support\n    if use_gpu:\n        assert gpu_support, \"GPU support is not enabled, Cupy might need to be installed...\"\n\n    xp = np if not use_gpu else cp\n    model = model if not use_gpu else model.gpu_model\n\n    # Value function initialization\n    if initial_value_function is None:\n        V = ValueFunction(model, model.expected_rewards_table.T, model.actions)\n    else:\n        V = initial_value_function.to_gpu() if use_gpu else initial_value_function\n    V_opt = xp.max(V.alpha_vector_array, axis=0)\n\n    # History tracking setup\n    solve_history = SolverHistory(tracking_level=history_tracking_level,\n                                  model=model,\n                                  gamma=gamma,\n                                  eps=eps,\n                                  initial_value_function=V)\n\n    # Computing max allowed change from epsilon and gamma parameters\n    max_allowed_change = eps * (gamma / (1-gamma))\n\n    iterator = trange(horizon) if print_progress else range(horizon)\n    for _ in iterator:\n        old_V_opt = V_opt\n\n        start = datetime.now()\n\n        # Computing the new alpha vectors\n        alpha_vectors = model.expected_rewards_table.T + (gamma * xp.einsum('sar,sar-&gt;as', model.reachable_probabilities, V_opt[model.reachable_states]))\n        V = ValueFunction(model, alpha_vectors, model.actions)\n\n        V_opt = xp.max(V.alpha_vector_array, axis=0)\n\n        # Change computation\n        max_change = xp.max(xp.abs(V_opt - old_V_opt))\n\n        # Tracking the history\n        iteration_time = (datetime.now() - start).total_seconds()\n        solve_history.add(iteration_time=iteration_time,\n                            value_function_change=max_change,\n                            value_function=V)\n\n        # Convergence check\n        if max_change &lt; max_allowed_change:\n            iterator.close()\n            break\n\n    return V, solve_history\n</code></pre>"}]}