La sottrazione del max al gradiente dovrebbe essere un'operazione innocua a causa del softmax.
In primis, la funzione softmax di scipy, per evitare problemi numerici e di overflow, lo fa già di suo.
In più, se nel passo, oltre ad alpha, moltiplichiamo anche per l'inverso della norma del gradiente, abbiamo che
dopo aver sottratto il max, la norma è tendenzialmente minore di 1; In questo modo, effettivamente allunghiamo il passo.
N.B.: Non sono super convinto che sia una cosa buona in generale, ma forse in questo caso non da problemi.

Infine, anche "logicamente", non dovrebbe essere un probelma perchè noi stiamo cercando la policy(theta) che massimizza J,
non i theta veri e propri. Quindi, la sottrazione del max al gradiente, non è un'operazione carina in R^n, dove vivono i theta,
ma nello spazio delle policy è un'operazione neutra, che lascia inalterata la policy.


Ho runnato di nuovo con alpha 0.001 senza fare la divisione per la norma infinito del gradiente.
Togliendo il max per righe al gradiente oppure lasciandolo così com'è la differenza di risultati sulla policy è attorno al 10^-12,
errore che suppongo più imputabile a errori floating point che altro.

Ho runnato facendo anche rescaling e togliendo il max. La norma inf dei gradienti è inizialmente ~7, ma 
diminuisce velocemente assestandosi attorno a ~0.5, quindi è come che abbia un alpha più grande; 
ed infatti, dopo 30000 iterazioni "converge", mentre senza rescaling avrebbe continuato anche dopo le 35000

Ho runnato facendo rescaling ma senza togliere il max. La norma inf dei gradienti è sempre molto grande, quindi il passo effettivo
è praticamente nullo: dopo 35000 iterazioni, il massimo della differenza con la policy inziale è del 1%


Ho rifatto le run con alpha 0.01. Se non faccio rescaling, sia che sottragga che no, "convergo" alla policy che trovavo già da prima.
Se invece, faccio rescaling con la norma infinito del gradiente e sottraggo, allora "convergo" alla policy che trovano anche loro.
Se faccio rescaling ma non sottraggo, allora il passo sarà troppo breve, esattamente come sopra.

Nell'actor critic inizializzare V a -1 sembra essere o uguale o un po' peggio che inizializzare a 0

Ho rilanciato modelBased con alpha 1e-2, facendo rescaling e sottraendo, e tutte le volte arriva alla policy corretta, partendo ogni volta da policy diverse

La policy corretta è MOLTO sensibile: Una differenza di 5e-3 per le azioni crosswind peggiora notevolemnte i risultati [eval/diff5.png]
quindi anche le policy vicine non bastano.

ho provato 10 run per ogni combinazione di alpha tra 0.1 0.01 e 0.005 sia per l'actor che per il critic. Quando il learning rate 
del critic è 0.01 o 0.005, la policy rimane praticamente uniforme, mentre negli altri casi si avvicina alla policy corretta.
è molto probabile che 0.1 sia un learning rate "abbastanza buono" anche da considerazioni teoriche, Sutton pagina 330.

Sulle 90 run fatte, oneStepAC "impazzisce" solo 5 volte, e solo in una di queste non si riprende. Sembra però che con più episodi, ci
sarebbe riuscita anche questa. (0.005 0.1 run3).

Quando il learning rate dell'actor è 0.1 o 0.01, le policy hanno abbastanza varianza

Quando il learning rate dell'actor è 0.005, dopo 50000 episodi le policy ottenute sono abbastanza simili tra loro e non troppo distanti
da quella corretta. POSSIBILE che con più episodi possa "convergere" 


50000 episodi sono decisamente troppi. IMPORTANTISSIMO decidere come fermarsi.

TD lambda 0.7 0.7 attorno alle 30000, ma anche prima
    run 7 31000 sembra carina
    run 8 27000 è quella corretta.
    run 9 33000 anche

TD lambda 0.7 0.8 attorno alle 20000
    run 3 18000 è quella corretta

TD lambda 0.9 0.9
    run 3 11000 

sembra che TD lambda 0.8 0.7 arriva a delle policy vicine attorno ai 20000 episodi, per poi aggirarsi attorno ma peggiorare
    TD lambda 0.8 0.7 run 2 è impazzito attorno al 14000 episodio, ma non ha raggiunto il determinismo puro, lasciando 1.7e-4 di scegliere
    di andare comunque upwind. al 21000 episodio era tornato al 98% upwind.
    run 5 è impazzita al 19000 episodio, arrivando al determinismo puro e quindi non si è più ripresa
    run 6 la policy ai 20000 sembra ottima


sembra che TD lambda 0.8 0.8 arriva alla policy corretta attorno ai 20000 episodi, per poi aggirarsi attorno ma peggiorare
    run 3 è impazzita ai 25000 episodi, ma ai 23000 aveva una policy che sembra molto bella
    run 7 19000 sembra lo stesso molto vicina


TD_lambda sembra abbastanza instabile quando lambda critic è 0.9: sulle 30 run, 5 è arrivato al determinismo anche quando NON osserva
Sulle 90 run totali, 14 sono "impazzite" quando osserva, 6 delle quali si sono riprese e 8 no. 
di quelle che si sono riprese:
    1 aLambda 0.8 cLambda 0.7 
    2 aLambda 0.7 cLambda 0.9 
    1 aLambda 0.8 cLambda 0.9 
    2 aLambda 0.9 cLambda 0.9 
delle altre:
    1 aLambda 0.8 cLambda 0.7 
    1 aLambda 0.8 cLambda 0.8 
    1 aLambda 0.8 cLambda 0.9
    2 aLambda 0.9 cLambda 0.8 
    3 aLambda 0.9 cLambda 0.9 

Le policy vicine con TD_Lambda (tdLambda_possibili.out) sono poche perchè ho abbassato la differenza massima per considerarle corretta,
perchè se avessi usato la stessa usata per oneStepAC, rispettivamente 0.05 e 0.5, avrei avuto troppe policy da testare (~750)

0.7 0.7 0.1 Sched 0.1 ha bisogno di più di 50000 episodi
0.7 0.8 0.1 Sched 0.1 arriva a policy vicine dopo ~40000 episodi
0.7 0.9 0.1 Sched 0.1 
0.8 0.7 0.1 Sched 0.1
0.8 0.8 0.1 Sched 0.1
0.8 0.9 0.1 Sched 0.1
0.9 0.7 0.1 Sched 0.1
0.9 0.8 0.1 Sched 0.1
0.9 0.9 0.1 Sched 0.1 


La policy 0.7 0.7 0.2 0.2 Sched, nonostante sia parecchio diversa da quella ottimale, raggiunge la sorgente più spesso, ma in più tempo.
La policy praticamente inverte le probabilità di crosswind con quelle up e downwind. 


le run longRun sono run in cui ho schedulato solo l'actor ma non il critic
le run longRunCont sono il continuo delle run longRun, ma ho fatto riprendere lo scheduling dell'actor da dove si era interrrotto, mentre ho fatto iniziare lo scheduling del critic come se iniziasse li. sono entrambi fatti con alhpa0/sqrt(t)
